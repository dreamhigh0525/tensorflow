// RUN: hlo_to_llvm_ir %s | FileCheck %s

HloModule TestModule

compare {
  p.0.lhs = f32[] parameter(0)
  p.0.rhs = f32[] parameter(1)
  ROOT lt = pred[] compare(p.0.lhs, p.0.rhs), direction=LT
}

// CHECK: define void @sort(i8* noalias align 64 dereferenceable(24) [[ALLOC0:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC1:%.*]])
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[COMPARE_RETURN_BUFFER:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[SORT_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC0]], i64 0
// CHECK-NEXT:    [[SORT_TYPED:%.*]] = bitcast i8* [[SORT_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[X_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC1]], i64 0
// CHECK-NEXT:    [[X_TYPED:%.*]] = bitcast i8* [[X_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK-NEXT:    [[BLOCK_ID:%.*]] = zext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK-NEXT:    [[THREAD_ID:%.*]] = zext i32 [[TMP1]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = mul nuw nsw i64 [[BLOCK_ID]], 4
// CHECK-NEXT:    [[LINEAR_INDEX:%.*]] = add nuw nsw i64 [[TMP2]], [[THREAD_ID]]
// CHECK-NEXT:    [[LINEAR_INDEX_IN_RANGE:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    call void @llvm.assume(i1 [[LINEAR_INDEX_IN_RANGE]])
// CHECK-NEXT:    [[TMP3:%.*]] = udiv i64 [[LINEAR_INDEX]], 1
// CHECK-NEXT:    [[TMP4:%.*]] = urem i64 [[TMP3]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = udiv i64 [[LINEAR_INDEX]], 2
// CHECK-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    br i1 [[TMP6]], label [[SORT_IN_BOUNDS_TRUE:%.*]], label [[SORT_IN_BOUNDS_AFTER:%.*]]
// CHECK:       sort.in_bounds-after:
// CHECK-NEXT:    ret void
// CHECK:       sort.in_bounds-true:
// CHECK-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP4]], 2
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = icmp slt i64 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[TMP10:%.*]] = icmp slt i64 [[TMP8]], 3
// CHECK-NEXT:    [[TMP11:%.*]] = and i1 [[TMP9]], [[TMP10]]
// CHECK-NEXT:    br i1 [[TMP11]], label [[SMALLER_COMPARISON_INDEX_TRUE:%.*]], label [[SMALLER_COMPARISON_INDEX_AFTER:%.*]]
// CHECK:       smaller_comparison_index-after:
// CHECK-NEXT:    br label [[SORT_IN_BOUNDS_AFTER]]
// CHECK:       smaller_comparison_index-true:
// CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    call void @compare(float* [[TMP12]], float* [[TMP13]], i8* [[COMPARE_RETURN_BUFFER]])
// CHECK-NEXT:    [[TMP14:%.*]] = load i8, i8* [[COMPARE_RETURN_BUFFER]], align 1
// CHECK-NEXT:    [[BOOLEAN_PREDICATE:%.*]] = icmp ne i8 [[TMP14]], 0
// CHECK-NEXT:    br i1 [[BOOLEAN_PREDICATE]], label [[IS_SMALLER_THAN_TRUE:%.*]], label [[IS_SMALLER_THAN_AFTER:%.*]]
// CHECK:       is_smaller_than-after:
// CHECK-NEXT:    br label [[SMALLER_COMPARISON_INDEX_AFTER]]
// CHECK:       is_smaller_than-true:
// CHECK-NEXT:    [[TMP15:%.*]] = load float, float* [[TMP12]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = load float, float* [[TMP13]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    store float [[TMP15]], float* [[TMP17]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    store float [[TMP16]], float* [[TMP18]], align 4
// CHECK-NEXT:    br label [[IS_SMALLER_THAN_AFTER]]

// CHECK: define internal void @compare(float* dereferenceable(4) [[P_0_LHS_TYPED:%.*]], float* dereferenceable(4) [[P_0_RHS_TYPED:%.*]], i8* dereferenceable(1) [[OUTPUT_ARG:%.*]])
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[LT_TYPED:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load float, float* [[P_0_LHS_TYPED]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load float, float* [[P_0_RHS_TYPED]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp olt float [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i8
// CHECK-NEXT:    store i8 [[TMP3]], i8* [[LT_TYPED]], align 1
// CHECK-NEXT:    [[LOAD_RET_VALUE:%.*]] = load i8, i8* [[LT_TYPED]], align 1
// CHECK-NEXT:    store i8 [[LOAD_RET_VALUE]], i8* [[OUTPUT_ARG]], align 1
// CHECK-NEXT:    ret void

// CHECK: define void @sort__1(i8* noalias align 64 dereferenceable(24) [[ALLOC0:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC1:%.*]]) {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[COMPARE_RETURN_BUFFER:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[SORT_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC0]], i64 0
// CHECK-NEXT:    [[SORT_TYPED:%.*]] = bitcast i8* [[SORT_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[X_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC1]], i64 0
// CHECK-NEXT:    [[X_TYPED:%.*]] = bitcast i8* [[X_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK-NEXT:    [[BLOCK_ID:%.*]] = zext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK-NEXT:    [[THREAD_ID:%.*]] = zext i32 [[TMP1]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = mul nuw nsw i64 [[BLOCK_ID]], 4
// CHECK-NEXT:    [[LINEAR_INDEX:%.*]] = add nuw nsw i64 [[TMP2]], [[THREAD_ID]]
// CHECK-NEXT:    [[LINEAR_INDEX_IN_RANGE:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    call void @llvm.assume(i1 [[LINEAR_INDEX_IN_RANGE]])
// CHECK-NEXT:    [[TMP3:%.*]] = udiv i64 [[LINEAR_INDEX]], 1
// CHECK-NEXT:    [[TMP4:%.*]] = urem i64 [[TMP3]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = udiv i64 [[LINEAR_INDEX]], 2
// CHECK-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    br i1 [[TMP6]], label [[SORT_IN_BOUNDS_TRUE:%.*]], label [[SORT_IN_BOUNDS_AFTER:%.*]]
// CHECK:       sort.in_bounds-after:
// CHECK-NEXT:    ret void
// CHECK:       sort.in_bounds-true:
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP4]], 3
// CHECK-NEXT:    [[TMP8:%.*]] = icmp slt i64 [[TMP4]], [[TMP7]]
// CHECK-NEXT:    [[TMP9:%.*]] = icmp slt i64 [[TMP7]], 3
// CHECK-NEXT:    [[TMP10:%.*]] = and i1 [[TMP8]], [[TMP9]]
// CHECK-NEXT:    br i1 [[TMP10]], label [[SMALLER_COMPARISON_INDEX_TRUE:%.*]], label [[SMALLER_COMPARISON_INDEX_AFTER:%.*]]
// CHECK:       smaller_comparison_index-after:
// CHECK-NEXT:    br label [[SORT_IN_BOUNDS_AFTER]]
// CHECK:       smaller_comparison_index-true:
// CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP4]]
// CHECK-NEXT:    call void @compare(float* [[TMP11]], float* [[TMP12]], i8* [[COMPARE_RETURN_BUFFER]])
// CHECK-NEXT:    [[TMP13:%.*]] = load i8, i8* [[COMPARE_RETURN_BUFFER]], align 1
// CHECK-NEXT:    [[BOOLEAN_PREDICATE:%.*]] = icmp ne i8 [[TMP13]], 0
// CHECK-NEXT:    br i1 [[BOOLEAN_PREDICATE]], label [[IS_SMALLER_THAN_TRUE:%.*]], label [[IS_SMALLER_THAN_AFTER:%.*]]
// CHECK:       is_smaller_than-after:
// CHECK-NEXT:    br label [[SMALLER_COMPARISON_INDEX_AFTER]]
// CHECK:       is_smaller_than-true:
// CHECK-NEXT:    [[TMP14:%.*]] = load float, float* [[TMP11]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load float, float* [[TMP12]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP4]]
// CHECK-NEXT:    store float [[TMP14]], float* [[TMP16]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    store float [[TMP15]], float* [[TMP17]], align 4
// CHECK-NEXT:    br label [[IS_SMALLER_THAN_AFTER]]

// CHECK: define void @sort__2(i8* noalias align 64 dereferenceable(24) [[ALLOC0:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC1:%.*]]) {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[COMPARE_RETURN_BUFFER:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[SORT_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC0:%.*]], i64 0
// CHECK-NEXT:    [[SORT_TYPED:%.*]] = bitcast i8* [[SORT_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[X_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC1:%.*]], i64 0
// CHECK-NEXT:    [[X_TYPED:%.*]] = bitcast i8* [[X_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK-NEXT:    [[BLOCK_ID:%.*]] = zext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK-NEXT:    [[THREAD_ID:%.*]] = zext i32 [[TMP1]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = mul nuw nsw i64 [[BLOCK_ID]], 4
// CHECK-NEXT:    [[LINEAR_INDEX:%.*]] = add nuw nsw i64 [[TMP2]], [[THREAD_ID]]
// CHECK-NEXT:    [[LINEAR_INDEX_IN_RANGE:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    call void @llvm.assume(i1 [[LINEAR_INDEX_IN_RANGE]])
// CHECK-NEXT:    [[TMP3:%.*]] = udiv i64 [[LINEAR_INDEX]], 1
// CHECK-NEXT:    [[TMP4:%.*]] = urem i64 [[TMP3]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = udiv i64 [[LINEAR_INDEX]], 2
// CHECK-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    br i1 [[TMP6]], label [[SORT_IN_BOUNDS_TRUE:%.*]], label [[SORT_IN_BOUNDS_AFTER:%.*]]
// CHECK:       sort.in_bounds-after:
// CHECK-NEXT:    ret void
// CHECK:       sort.in_bounds-true:
// CHECK-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP4]], 2
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = icmp slt i64 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[TMP10:%.*]] = icmp slt i64 [[TMP8]], 3
// CHECK-NEXT:    [[TMP11:%.*]] = and i1 [[TMP9]], [[TMP10]]
// CHECK-NEXT:    br i1 [[TMP11]], label [[SMALLER_COMPARISON_INDEX_TRUE:%.*]], label [[SMALLER_COMPARISON_INDEX_AFTER:%.*]]
// CHECK:       smaller_comparison_index-after:
// CHECK-NEXT:    br label [[SORT_IN_BOUNDS_AFTER]]
// CHECK:       smaller_comparison_index-true:
// CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    call void @compare(float* [[TMP12]], float* [[TMP13]], i8* [[COMPARE_RETURN_BUFFER]])
// CHECK-NEXT:    [[TMP14:%.*]] = load i8, i8* [[COMPARE_RETURN_BUFFER]], align 1
// CHECK-NEXT:    [[BOOLEAN_PREDICATE:%.*]] = icmp ne i8 [[TMP14]], 0
// CHECK-NEXT:    br i1 [[BOOLEAN_PREDICATE]], label [[IS_SMALLER_THAN_TRUE:%.*]], label [[IS_SMALLER_THAN_AFTER:%.*]]
// CHECK:       is_smaller_than-after:
// CHECK-NEXT:    br label [[SMALLER_COMPARISON_INDEX_AFTER]]
// CHECK:       is_smaller_than-true:
// CHECK-NEXT:    [[TMP15:%.*]] = load float, float* [[TMP12]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = load float, float* [[TMP13]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    store float [[TMP15]], float* [[TMP17]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    store float [[TMP16]], float* [[TMP18]], align 4
// CHECK-NEXT:    br label [[IS_SMALLER_THAN_AFTER]]
ENTRY main {
  x = f32[2, 3] parameter(0)
  ROOT sort = f32[2, 3] sort(x), dimensions={1}, to_apply=compare
}

// -----

HloModule TestModule

compare {
  p.0.lhs = s32[] parameter(0)
  p.0.rhs = s32[] parameter(1)
  p.1.lhs = f32[] parameter(2)
  p.1.rhs = f32[] parameter(3)
  ROOT lt = pred[] compare(p.1.lhs, p.1.rhs), direction=LT
}

// CHECK: define void @sort(i8* noalias align 64 dereferenceable(24) [[ALLOC0:%.*]], i8* noalias align 64 dereferenceable(24) [[ALLOC1:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC2:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC3:%.*]], i8* noalias align 64 dereferenceable(16) [[ALLOC4:%.*]])
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[COMPARE_RETURN_BUFFER:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[SORT_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC4]], i64 0
// CHECK-NEXT:    [[SORT_TYPED:%.*]] = bitcast i8* [[SORT_RAW]] to [2 x i8*]*
// CHECK-NEXT:    [[SORT_RAW1:%.*]] = getelementptr inbounds i8, i8* [[ALLOC0]], i64 0
// CHECK-NEXT:    [[SORT_TYPED2:%.*]] = bitcast i8* [[SORT_RAW1]] to [2 x [3 x i32]]*
// CHECK-NEXT:    [[SORT_RAW3:%.*]] = getelementptr inbounds i8, i8* [[ALLOC1]], i64 0
// CHECK-NEXT:    [[SORT_TYPED4:%.*]] = bitcast i8* [[SORT_RAW3]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[X_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC2]], i64 0
// CHECK-NEXT:    [[X_TYPED:%.*]] = bitcast i8* [[X_RAW]] to [2 x [3 x i32]]*
// CHECK-NEXT:    [[Y_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC3]], i64 0
// CHECK-NEXT:    [[Y_TYPED:%.*]] = bitcast i8* [[Y_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK-NEXT:    [[BLOCK_ID:%.*]] = zext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK-NEXT:    [[THREAD_ID:%.*]] = zext i32 [[TMP1]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = mul nuw nsw i64 [[BLOCK_ID]], 4
// CHECK-NEXT:    [[LINEAR_INDEX:%.*]] = add nuw nsw i64 [[TMP2]], [[THREAD_ID]]
// CHECK-NEXT:    [[LINEAR_INDEX_IN_RANGE:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    call void @llvm.assume(i1 [[LINEAR_INDEX_IN_RANGE]])
// CHECK-NEXT:    [[TMP3:%.*]] = udiv i64 [[LINEAR_INDEX]], 1
// CHECK-NEXT:    [[TMP4:%.*]] = urem i64 [[TMP3]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = udiv i64 [[LINEAR_INDEX]], 2
// CHECK-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    br i1 [[TMP6]], label [[SORT_IN_BOUNDS_TRUE:%.*]], label [[SORT_IN_BOUNDS_AFTER:%.*]]
// CHECK:       sort.in_bounds-after:
// CHECK-NEXT:    ret void
// CHECK:       sort.in_bounds-true:
// CHECK-NEXT:    [[TMP7:%.*]] = mul i64 [[TMP4]], 2
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = icmp slt i64 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[TMP10:%.*]] = icmp slt i64 [[TMP8]], 3
// CHECK-NEXT:    [[TMP11:%.*]] = and i1 [[TMP9]], [[TMP10]]
// CHECK-NEXT:    br i1 [[TMP11]], label [[SMALLER_COMPARISON_INDEX_TRUE:%.*]], label [[SMALLER_COMPARISON_INDEX_AFTER:%.*]]
// CHECK:       smaller_comparison_index-after:
// CHECK-NEXT:    br label [[SORT_IN_BOUNDS_AFTER]]
// CHECK:       smaller_comparison_index-true:
// CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    call void @compare(i32* [[TMP12]], i32* [[TMP13]], float* [[TMP14]], float* [[TMP15]], i8* [[COMPARE_RETURN_BUFFER]])
// CHECK-NEXT:    [[TMP16:%.*]] = load i8, i8* [[COMPARE_RETURN_BUFFER]], align 1
// CHECK-NEXT:    [[BOOLEAN_PREDICATE:%.*]] = icmp ne i8 [[TMP16]], 0
// CHECK-NEXT:    br i1 [[BOOLEAN_PREDICATE]], label [[IS_SMALLER_THAN_TRUE:%.*]], label [[IS_SMALLER_THAN_AFTER:%.*]]
// CHECK:       is_smaller_than-after:
// CHECK-NEXT:    br label [[SMALLER_COMPARISON_INDEX_AFTER]]
// CHECK:       is_smaller_than-true:
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, i32* [[TMP12]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, i32* [[TMP13]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    store i32 [[TMP17]], i32* [[TMP19]], align 4
// CHECK-NEXT:    [[TMP20:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    store i32 [[TMP18]], i32* [[TMP20]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load float, float* [[TMP14]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = load float, float* [[TMP15]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    store float [[TMP21]], float* [[TMP23]], align 4
// CHECK-NEXT:    [[TMP24:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP8]]
// CHECK-NEXT:    store float [[TMP22]], float* [[TMP24]], align 4
// CHECK-NEXT:    br label [[IS_SMALLER_THAN_AFTER]]

// CHECK: define internal void @compare(i32* dereferenceable(4) [[P_0_LHS_TYPED:%.*]], i32* dereferenceable(4) [[P_0_RHS_TYPED:%.*]], float* dereferenceable(4) [[P_1_LHS_TYPED:%.*]], float* dereferenceable(4) [[P_1_RHS_TYPED:%.*]], i8* dereferenceable(1) [[OUTPUT_ARG:%.*]])
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[LT_TYPED:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load float, float* [[P_1_LHS_TYPED]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load float, float* [[P_1_RHS_TYPED]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp olt float [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i8
// CHECK-NEXT:    store i8 [[TMP3]], i8* [[LT_TYPED]], align 1
// CHECK-NEXT:    [[LOAD_RET_VALUE:%.*]] = load i8, i8* [[LT_TYPED]], align 1
// CHECK-NEXT:    store i8 [[LOAD_RET_VALUE]], i8* [[OUTPUT_ARG]], align 1
// CHECK-NEXT:    ret void

// CHECK: define void @sort__1(i8* noalias align 64 dereferenceable(24) [[ALLOC0:%.*]], i8* noalias align 64 dereferenceable(24) [[ALLOC1:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC2:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC3:%.*]], i8* noalias align 64 dereferenceable(16) [[ALLOC4:%.*]])
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[COMPARE_RETURN_BUFFER:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[SORT_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC4:%.*]], i64 0
// CHECK-NEXT:    [[SORT_TYPED:%.*]] = bitcast i8* [[SORT_RAW]] to [2 x i8*]*
// CHECK-NEXT:    [[SORT_RAW1:%.*]] = getelementptr inbounds i8, i8* [[ALLOC0:%.*]], i64 0
// CHECK-NEXT:    [[SORT_TYPED2:%.*]] = bitcast i8* [[SORT_RAW1]] to [2 x [3 x i32]]*
// CHECK-NEXT:    [[SORT_RAW3:%.*]] = getelementptr inbounds i8, i8* [[ALLOC1:%.*]], i64 0
// CHECK-NEXT:    [[SORT_TYPED4:%.*]] = bitcast i8* [[SORT_RAW3]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[X_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC2:%.*]], i64 0
// CHECK-NEXT:    [[X_TYPED:%.*]] = bitcast i8* [[X_RAW]] to [2 x [3 x i32]]*
// CHECK-NEXT:    [[Y_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC3:%.*]], i64 0
// CHECK-NEXT:    [[Y_TYPED:%.*]] = bitcast i8* [[Y_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK-NEXT:    [[BLOCK_ID:%.*]] = zext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK-NEXT:    [[THREAD_ID:%.*]] = zext i32 [[TMP1]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = mul nuw nsw i64 [[BLOCK_ID]], 4
// CHECK-NEXT:    [[LINEAR_INDEX:%.*]] = add nuw nsw i64 [[TMP2]], [[THREAD_ID]]
// CHECK-NEXT:    [[LINEAR_INDEX_IN_RANGE:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    call void @llvm.assume(i1 [[LINEAR_INDEX_IN_RANGE]])
// CHECK-NEXT:    [[TMP3:%.*]] = udiv i64 [[LINEAR_INDEX]], 1
// CHECK-NEXT:    [[TMP4:%.*]] = urem i64 [[TMP3]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = udiv i64 [[LINEAR_INDEX]], 2
// CHECK-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    br i1 [[TMP6]], label [[SORT_IN_BOUNDS_TRUE:%.*]], label [[SORT_IN_BOUNDS_AFTER:%.*]]
// CHECK:       sort.in_bounds-after:
// CHECK-NEXT:    ret void
// CHECK:       sort.in_bounds-true:
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP4]], 3
// CHECK-NEXT:    [[TMP8:%.*]] = icmp slt i64 [[TMP4]], [[TMP7]]
// CHECK-NEXT:    [[TMP9:%.*]] = icmp slt i64 [[TMP7]], 3
// CHECK-NEXT:    [[TMP10:%.*]] = and i1 [[TMP8]], [[TMP9]]
// CHECK-NEXT:    br i1 [[TMP10]], label [[SMALLER_COMPARISON_INDEX_TRUE:%.*]], label [[SMALLER_COMPARISON_INDEX_AFTER:%.*]]
// CHECK:       smaller_comparison_index-after:
// CHECK-NEXT:    br label [[SORT_IN_BOUNDS_AFTER]]
// CHECK:       smaller_comparison_index-true:
// CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP4]]
// CHECK-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP4]]
// CHECK-NEXT:    call void @compare(i32* [[TMP11]], i32* [[TMP12]], float* [[TMP13]], float* [[TMP14]], i8* [[COMPARE_RETURN_BUFFER]])
// CHECK-NEXT:    [[TMP15:%.*]] = load i8, i8* [[COMPARE_RETURN_BUFFER]], align 1
// CHECK-NEXT:    [[BOOLEAN_PREDICATE:%.*]] = icmp ne i8 [[TMP15]], 0
// CHECK-NEXT:    br i1 [[BOOLEAN_PREDICATE]], label [[IS_SMALLER_THAN_TRUE:%.*]], label [[IS_SMALLER_THAN_AFTER:%.*]]
// CHECK:       is_smaller_than-after:
// CHECK-NEXT:    br label [[SMALLER_COMPARISON_INDEX_AFTER]]
// CHECK:       is_smaller_than-true:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, i32* [[TMP11]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, i32* [[TMP12]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP4]]
// CHECK-NEXT:    store i32 [[TMP16]], i32* [[TMP18]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    store i32 [[TMP17]], i32* [[TMP19]], align 4
// CHECK-NEXT:    [[TMP20:%.*]] = load float, float* [[TMP13]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load float, float* [[TMP14]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP4]]
// CHECK-NEXT:    store float [[TMP20]], float* [[TMP22]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP7]]
// CHECK-NEXT:    store float [[TMP21]], float* [[TMP23]], align 4
// CHECK-NEXT:    br label [[IS_SMALLER_THAN_AFTER]]

// CHECK: define void @sort__2(i8* noalias align 64 dereferenceable(24) [[ALLOC0:%.*]], i8* noalias align 64 dereferenceable(24) [[ALLOC1:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC2:%.*]], i8* noalias align 16 dereferenceable(24) [[ALLOC3:%.*]], i8* noalias align 64 dereferenceable(16) [[ALLOC4:%.*]])
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[COMPARE_RETURN_BUFFER:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[SORT_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC4:%.*]], i64 0
// CHECK-NEXT:    [[SORT_TYPED:%.*]] = bitcast i8* [[SORT_RAW]] to [2 x i8*]*
// CHECK-NEXT:    [[SORT_RAW1:%.*]] = getelementptr inbounds i8, i8* [[ALLOC0:%.*]], i64 0
// CHECK-NEXT:    [[SORT_TYPED2:%.*]] = bitcast i8* [[SORT_RAW1]] to [2 x [3 x i32]]*
// CHECK-NEXT:    [[SORT_RAW3:%.*]] = getelementptr inbounds i8, i8* [[ALLOC1:%.*]], i64 0
// CHECK-NEXT:    [[SORT_TYPED4:%.*]] = bitcast i8* [[SORT_RAW3]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[X_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC2:%.*]], i64 0
// CHECK-NEXT:    [[X_TYPED:%.*]] = bitcast i8* [[X_RAW]] to [2 x [3 x i32]]*
// CHECK-NEXT:    [[Y_RAW:%.*]] = getelementptr inbounds i8, i8* [[ALLOC3:%.*]], i64 0
// CHECK-NEXT:    [[Y_TYPED:%.*]] = bitcast i8* [[Y_RAW]] to [2 x [3 x float]]*
// CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK-NEXT:    [[BLOCK_ID:%.*]] = zext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK-NEXT:    [[THREAD_ID:%.*]] = zext i32 [[TMP1]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = mul nuw nsw i64 [[BLOCK_ID]], 4
// CHECK-NEXT:    [[LINEAR_INDEX:%.*]] = add nuw nsw i64 [[TMP2]], [[THREAD_ID]]
// CHECK-NEXT:    [[LINEAR_INDEX_IN_RANGE:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    call void @llvm.assume(i1 [[LINEAR_INDEX_IN_RANGE]])
// CHECK-NEXT:    [[TMP3:%.*]] = udiv i64 [[LINEAR_INDEX]], 1
// CHECK-NEXT:    [[TMP4:%.*]] = urem i64 [[TMP3]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = udiv i64 [[LINEAR_INDEX]], 2
// CHECK-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[LINEAR_INDEX]], 4
// CHECK-NEXT:    br i1 [[TMP6]], label [[SORT_IN_BOUNDS_TRUE:%.*]], label [[SORT_IN_BOUNDS_AFTER:%.*]]
// CHECK:       sort.in_bounds-after:
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast [2 x [3 x i32]]* [[SORT_TYPED2]] to i8*
// CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [2 x i8*], [2 x i8*]* [[SORT_TYPED]], i64 0, i64 0
// CHECK-NEXT:    store i8* [[TMP7]], i8** [[TMP8]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast [2 x [3 x float]]* [[SORT_TYPED4]] to i8*
// CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [2 x i8*], [2 x i8*]* [[SORT_TYPED]], i64 0, i64 1
// CHECK-NEXT:    store i8* [[TMP9]], i8** [[TMP10]], align 8
// CHECK-NEXT:    ret void
// CHECK:       sort.in_bounds-true:
// CHECK-NEXT:    [[TMP11:%.*]] = mul i64 [[TMP4]], 2
// CHECK-NEXT:    [[TMP12:%.*]] = xor i64 [[TMP11]], 1
// CHECK-NEXT:    [[TMP13:%.*]] = icmp slt i64 [[TMP11]], [[TMP12]]
// CHECK-NEXT:    [[TMP14:%.*]] = icmp slt i64 [[TMP12]], 3
// CHECK-NEXT:    [[TMP15:%.*]] = and i1 [[TMP13]], [[TMP14]]
// CHECK-NEXT:    br i1 [[TMP15]], label [[SMALLER_COMPARISON_INDEX_TRUE:%.*]], label [[SMALLER_COMPARISON_INDEX_AFTER:%.*]]
// CHECK:       smaller_comparison_index-after:
// CHECK-NEXT:    br label [[SORT_IN_BOUNDS_AFTER]]
// CHECK:       smaller_comparison_index-true:
// CHECK-NEXT:    [[TMP16:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP12]]
// CHECK-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP11]]
// CHECK-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP12]]
// CHECK-NEXT:    [[TMP19:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP11]]
// CHECK-NEXT:    call void @compare(i32* [[TMP16]], i32* [[TMP17]], float* [[TMP18]], float* [[TMP19]], i8* [[COMPARE_RETURN_BUFFER]])
// CHECK-NEXT:    [[TMP20:%.*]] = load i8, i8* [[COMPARE_RETURN_BUFFER]], align 1
// CHECK-NEXT:    [[BOOLEAN_PREDICATE:%.*]] = icmp ne i8 [[TMP20]], 0
// CHECK-NEXT:    br i1 [[BOOLEAN_PREDICATE]], label [[IS_SMALLER_THAN_TRUE:%.*]], label [[IS_SMALLER_THAN_AFTER:%.*]]
// CHECK:       is_smaller_than-after:
// CHECK-NEXT:    br label [[SMALLER_COMPARISON_INDEX_AFTER]]
// CHECK:       is_smaller_than-true:
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, i32* [[TMP16]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, i32* [[TMP17]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP11]]
// CHECK-NEXT:    store i32 [[TMP21]], i32* [[TMP23]], align 4
// CHECK-NEXT:    [[TMP24:%.*]] = getelementptr inbounds [2 x [3 x i32]], [2 x [3 x i32]]* [[SORT_TYPED2]], i64 0, i64 [[TMP5]], i64 [[TMP12]]
// CHECK-NEXT:    store i32 [[TMP22]], i32* [[TMP24]], align 4
// CHECK-NEXT:    [[TMP25:%.*]] = load float, float* [[TMP18]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = load float, float* [[TMP19]], align 4
// CHECK-NEXT:    [[TMP27:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP11]]
// CHECK-NEXT:    store float [[TMP25]], float* [[TMP27]], align 4
// CHECK-NEXT:    [[TMP28:%.*]] = getelementptr inbounds [2 x [3 x float]], [2 x [3 x float]]* [[SORT_TYPED4]], i64 0, i64 [[TMP5]], i64 [[TMP12]]
// CHECK-NEXT:    store float [[TMP26]], float* [[TMP28]], align 4
// CHECK-NEXT:    br label [[IS_SMALLER_THAN_AFTER]]
ENTRY main {
  x = s32[2, 3] parameter(0)
  y = f32[2, 3] parameter(1)
  ROOT sort = (s32[2, 3], f32[2, 3]) sort(x, y), dimensions={1}, to_apply=compare
}
