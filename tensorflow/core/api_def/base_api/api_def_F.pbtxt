op {
  graph_op_name: "FFT"
  endpoint {
    name: "FFT"
  }
  summary: "Fast Fourier transform."
  description: <<END
Computes the 1-dimensional discrete Fourier transform over the inner-most
dimension of `input`.
END
}
op {
  graph_op_name: "FFT2D"
  endpoint {
    name: "FFT2D"
  }
  summary: "2D fast Fourier transform."
  description: <<END
Computes the 2-dimensional discrete Fourier transform over the inner-most
2 dimensions of `input`.
END
}
op {
  graph_op_name: "FFT3D"
  endpoint {
    name: "FFT3D"
  }
  summary: "3D fast Fourier transform."
  description: <<END
Computes the 3-dimensional discrete Fourier transform over the inner-most 3
dimensions of `input`.
END
}
op {
  graph_op_name: "FIFOQueue"
  endpoint {
    name: "FIFOQueue"
  }
  summary: "A queue that produces elements in first-in first-out order."
}
op {
  graph_op_name: "FIFOQueueV2"
  endpoint {
    name: "FIFOQueueV2"
  }
  summary: "A queue that produces elements in first-in first-out order."
}
op {
  graph_op_name: "Fact"
  endpoint {
    name: "Fact"
  }
  summary: "Output a fact about factorials."
}
op {
  graph_op_name: "FakeQuantWithMinMaxArgs"
  endpoint {
    name: "FakeQuantWithMinMaxArgs"
  }
  summary: "Fake-quantize the \'inputs\' tensor, type float to \'outputs\' tensor of same type."
  description: <<END
Attributes `[min; max]` define the clamping range for the `inputs` data.
`inputs` values are quantized into the quantization range (`[0; 2^num_bits - 1]`
when `narrow_range` is false and `[1; 2^num_bits - 1]` when it is true) and
then de-quantized and output as floats in `[min; max]` interval.
`num_bits` is the bitwidth of the quantization; between 2 and 8, inclusive.

Quantization is called fake since the output is still in floating point.
END
}
op {
  graph_op_name: "FakeQuantWithMinMaxArgsGradient"
  endpoint {
    name: "FakeQuantWithMinMaxArgsGradient"
  }
  summary: "Compute gradients for a FakeQuantWithMinMaxArgs operation."
}
op {
  graph_op_name: "FakeQuantWithMinMaxVars"
  endpoint {
    name: "FakeQuantWithMinMaxVars"
  }
  summary: "Fake-quantize the \'inputs\' tensor of type float via global float scalars `min`"
  description: <<END
and `max` to 'outputs' tensor of same shape as `inputs`.

`[min; max]` define the clamping range for the `inputs` data.
`inputs` values are quantized into the quantization range (`[0; 2^num_bits - 1]`
when `narrow_range` is false and `[1; 2^num_bits - 1]` when it is true) and
then de-quantized and output as floats in `[min; max]` interval.
`num_bits` is the bitwidth of the quantization; between 2 and 8, inclusive.

This operation has a gradient and thus allows for training `min` and `max`
values.
END
}
op {
  graph_op_name: "FakeQuantWithMinMaxVarsGradient"
  endpoint {
    name: "FakeQuantWithMinMaxVarsGradient"
  }
  summary: "Compute gradients for a FakeQuantWithMinMaxVars operation."
}
op {
  graph_op_name: "FakeQuantWithMinMaxVarsPerChannel"
  endpoint {
    name: "FakeQuantWithMinMaxVarsPerChannel"
  }
  summary: "Fake-quantize the \'inputs\' tensor of type float and one of the shapes: `[d]`,"
  description: <<END
`[b, d]` `[b, h, w, d]` via per-channel floats `min` and `max` of shape `[d]`
to 'outputs' tensor of same shape as `inputs`.

`[min; max]` define the clamping range for the `inputs` data.
`inputs` values are quantized into the quantization range (`[0; 2^num_bits - 1]`
when `narrow_range` is false and `[1; 2^num_bits - 1]` when it is true) and
then de-quantized and output as floats in `[min; max]` interval.
`num_bits` is the bitwidth of the quantization; between 2 and 8, inclusive.

This operation has a gradient and thus allows for training `min` and `max`
values.
END
}
op {
  graph_op_name: "FakeQuantWithMinMaxVarsPerChannelGradient"
  endpoint {
    name: "FakeQuantWithMinMaxVarsPerChannelGradient"
  }
  summary: "Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation."
}
op {
  graph_op_name: "FakeQueue"
  endpoint {
    name: "FakeQueue"
  }
  summary: "Deprecated. Do not use."
}
op {
  graph_op_name: "Fill"
  endpoint {
    name: "Fill"
  }
  summary: "Creates a tensor filled with a scalar value."
  description: <<END
This operation creates a tensor of shape `dims` and fills it with `value`.

For example:

```
# Output tensor has shape [2, 3].
fill([2, 3], 9) ==> [[9, 9, 9]
                     [9, 9, 9]]
```
END
}
op {
  graph_op_name: "FilterDataset"
  endpoint {
    name: "FilterDataset"
  }
  summary: "Creates a dataset containing elements of `input_dataset` matching `predicate`."
  description: <<END
The `predicate` function must return a scalar boolean and accept the
following arguments:

* One tensor for each component of an element of `input_dataset`.
* One tensor for each value in `other_arguments`.
END
}
op {
  graph_op_name: "FixedLengthRecordDataset"
  endpoint {
    name: "FixedLengthRecordDataset"
  }
  summary: "Creates a dataset that emits the records from one or more binary files."
}
op {
  graph_op_name: "FixedLengthRecordReader"
  endpoint {
    name: "FixedLengthRecordReader"
  }
  summary: "A Reader that outputs fixed-length records from a file."
}
op {
  graph_op_name: "FixedLengthRecordReaderV2"
  endpoint {
    name: "FixedLengthRecordReaderV2"
  }
  summary: "A Reader that outputs fixed-length records from a file."
}
op {
  graph_op_name: "FixedUnigramCandidateSampler"
  endpoint {
    name: "FixedUnigramCandidateSampler"
  }
  summary: "Generates labels for candidate sampling with a learned unigram distribution."
  description: <<END
A unigram sampler could use a fixed unigram distribution read from a
file or passed in as an in-memory array instead of building up the distribution
from data on the fly. There is also an option to skew the distribution by
applying a distortion power to the weights.

The vocabulary file should be in CSV-like format, with the last field
being the weight associated with the word.

For each batch, this op picks a single set of sampled candidate labels.

The advantages of sampling candidates per-batch are simplicity and the
possibility of efficient dense matrix multiplication. The disadvantage is that
the sampled candidates must be chosen independently of the context and of the
true labels.
END
}
op {
  graph_op_name: "FlatMapDataset"
  endpoint {
    name: "FlatMapDataset"
  }
  summary: "Creates a dataset that applies `f` to the outputs of `input_dataset`."
  description: <<END
Unlike MapDataset, the `f` in FlatMapDataset is expected to return a
Dataset variant, and FlatMapDataset will flatten successive results
into a single Dataset.
END
}
op {
  graph_op_name: "Floor"
  endpoint {
    name: "Floor"
  }
  summary: "Returns element-wise largest integer not greater than x."
}
op {
  graph_op_name: "FloorDiv"
  endpoint {
    name: "FloorDiv"
  }
  summary: "Returns x // y element-wise."
  description: <<END
*NOTE*: `FloorDiv` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
END
}
op {
  graph_op_name: "FloorMod"
  endpoint {
    name: "FloorMod"
  }
  summary: "Returns element-wise remainder of division. When `x < 0` xor `y < 0` is"
  description: <<END
true, this follows Python semantics in that the result here is consistent
with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.

*NOTE*: `FloorMod` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
END
}
op {
  graph_op_name: "FractionalAvgPool"
  endpoint {
    name: "FractionalAvgPool"
  }
  summary: "Performs fractional average pooling on the input."
  description: <<END
Fractional average pooling is similar to Fractional max pooling in the pooling
region generation step. The only difference is that after pooling regions are
generated, a mean operation is performed instead of a max operation in each
pooling region.
END
}
op {
  graph_op_name: "FractionalAvgPoolGrad"
  endpoint {
    name: "FractionalAvgPoolGrad"
  }
  summary: "Computes gradient of the FractionalAvgPool function."
  description: <<END
Unlike FractionalMaxPoolGrad, we don't need to find arg_max for
FractionalAvgPoolGrad, we just need to evenly back-propagate each element of
out_backprop to those indices that form the same pooling cell. Therefore, we
just need to know the shape of original input tensor, instead of the whole
tensor.
END
}
op {
  graph_op_name: "FractionalMaxPool"
  endpoint {
    name: "FractionalMaxPool"
  }
  summary: "Performs fractional max pooling on the input."
  description: <<END
Fractional max pooling is slightly different than regular max pooling.  In
regular max pooling, you downsize an input set by taking the maximum value of
smaller N x N subsections of the set (often 2x2), and try to reduce the set by
a factor of N, where N is an integer.  Fractional max pooling, as you might
expect from the word "fractional", means that the overall reduction ratio N
does not have to be an integer.

The sizes of the pooling regions are generated randomly but are fairly uniform.
For example, let's look at the height dimension, and the constraints on the
list of rows that will be pool boundaries.

First we define the following:

1.  input_row_length : the number of rows from the input set
2.  output_row_length : which will be smaller than the input
3.  alpha = input_row_length / output_row_length : our reduction ratio
4.  K = floor(alpha)
5.  row_pooling_sequence : this is the result list of pool boundary rows

Then, row_pooling_sequence should satisfy:

1.  a[0] = 0 : the first value of the sequence is 0
2.  a[end] = input_row_length : the last value of the sequence is the size
3.  K <= (a[i+1] - a[i]) <= K+1 : all intervals are K or K+1 size
4.  length(row_pooling_sequence) = output_row_length+1

For more details on fractional max pooling, see this paper:
[Benjamin Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071)
END
}
op {
  graph_op_name: "FractionalMaxPoolGrad"
  endpoint {
    name: "FractionalMaxPoolGrad"
  }
  summary: "Computes gradient of the FractionalMaxPool function."
}
op {
  graph_op_name: "FusedBatchNorm"
  endpoint {
    name: "FusedBatchNorm"
  }
  summary: "Batch normalization."
  description: <<END
Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.
END
}
op {
  graph_op_name: "FusedBatchNormGrad"
  endpoint {
    name: "FusedBatchNormGrad"
  }
  summary: "Gradient for batch normalization."
  description: <<END
Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.
END
}
op {
  graph_op_name: "FusedBatchNormGradV2"
  endpoint {
    name: "FusedBatchNormGradV2"
  }
  summary: "Gradient for batch normalization."
  description: <<END
Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.
END
}
op {
  graph_op_name: "FusedBatchNormV2"
  endpoint {
    name: "FusedBatchNormV2"
  }
  summary: "Batch normalization."
  description: <<END
Note that the size of 4D Tensors are defined by either "NHWC" or "NCHW".
The size of 1D Tensors matches the dimension C of the 4D Tensors.
END
}
op {
  graph_op_name: "FusedPadConv2D"
  endpoint {
    name: "FusedPadConv2D"
  }
  summary: "Performs a padding as a preprocess during a convolution."
  description: <<END
Similar to FusedResizeAndPadConv2d, this op allows for an optimized
implementation where the spatial padding transformation stage is fused with the
im2col lookup, but in this case without the bilinear filtering required for
resizing. Fusing the padding prevents the need to write out the intermediate
results as whole tensors, reducing memory pressure, and we can get some latency
gains by merging the transformation calculations.
The data_format attribute for Conv2D isn't supported by this op, and 'NHWC'
order is used instead.
Internally this op uses a single per-graph scratch buffer, which means that it
will block if multiple versions are being run in parallel. This is because this
operator is primarily an optimization to minimize memory usage.
END
}
op {
  graph_op_name: "FusedResizeAndPadConv2D"
  endpoint {
    name: "FusedResizeAndPadConv2D"
  }
  summary: "Performs a resize and padding as a preprocess during a convolution."
  description: <<END
It's often possible to do spatial transformations more efficiently as part of
the packing stage of a convolution, so this op allows for an optimized
implementation where these stages are fused together. This prevents the need to
write out the intermediate results as whole tensors, reducing memory pressure,
and we can get some latency gains by merging the transformation calculations.
The data_format attribute for Conv2D isn't supported by this op, and defaults to
'NHWC' order.
Internally this op uses a single per-graph scratch buffer, which means that it
will block if multiple versions are being run in parallel. This is because this
operator is primarily an optimization to minimize memory usage.
END
}
