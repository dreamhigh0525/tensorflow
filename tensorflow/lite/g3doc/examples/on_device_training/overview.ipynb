{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "g_nWetWWd_ns",
      "metadata": {
        "id": "g_nWetWWd_ns"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2pHVBk_seED1",
      "metadata": {
        "cellView": "form",
        "id": "2pHVBk_seED1"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M7vSdG6sAIQn",
      "metadata": {
        "id": "M7vSdG6sAIQn"
      },
      "source": [
        "# On-Device Training in TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fwc5GKHBASdc",
      "metadata": {
        "id": "fwc5GKHBASdc"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/lite/examples/on_device_training/overview\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee074e4",
      "metadata": {
        "id": "9ee074e4"
      },
      "source": [
        "**Note:** This API is new and only available via `pip install tf-nightly`. It will be available in TensorFlow version 2.7.\n",
        "\n",
        "To use TensorFlow Lite, a developer needs to prepare a TensorFlow model, use the converter to convert it to TensorFlow Lite model format, and run the model with the TensorFlow Lite runtime on device. This is true for inference use cases, and a similar flow can be applied to training too.\n",
        "\n",
        "The following code illustrates the high-level flow of preparing a TensorFlow training model, converting it to TensorFlow Lite model and running in TensorFlow Lite runtime for a training use case.\n",
        "\n",
        "The implementation is based on the [Keras classification example](https://www.tensorflow.org/tutorials/keras/classification) in the TensorFlow official guide page."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UaWdLA3fQDK2",
      "metadata": {
        "id": "UaWdLA3fQDK2"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uZJ35RWsQHG2",
      "metadata": {
        "id": "uZJ35RWsQHG2"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow keras\n",
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9j4MGqyKQEo4",
      "metadata": {
        "id": "9j4MGqyKQEo4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_omifE5JKpt4",
      "metadata": {
        "id": "_omifE5JKpt4"
      },
      "source": [
        "# Classify images of clothing\n",
        "\n",
        "This CoLab trains a neural network model to classify images of clothing, like sneakers and shirts.\n",
        "\n",
        "Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:\n",
        "\n",
        "Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "* The `train_images` and `train_labels` arrays are the *training set*â€”the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NDo-K3Mq63iK",
      "metadata": {
        "id": "NDo-K3Mq63iK"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jN1O10csLcVs",
      "metadata": {
        "id": "jN1O10csLcVs"
      },
      "source": [
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CnwN4QoB7BN7",
      "metadata": {
        "id": "CnwN4QoB7BN7"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p0yf3e-zLdUW",
      "metadata": {
        "id": "p0yf3e-zLdUW"
      },
      "source": [
        "\n",
        "\u003ctable\u003e\n",
        "  \u003ctr\u003e\n",
        "    \u003cth\u003eLabel\u003c/th\u003e\n",
        "    \u003cth\u003eClass\u003c/th\u003e\n",
        "  \u003c/tr\u003e\n",
        "  \u003ctr\u003e\n",
        "    \u003ctd\u003e0\u003c/td\u003e\n",
        "    \u003ctd\u003eT-shirt/top\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "  \u003ctr\u003e\n",
        "    \u003ctd\u003e1\u003c/td\u003e\n",
        "    \u003ctd\u003eTrouser\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e2\u003c/td\u003e\n",
        "    \u003ctd\u003ePullover\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e3\u003c/td\u003e\n",
        "    \u003ctd\u003eDress\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e4\u003c/td\u003e\n",
        "    \u003ctd\u003eCoat\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e5\u003c/td\u003e\n",
        "    \u003ctd\u003eSandal\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e6\u003c/td\u003e\n",
        "    \u003ctd\u003eShirt\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e7\u003c/td\u003e\n",
        "    \u003ctd\u003eSneaker\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e8\u003c/td\u003e\n",
        "    \u003ctd\u003eBag\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "    \u003ctr\u003e\n",
        "    \u003ctd\u003e9\u003c/td\u003e\n",
        "    \u003ctd\u003eAnkle boot\u003c/td\u003e\n",
        "  \u003c/tr\u003e\n",
        "\u003c/table\u003e\n",
        "\n",
        "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l610mvhA695s",
      "metadata": {
        "id": "l610mvhA695s"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wz7l27Lz9S1P",
      "metadata": {
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the *training set* and the *testing set* be preprocessed in the same way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w3OkzF7O7Hr8",
      "metadata": {
        "id": "w3OkzF7O7Hr8"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NgqvO8Eh7NFx",
      "metadata": {
        "id": "NgqvO8Eh7NFx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FN2N6hPEP-Ay",
      "metadata": {
        "id": "FN2N6hPEP-Ay"
      },
      "source": [
        "## TensorFlow Model for Training\n",
        "\n",
        "Instead of converting a single TensorFlow model or tf.function to a TensorFlow Lite model with a single entry point, we can convert multiple tf.function(s) into a TensorFlow Lite model. To be able to do that, we're extending the TensorFlow Lite's converter \u0026 runtime to handle multiple signatures.\n",
        "\n",
        "Preparing a TensorFlow Model. The code constructs a tf.module with 4 tf.functions:\n",
        "*   train function trains the model with training data.\n",
        "*   infer function invokes the inference.\n",
        "*   save function saves the trainable weights into the file system.\n",
        "*   restore function loads the trainable weights from the file system.\n",
        "\n",
        "The weights will be serialized as a TensorFlow version one checkpoint file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8577c80",
      "metadata": {
        "id": "d8577c80"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 28\n",
        "\n",
        "\n",
        "class Model(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    self.model.compile(\n",
        "        optimizer='sgd',\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy'])\n",
        "    self._LOSS_FN = tf.keras.losses.CategoricalCrossentropy()\n",
        "    self._OPTIM = tf.optimizers.SGD()\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
        "      tf.TensorSpec([None, 10], tf.float32),\n",
        "  ])\n",
        "  def train(self, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "      prediction = self.model(x)\n",
        "      loss = self._LOSS_FN(prediction, y)\n",
        "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self._OPTIM.apply_gradients(\n",
        "        zip(gradients, self.model.trainable_variables))\n",
        "    return gradients\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32)])\n",
        "  def predict(self, x):\n",
        "    return {\n",
        "        \"output\": self.model(x)\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def save(self, checkpoint_path):\n",
        "    tensor_names = [weight.name for weight in self.model.weights]\n",
        "    tensors_to_save = [weight.read_value() for weight in self.model.weights]\n",
        "    tf.raw_ops.Save(\n",
        "        filename=checkpoint_path, tensor_names=tensor_names,\n",
        "        data=tensors_to_save, name='save')\n",
        "    return {\n",
        "        \"checkpoint_path\": checkpoint_path\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def restore(self, checkpoint_path):\n",
        "    restored_tensors = {}\n",
        "    for var in self.model.weights:\n",
        "      restored = tf.raw_ops.Restore(\n",
        "          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\n",
        "          name='restore')\n",
        "      var.assign(restored)\n",
        "      restored_tensors[var.name] = restored\n",
        "    return restored_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A8YUTIvzMVw5",
      "metadata": {
        "id": "A8YUTIvzMVw5"
      },
      "source": [
        "## Converting to TensorFlow Lite format.\n",
        "\n",
        "Note the TFLiteConverter API will be extended to choose which functions / signatures to convert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WwsDUEKFMYtq",
      "metadata": {
        "id": "WwsDUEKFMYtq"
      },
      "outputs": [],
      "source": [
        "# Export the TensorFlow model to the saved model\n",
        "SAVED_MODEL_DIR = \"/content/saved_model\"\n",
        "m= Model()\n",
        "tf.saved_model.save(\n",
        "    m,\n",
        "    SAVED_MODEL_DIR,\n",
        "    signatures={\n",
        "        'train':\n",
        "            m.train.get_concrete_function(),\n",
        "        'infer':\n",
        "            m.predict.get_concrete_function(),\n",
        "        'save':\n",
        "            m.save.get_concrete_function(),\n",
        "        'restore':\n",
        "            m.restore.get_concrete_function(),\n",
        "    })\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
        "]\n",
        "converter.experimental_enable_resource_variables = True\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B-mXb38wM6Nf",
      "metadata": {
        "id": "B-mXb38wM6Nf"
      },
      "source": [
        "## Executing in TensorFlow Lite runtime.\n",
        "\n",
        "TensorFlow Lite's Interpreter capability will be extended to support multiple signatures too. Developers can choose to invoke restoring, training, saving and inferring signatures separately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qNX2vqXd2-HM",
      "metadata": {
        "id": "qNX2vqXd2-HM"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "\n",
        "train = interpreter.get_signature_runner(\"train\")\n",
        "infer = interpreter.get_signature_runner(\"infer\")\n",
        "save = interpreter.get_signature_runner(\"save\")\n",
        "restore = interpreter.get_signature_runner(\"restore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5jhWXwVCO09F",
      "metadata": {
        "id": "5jhWXwVCO09F"
      },
      "source": [
        "## Training with data set\n",
        "\n",
        "Training can be done with the `train` signature method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Diwn1MmkNVeX",
      "metadata": {
        "id": "Diwn1MmkNVeX"
      },
      "outputs": [],
      "source": [
        "# Generate the training labels.\n",
        "processed_train_labels = []\n",
        "for i in range(len(train_images)):\n",
        "  train_label = [0,0,0,0,0,0,0,0,0,0]\n",
        "  train_label[train_labels[i]] = 1\n",
        "  processed_train_labels.append(train_label)\n",
        "\n",
        "# Run training for a few steps.\n",
	"for i in range(20):\n",
        "  train(\n",
        "      x=tf.constant(train_images, shape=(len(train_images), IMG_SIZE, IMG_SIZE), dtype=tf.float32),\n",
        "      y=tf.constant(processed_train_labels, shape=(len(train_images), 10), dtype=tf.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UDIi0_RlPb2n",
      "metadata": {
        "id": "UDIi0_RlPb2n"
      },
      "source": [
        "## Exporting the trained weights to the checkpoint file\n",
        "\n",
        "The checkpoint file can be generated through the `save` signature method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sgeX9XiXOyex",
      "metadata": {
        "id": "sgeX9XiXOyex"
      },
      "outputs": [],
      "source": [
        "# Export the traind weights to /tmp/model.ckpt\n",
        "save(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SSDydMyOQfL5",
      "metadata": {
        "id": "SSDydMyOQfL5"
      },
      "source": [
        "## Restoring the trained weights from the checkpoint file\n",
        "\n",
        "The exported checkpoint file can be restored through the `restore` signature method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5yIZoLveRZgp",
      "metadata": {
        "id": "5yIZoLveRZgp"
      },
      "outputs": [],
      "source": [
        "another_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "\n",
        "train = another_interpreter.get_signature_runner(\"train\")\n",
        "infer = another_interpreter.get_signature_runner(\"infer\")\n",
        "save = another_interpreter.get_signature_runner(\"save\")\n",
        "restore = another_interpreter.get_signature_runner(\"restore\")\n",
        "\n",
        "# Restore the trained weights from /tmp/model.ckpt\n",
        "restore(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zjcrv57DSkz2",
      "metadata": {
        "id": "zjcrv57DSkz2"
      },
      "source": [
        "## Run the inference through the trained weights\n",
        "\n",
        "Developers can use the trained model to run inference through the `infer` signature method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ROmlpHWS0nX",
      "metadata": {
        "id": "_ROmlpHWS0nX"
      },
      "outputs": [],
      "source": [
        "# Run the inference\n",
        "result = infer(\n",
        "    x=tf.constant(test_images, shape=(len(test_images), IMG_SIZE, IMG_SIZE), dtype=tf.float32))\n",
        "\n",
        "test_lables = np.argmax(result[\"output\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GHbRasdfasd4",
      "metadata": {
        "id": "GHbRasdfasd4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
	"for i in range(25):\n",
	"    plt.subplot(5,5,i+1)\n",
	"    plt.xticks([])\n",
	"    plt.yticks([])\n",
	"    plt.grid(False)\n",
	"    plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
	"    plt.xlabel(class_names[test_labels[i]])\n",
	"plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "On-Device Training in TensorFlow Lite",
      "provenance": [],
      "toc_visible": true
    },
    "id": "a1b42e5b",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
