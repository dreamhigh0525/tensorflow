op {
  graph_op_name: "RFFT"
  endpoint {
    name: "RFFT"
  }
  summary: "Real-valued fast Fourier transform."
  description: <<END
Computes the 1-dimensional discrete Fourier transform of a real-valued signal
over the inner-most dimension of `input`.

Since the DFT of a real signal is Hermitian-symmetric, `RFFT` only returns the
`fft_length / 2 + 1` unique components of the FFT: the zero-frequency term,
followed by the `fft_length / 2` positive-frequency terms.

Along the axis `RFFT` is computed on, if `fft_length` is smaller than the
corresponding dimension of `input`, the dimension is cropped. If it is larger,
the dimension is padded with zeros.
END
}
op {
  graph_op_name: "RFFT2D"
  endpoint {
    name: "RFFT2D"
  }
  summary: "2D real-valued fast Fourier transform."
  description: <<END
Computes the 2-dimensional discrete Fourier transform of a real-valued signal
over the inner-most 2 dimensions of `input`.

Since the DFT of a real signal is Hermitian-symmetric, `RFFT2D` only returns the
`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension
of `output`: the zero-frequency term, followed by the `fft_length / 2`
positive-frequency terms.

Along each axis `RFFT2D` is computed on, if `fft_length` is smaller than the
corresponding dimension of `input`, the dimension is cropped. If it is larger,
the dimension is padded with zeros.
END
}
op {
  graph_op_name: "RFFT3D"
  endpoint {
    name: "RFFT3D"
  }
  summary: "3D real-valued fast Fourier transform."
  description: <<END
Computes the 3-dimensional discrete Fourier transform of a real-valued signal
over the inner-most 3 dimensions of `input`.

Since the DFT of a real signal is Hermitian-symmetric, `RFFT3D` only returns the
`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension
of `output`: the zero-frequency term, followed by the `fft_length / 2`
positive-frequency terms.

Along each axis `RFFT3D` is computed on, if `fft_length` is smaller than the
corresponding dimension of `input`, the dimension is cropped. If it is larger,
the dimension is padded with zeros.
END
}
op {
  graph_op_name: "RGBToHSV"
  endpoint {
    name: "RGBToHSV"
  }
  summary: "Converts one or more images from RGB to HSV."
  description: <<END
Outputs a tensor of the same shape as the `images` tensor, containing the HSV
value of the pixels. The output is only well defined if the value in `images`
are in `[0,1]`.

`output[..., 0]` contains hue, `output[..., 1]` contains saturation, and
`output[..., 2]` contains value. All HSV values are in `[0,1]`. A hue of 0
corresponds to pure red, hue 1/3 is pure green, and 2/3 is pure blue.
END
}
op {
  graph_op_name: "RandomCrop"
  endpoint {
    name: "RandomCrop"
  }
  summary: "Randomly crop `image`."
  description: <<END
`size` is a 1-D int64 tensor with 2 elements representing the crop height and
width.  The values must be non negative.

This Op picks a random location in `image` and crops a `height` by `width`
rectangle from that location.  The random location is picked so the cropped
area will fit inside the original image.
END
}
op {
  graph_op_name: "RandomGamma"
  endpoint {
    name: "RandomGamma"
  }
  summary: "Outputs random values from the Gamma distribution(s) described by alpha."
  description: <<END
This op uses the algorithm by Marsaglia et al. to acquire samples via
transformation-rejection from pairs of uniform and normal random variables.
See http://dl.acm.org/citation.cfm?id=358414
END
}
op {
  graph_op_name: "RandomPoisson"
  endpoint {
    name: "RandomPoisson"
  }
  summary: "Outputs random values from the Poisson distribution(s) described by rate."
  description: <<END
This op uses two algorithms, depending on rate. If rate >= 10, then
the algorithm by Hormann is used to acquire samples via
transformation-rejection.
See http://www.sciencedirect.com/science/article/pii/0167668793909974.

Otherwise, Knuth's algorithm is used to acquire samples via multiplying uniform
random variables.
See Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer
Programming, Volume 2. Addison Wesley
END
}
op {
  graph_op_name: "RandomPoissonV2"
  endpoint {
    name: "RandomPoissonV2"
  }
  summary: "Outputs random values from the Poisson distribution(s) described by rate."
  description: <<END
This op uses two algorithms, depending on rate. If rate >= 10, then
the algorithm by Hormann is used to acquire samples via
transformation-rejection.
See http://www.sciencedirect.com/science/article/pii/0167668793909974.

Otherwise, Knuth's algorithm is used to acquire samples via multiplying uniform
random variables.
See Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer
Programming, Volume 2. Addison Wesley
END
}
op {
  graph_op_name: "RandomShuffle"
  endpoint {
    name: "RandomShuffle"
  }
  summary: "Randomly shuffles a tensor along its first dimension."
  description: <<END
  The tensor is shuffled along dimension 0, such that each `value[j]` is mapped
  to one and only one `output[i]`. For example, a mapping that might occur for a
  3x2 tensor is:

```
[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
```
END
}
op {
  graph_op_name: "RandomShuffleQueue"
  endpoint {
    name: "RandomShuffleQueue"
  }
  summary: "A queue that randomizes the order of elements."
}
op {
  graph_op_name: "RandomShuffleQueueV2"
  endpoint {
    name: "RandomShuffleQueueV2"
  }
  summary: "A queue that randomizes the order of elements."
}
op {
  graph_op_name: "RandomStandardNormal"
  endpoint {
    name: "RandomStandardNormal"
  }
  summary: "Outputs random values from a normal distribution."
  description: <<END
The generated values will have mean 0 and standard deviation 1.
END
}
op {
  graph_op_name: "RandomUniform"
  endpoint {
    name: "RandomUniform"
  }
  summary: "Outputs random values from a uniform distribution."
  description: <<END
The generated values follow a uniform distribution in the range `[0, 1)`. The
lower bound 0 is included in the range, while the upper bound 1 is excluded.
END
}
op {
  graph_op_name: "RandomUniformInt"
  endpoint {
    name: "RandomUniformInt"
  }
  summary: "Outputs random integers from a uniform distribution."
  description: <<END
The generated values are uniform integers in the range `[minval, maxval)`.
The lower bound `minval` is included in the range, while the upper bound
`maxval` is excluded.

The random integers are slightly biased unless `maxval - minval` is an exact
power of two.  The bias is small for values of `maxval - minval` significantly
smaller than the range of the output (either `2^32` or `2^64`).
END
}
op {
  graph_op_name: "Range"
  endpoint {
    name: "Range"
  }
  summary: "Creates a sequence of numbers."
  description: <<END
This operation creates a sequence of numbers that begins at `start` and
extends by increments of `delta` up to but not including `limit`.

For example:

```
# 'start' is 3
# 'limit' is 18
# 'delta' is 3
tf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]
```
END
}
op {
  graph_op_name: "RangeDataset"
  endpoint {
    name: "RangeDataset"
  }
  summary: "Creates a dataset with a range of values. Corresponds to python\'s xrange."
}
op {
  graph_op_name: "Rank"
  endpoint {
    name: "Rank"
  }
  summary: "Returns the rank of a tensor."
  description: <<END
This operation returns an integer representing the rank of `input`.

For example:

```
# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]
# shape of tensor 't' is [2, 2, 3]
rank(t) ==> 3
```

**Note**: The rank of a tensor is not the same as the rank of a matrix. The rank
of a tensor is the number of indices required to uniquely select each element
of the tensor. Rank is also known as "order", "degree", or "ndims."
END
}
op {
  graph_op_name: "ReadFile"
  endpoint {
    name: "ReadFile"
  }
  summary: "Reads and outputs the entire contents of the input filename."
}
op {
  graph_op_name: "ReaderNumRecordsProduced"
  endpoint {
    name: "ReaderNumRecordsProduced"
  }
  summary: "Returns the number of records this Reader has produced."
  description: <<END
This is the same as the number of ReaderRead executions that have
succeeded.
END
}
op {
  graph_op_name: "ReaderNumRecordsProducedV2"
  endpoint {
    name: "ReaderNumRecordsProducedV2"
  }
  summary: "Returns the number of records this Reader has produced."
  description: <<END
This is the same as the number of ReaderRead executions that have
succeeded.
END
}
op {
  graph_op_name: "ReaderNumWorkUnitsCompleted"
  endpoint {
    name: "ReaderNumWorkUnitsCompleted"
  }
  summary: "Returns the number of work units this Reader has finished processing."
}
op {
  graph_op_name: "ReaderNumWorkUnitsCompletedV2"
  endpoint {
    name: "ReaderNumWorkUnitsCompletedV2"
  }
  summary: "Returns the number of work units this Reader has finished processing."
}
op {
  graph_op_name: "ReaderRead"
  endpoint {
    name: "ReaderRead"
  }
  summary: "Returns the next record (key, value pair) produced by a Reader."
  description: <<END
Will dequeue from the input queue if necessary (e.g. when the
Reader needs to start reading from a new file since it has finished
with the previous file).
END
}
op {
  graph_op_name: "ReaderReadUpTo"
  endpoint {
    name: "ReaderReadUpTo"
  }
  summary: "Returns up to `num_records` (key, value) pairs produced by a Reader."
  description: <<END
Will dequeue from the input queue if necessary (e.g. when the
Reader needs to start reading from a new file since it has finished
with the previous file).
It may return less than `num_records` even before the last batch.
END
}
op {
  graph_op_name: "ReaderReadUpToV2"
  endpoint {
    name: "ReaderReadUpToV2"
  }
  summary: "Returns up to `num_records` (key, value) pairs produced by a Reader."
  description: <<END
Will dequeue from the input queue if necessary (e.g. when the
Reader needs to start reading from a new file since it has finished
with the previous file).
It may return less than `num_records` even before the last batch.
END
}
op {
  graph_op_name: "ReaderReadV2"
  endpoint {
    name: "ReaderReadV2"
  }
  summary: "Returns the next record (key, value pair) produced by a Reader."
  description: <<END
Will dequeue from the input queue if necessary (e.g. when the
Reader needs to start reading from a new file since it has finished
with the previous file).
END
}
op {
  graph_op_name: "ReaderReset"
  endpoint {
    name: "ReaderReset"
  }
  summary: "Restore a Reader to its initial clean state."
}
op {
  graph_op_name: "ReaderResetV2"
  endpoint {
    name: "ReaderResetV2"
  }
  summary: "Restore a Reader to its initial clean state."
}
op {
  graph_op_name: "ReaderRestoreState"
  endpoint {
    name: "ReaderRestoreState"
  }
  summary: "Restore a reader to a previously saved state."
  description: <<END
Not all Readers support being restored, so this can produce an
Unimplemented error.
END
}
op {
  graph_op_name: "ReaderRestoreStateV2"
  endpoint {
    name: "ReaderRestoreStateV2"
  }
  summary: "Restore a reader to a previously saved state."
  description: <<END
Not all Readers support being restored, so this can produce an
Unimplemented error.
END
}
op {
  graph_op_name: "ReaderSerializeState"
  endpoint {
    name: "ReaderSerializeState"
  }
  summary: "Produce a string tensor that encodes the state of a Reader."
  description: <<END
Not all Readers support being serialized, so this can produce an
Unimplemented error.
END
}
op {
  graph_op_name: "ReaderSerializeStateV2"
  endpoint {
    name: "ReaderSerializeStateV2"
  }
  summary: "Produce a string tensor that encodes the state of a Reader."
  description: <<END
Not all Readers support being serialized, so this can produce an
Unimplemented error.
END
}
op {
  graph_op_name: "Real"
  endpoint {
    name: "Real"
  }
  summary: "Returns the real part of a complex number."
  description: <<END
Given a tensor `input` of complex numbers, this operation returns a tensor of
type `float` that is the real part of each element in `input`. All elements in
`input` must be complex numbers of the form \\(a + bj\\), where *a* is the real
 part returned by this operation and *b* is the imaginary part.

For example:

```
# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.real(input) ==> [-2.25, 3.25]
```
END
}
op {
  graph_op_name: "RealDiv"
  endpoint {
    name: "RealDiv"
  }
  summary: "Returns x / y element-wise for real types."
  description: <<END
If `x` and `y` are reals, this will return the floating-point division.

*NOTE*: `Div` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
END
}
op {
  graph_op_name: "Reciprocal"
  endpoint {
    name: "Reciprocal"
  }
  summary: "Computes the reciprocal of x element-wise."
  description: <<END
I.e., \\(y = 1 / x\\).
END
}
op {
  graph_op_name: "ReciprocalGrad"
  endpoint {
    name: "ReciprocalGrad"
  }
  summary: "Computes the gradient for the inverse of `x` wrt its input."
  description: <<END
Specifically, `grad = -dy * y*y`, where `y = 1/x`, and `dy`
is the corresponding input gradient.
END
}
op {
  graph_op_name: "RecordInput"
  endpoint {
    name: "RecordInput"
  }
  summary: "Emits randomized records."
}
op {
  graph_op_name: "ReduceJoin"
  endpoint {
    name: "ReduceJoin"
  }
  summary: "Joins a string Tensor across the given dimensions."
  description: <<END
Computes the string join across dimensions in the given string Tensor of shape
`[d_0, d_1, ..., d_n-1]`.  Returns a new Tensor created by joining the input
strings with the given separator (default: empty string).  Negative indices are
counted backwards from the end, with `-1` being equivalent to `n - 1`.

For example:

```python
# tensor `a` is [["a", "b"], ["c", "d"]]
tf.reduce_join(a, 0) ==> ["ac", "bd"]
tf.reduce_join(a, 1) ==> ["ab", "cd"]
tf.reduce_join(a, -2) = tf.reduce_join(a, 0) ==> ["ac", "bd"]
tf.reduce_join(a, -1) = tf.reduce_join(a, 1) ==> ["ab", "cd"]
tf.reduce_join(a, 0, keep_dims=True) ==> [["ac", "bd"]]
tf.reduce_join(a, 1, keep_dims=True) ==> [["ab"], ["cd"]]
tf.reduce_join(a, 0, separator=".") ==> ["a.c", "b.d"]
tf.reduce_join(a, [0, 1]) ==> ["acbd"]
tf.reduce_join(a, [1, 0]) ==> ["abcd"]
tf.reduce_join(a, []) ==> ["abcd"]
```
END
}
op {
  graph_op_name: "RefEnter"
  endpoint {
    name: "RefEnter"
  }
  summary: "Creates or finds a child frame, and makes `data` available to the child frame."
  description: <<END
The unique `frame_name` is used by the `Executor` to identify frames. If
`is_constant` is true, `output` is a constant in the child frame; otherwise
it may be changed in the child frame. At most `parallel_iterations` iterations
are run in parallel in the child frame.
END
}
op {
  graph_op_name: "RefExit"
  endpoint {
    name: "RefExit"
  }
  summary: "Exits the current frame to its parent frame."
  description: <<END
Exit makes its input `data` available to the parent frame.
END
}
op {
  graph_op_name: "RefIdentity"
  endpoint {
    name: "RefIdentity"
  }
  summary: "Return the same ref tensor as the input ref tensor."
}
op {
  graph_op_name: "RefMerge"
  endpoint {
    name: "RefMerge"
  }
  summary: "Forwards the value of an available tensor from `inputs` to `output`."
  description: <<END
`Merge` waits for at least one of the tensors in `inputs` to become available.
It is usually combined with `Switch` to implement branching.

`Merge` forwards the first tensor for become available to `output`, and sets
`value_index` to its index in `inputs`.
END
}
op {
  graph_op_name: "RefNextIteration"
  endpoint {
    name: "RefNextIteration"
  }
  summary: "Makes its input available to the next iteration."
}
op {
  graph_op_name: "RefSelect"
  endpoint {
    name: "RefSelect"
  }
  summary: "Forwards the `index`th element of `inputs` to `output`."
}
op {
  graph_op_name: "RefSwitch"
  endpoint {
    name: "RefSwitch"
  }
  summary: "Forwards the ref tensor `data` to the output port determined by `pred`."
  description: <<END
If `pred` is true, the `data` input is forwarded to `output_true`. Otherwise,
the data goes to `output_false`.

See also `Switch` and `Merge`.
END
}
op {
  graph_op_name: "Relu"
  endpoint {
    name: "Relu"
  }
  summary: "Computes rectified linear: `max(features, 0)`."
}
op {
  graph_op_name: "Relu6"
  endpoint {
    name: "Relu6"
  }
  summary: "Computes rectified linear 6: `min(max(features, 0), 6)`."
}
op {
  graph_op_name: "Relu6Grad"
  endpoint {
    name: "Relu6Grad"
  }
  summary: "Computes rectified linear 6 gradients for a Relu6 operation."
}
op {
  graph_op_name: "ReluGrad"
  endpoint {
    name: "ReluGrad"
  }
  summary: "Computes rectified linear gradients for a Relu operation."
}
op {
  graph_op_name: "RemoteCall"
  endpoint {
    name: "RemoteCall"
  }
  summary: "Runs function `f` on a remote device indicated by `target`."
}
op {
  graph_op_name: "RemoteFusedGraphExecute"
  endpoint {
    name: "RemoteFusedGraphExecute"
  }
  summary: "Execute a sub graph on a remote processor."
  description: <<END
The graph specifications(such as graph itself, input tensors and output names)
are stored as a serialized protocol buffer of RemoteFusedGraphExecuteInfo
as serialized_remote_fused_graph_execute_info.
The specifications will be passed to a dedicated registered
remote fused graph executor.  The executor will send the graph specifications
to a remote processor and execute that graph.  The execution results
will be passed to consumer nodes as outputs of this node.
END
}
op {
  graph_op_name: "RepeatDataset"
  endpoint {
    name: "RepeatDataset"
  }
  summary: "Creates a dataset that emits the outputs of `input_dataset` `count` times."
}
op {
  graph_op_name: "RequantizationRange"
  endpoint {
    name: "RequantizationRange"
  }
  summary: "Given a quantized tensor described by (input, input_min, input_max), outputs a"
  description: <<END
range that covers the actual values present in that tensor.  This op is
typically used to produce the requested_output_min and requested_output_max for
Requantize.
END
}
op {
  graph_op_name: "Requantize"
  endpoint {
    name: "Requantize"
  }
  summary: "Convert the quantized \'input\' tensor into a lower-precision \'output\', using the"
  description: <<END
output range specified with 'requested_output_min' and 'requested_output_max'.

[input_min, input_max] are scalar floats that specify the range for the float
interpretation of the 'input' data. For example, if input_min is -1.0f and
input_max is 1.0f, and we are dealing with quint16 quantized data, then a 0
value in the 16-bit data should be interpreted as -1.0f, and a 65535 means 1.0f.
END
}
op {
  graph_op_name: "Reshape"
  endpoint {
    name: "Reshape"
  }
  summary: "Reshapes a tensor."
  description: <<END
Given `tensor`, this operation returns a tensor that has the same values
as `tensor` with shape `shape`.

If one component of `shape` is the special value -1, the size of that dimension
is computed so that the total size remains constant.  In particular, a `shape`
of `[-1]` flattens into 1-D.  At most one component of `shape` can be -1.

If `shape` is 1-D or higher, then the operation returns a tensor with shape
`shape` filled with the values of `tensor`. In this case, the number of elements
implied by `shape` must be the same as the number of elements in `tensor`.

For example:

```
# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]
# tensor 't' has shape [9]
reshape(t, [3, 3]) ==> [[1, 2, 3],
                        [4, 5, 6],
                        [7, 8, 9]]

# tensor 't' is [[[1, 1], [2, 2]],
#                [[3, 3], [4, 4]]]
# tensor 't' has shape [2, 2, 2]
reshape(t, [2, 4]) ==> [[1, 1, 2, 2],
                        [3, 3, 4, 4]]

# tensor 't' is [[[1, 1, 1],
#                 [2, 2, 2]],
#                [[3, 3, 3],
#                 [4, 4, 4]],
#                [[5, 5, 5],
#                 [6, 6, 6]]]
# tensor 't' has shape [3, 2, 3]
# pass '[-1]' to flatten 't'
reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]

# -1 can also be used to infer the shape

# -1 is inferred to be 9:
reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 2:
reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 3:
reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],
                              [2, 2, 2],
                              [3, 3, 3]],
                             [[4, 4, 4],
                              [5, 5, 5],
                              [6, 6, 6]]]

# tensor 't' is [7]
# shape `[]` reshapes to a scalar
reshape(t, []) ==> 7
```
END
}
op {
  graph_op_name: "ResizeArea"
  endpoint {
    name: "ResizeArea"
  }
  summary: "Resize `images` to `size` using area interpolation."
  description: <<END
Input images can be of different types but output images are always float.

Each output pixel is computed by first transforming the pixel's footprint into
the input tensor and then averaging the pixels that intersect the footprint. An
input pixel's contribution to the average is weighted by the fraction of its
area that intersects the footprint.  This is the same as OpenCV's INTER_AREA.
END
}
op {
  graph_op_name: "ResizeBicubic"
  endpoint {
    name: "ResizeBicubic"
  }
  summary: "Resize `images` to `size` using bicubic interpolation."
  description: <<END
Input images can be of different types but output images are always float.
END
}
op {
  graph_op_name: "ResizeBicubicGrad"
  endpoint {
    name: "ResizeBicubicGrad"
  }
  summary: "Computes the gradient of bicubic interpolation."
}
op {
  graph_op_name: "ResizeBilinear"
  endpoint {
    name: "ResizeBilinear"
  }
  summary: "Resize `images` to `size` using bilinear interpolation."
  description: <<END
Input images can be of different types but output images are always float.
END
}
op {
  graph_op_name: "ResizeBilinearGrad"
  endpoint {
    name: "ResizeBilinearGrad"
  }
  summary: "Computes the gradient of bilinear interpolation."
}
op {
  graph_op_name: "ResizeNearestNeighbor"
  endpoint {
    name: "ResizeNearestNeighbor"
  }
  summary: "Resize `images` to `size` using nearest neighbor interpolation."
}
op {
  graph_op_name: "ResizeNearestNeighborGrad"
  endpoint {
    name: "ResizeNearestNeighborGrad"
  }
  summary: "Computes the gradient of nearest neighbor interpolation."
}
op {
  graph_op_name: "ResourceApplyAdadelta"
  endpoint {
    name: "ResourceApplyAdadelta"
  }
  summary: "Update \'*var\' according to the adadelta scheme."
  description: <<END
accum = rho() * accum + (1 - rho()) * grad.square();
update = (update_accum + epsilon).sqrt() * (accum + epsilon()).rsqrt() * grad;
update_accum = rho() * update_accum + (1 - rho()) * update.square();
var -= update;
END
}
op {
  graph_op_name: "ResourceApplyAdagrad"
  endpoint {
    name: "ResourceApplyAdagrad"
  }
  summary: "Update \'*var\' according to the adagrad scheme."
  description: <<END
accum += grad * grad
var -= lr * grad * (1 / sqrt(accum))
END
}
op {
  graph_op_name: "ResourceApplyAdagradDA"
  endpoint {
    name: "ResourceApplyAdagradDA"
  }
  summary: "Update \'*var\' according to the proximal adagrad scheme."
}
op {
  graph_op_name: "ResourceApplyAdam"
  endpoint {
    name: "ResourceApplyAdam"
  }
  summary: "Update \'*var\' according to the Adam algorithm."
  description: <<END
lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)
m_t <- beta1 * m_{t-1} + (1 - beta1) * g_t
v_t <- beta2 * v_{t-1} + (1 - beta2) * g_t * g_t
variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)
END
}
op {
  graph_op_name: "ResourceApplyCenteredRMSProp"
  endpoint {
    name: "ResourceApplyCenteredRMSProp"
  }
  summary: "Update \'*var\' according to the centered RMSProp algorithm."
  description: <<END
The centered RMSProp algorithm uses an estimate of the centered second moment
(i.e., the variance) for normalization, as opposed to regular RMSProp, which
uses the (uncentered) second moment. This often helps with training, but is
slightly more expensive in terms of computation and memory.

Note that in dense implementation of this algorithm, mg, ms, and mom will
update even if the grad is zero, but in this sparse implementation, mg, ms,
and mom will not update in iterations during which the grad is zero.

mean_square = decay * mean_square + (1-decay) * gradient ** 2
mean_grad = decay * mean_grad + (1-decay) * gradient

Delta = learning_rate * gradient / sqrt(mean_square + epsilon - mean_grad ** 2)

mg <- rho * mg_{t-1} + (1-rho) * grad
ms <- rho * ms_{t-1} + (1-rho) * grad * grad
mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms - mg * mg + epsilon)
var <- var - mom
END
}
op {
  graph_op_name: "ResourceApplyFtrl"
  endpoint {
    name: "ResourceApplyFtrl"
  }
  summary: "Update \'*var\' according to the Ftrl-proximal scheme."
  description: <<END
accum_new = accum + grad * grad
linear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new
END
}
op {
  graph_op_name: "ResourceApplyFtrlV2"
  endpoint {
    name: "ResourceApplyFtrlV2"
  }
  summary: "Update \'*var\' according to the Ftrl-proximal scheme."
  description: <<END
grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad_with_shrinkage * grad_with_shrinkage
linear += grad_with_shrinkage +
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new
END
}
op {
  graph_op_name: "ResourceApplyGradientDescent"
  endpoint {
    name: "ResourceApplyGradientDescent"
  }
  summary: "Update \'*var\' by subtracting \'alpha\' * \'delta\' from it."
}
op {
  graph_op_name: "ResourceApplyMomentum"
  endpoint {
    name: "ResourceApplyMomentum"
  }
  summary: "Update \'*var\' according to the momentum scheme. Set use_nesterov = True if you"
  description: <<END
want to use Nesterov momentum.

accum = accum * momentum + grad
var -= lr * accum
END
}
op {
  graph_op_name: "ResourceApplyProximalAdagrad"
  endpoint {
    name: "ResourceApplyProximalAdagrad"
  }
  summary: "Update \'*var\' and \'*accum\' according to FOBOS with Adagrad learning rate."
  description: <<END
accum += grad * grad
prox_v = var - lr * grad * (1 / sqrt(accum))
var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}
END
}
op {
  graph_op_name: "ResourceApplyProximalGradientDescent"
  endpoint {
    name: "ResourceApplyProximalGradientDescent"
  }
  summary: "Update \'*var\' as FOBOS algorithm with fixed learning rate."
  description: <<END
prox_v = var - alpha * delta
var = sign(prox_v)/(1+alpha*l2) * max{|prox_v|-alpha*l1,0}
END
}
op {
  graph_op_name: "ResourceApplyRMSProp"
  endpoint {
    name: "ResourceApplyRMSProp"
  }
  summary: "Update \'*var\' according to the RMSProp algorithm."
  description: <<END
Note that in dense implementation of this algorithm, ms and mom will
update even if the grad is zero, but in this sparse implementation, ms
and mom will not update in iterations during which the grad is zero.

mean_square = decay * mean_square + (1-decay) * gradient ** 2
Delta = learning_rate * gradient / sqrt(mean_square + epsilon)

ms <- rho * ms_{t-1} + (1-rho) * grad * grad
mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
var <- var - mom
END
}
op {
  graph_op_name: "ResourceSparseApplyAdadelta"
  endpoint {
    name: "ResourceSparseApplyAdadelta"
  }
  summary: "var: Should be from a Variable()."
}
op {
  graph_op_name: "ResourceSparseApplyAdagrad"
  endpoint {
    name: "ResourceSparseApplyAdagrad"
  }
  summary: "Update relevant entries in \'*var\' and \'*accum\' according to the adagrad scheme."
  description: <<END
That is for rows we have grad for, we update var and accum as follows:
accum += grad * grad
var -= lr * grad * (1 / sqrt(accum))
END
}
op {
  graph_op_name: "ResourceSparseApplyAdagradDA"
  endpoint {
    name: "ResourceSparseApplyAdagradDA"
  }
  summary: "Update entries in \'*var\' and \'*accum\' according to the proximal adagrad scheme."
}
op {
  graph_op_name: "ResourceSparseApplyCenteredRMSProp"
  endpoint {
    name: "ResourceSparseApplyCenteredRMSProp"
  }
  summary: "Update \'*var\' according to the centered RMSProp algorithm."
  description: <<END
The centered RMSProp algorithm uses an estimate of the centered second moment
(i.e., the variance) for normalization, as opposed to regular RMSProp, which
uses the (uncentered) second moment. This often helps with training, but is
slightly more expensive in terms of computation and memory.

Note that in dense implementation of this algorithm, mg, ms, and mom will
update even if the grad is zero, but in this sparse implementation, mg, ms,
and mom will not update in iterations during which the grad is zero.

mean_square = decay * mean_square + (1-decay) * gradient ** 2
mean_grad = decay * mean_grad + (1-decay) * gradient
Delta = learning_rate * gradient / sqrt(mean_square + epsilon - mean_grad ** 2)

ms <- rho * ms_{t-1} + (1-rho) * grad * grad
mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
var <- var - mom
END
}
op {
  graph_op_name: "ResourceSparseApplyFtrl"
  endpoint {
    name: "ResourceSparseApplyFtrl"
  }
  summary: "Update relevant entries in \'*var\' according to the Ftrl-proximal scheme."
  description: <<END
That is for rows we have grad for, we update var, accum and linear as follows:
accum_new = accum + grad * grad
linear += grad + (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new
END
}
op {
  graph_op_name: "ResourceSparseApplyFtrlV2"
  endpoint {
    name: "ResourceSparseApplyFtrlV2"
  }
  summary: "Update relevant entries in \'*var\' according to the Ftrl-proximal scheme."
  description: <<END
That is for rows we have grad for, we update var, accum and linear as follows:
grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad_with_shrinkage * grad_with_shrinkage
linear += grad_with_shrinkage +
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new
END
}
op {
  graph_op_name: "ResourceSparseApplyMomentum"
  endpoint {
    name: "ResourceSparseApplyMomentum"
  }
  summary: "Update relevant entries in \'*var\' and \'*accum\' according to the momentum scheme."
  description: <<END
Set use_nesterov = True if you want to use Nesterov momentum.

That is for rows we have grad for, we update var and accum as follows:

accum = accum * momentum + grad
var -= lr * accum
END
}
op {
  graph_op_name: "ResourceSparseApplyProximalAdagrad"
  endpoint {
    name: "ResourceSparseApplyProximalAdagrad"
  }
  summary: "Sparse update entries in \'*var\' and \'*accum\' according to FOBOS algorithm."
  description: <<END
That is for rows we have grad for, we update var and accum as follows:
accum += grad * grad
prox_v = var
prox_v -= lr * grad * (1 / sqrt(accum))
var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}
END
}
op {
  graph_op_name: "ResourceSparseApplyProximalGradientDescent"
  endpoint {
    name: "ResourceSparseApplyProximalGradientDescent"
  }
  summary: "Sparse update \'*var\' as FOBOS algorithm with fixed learning rate."
  description: <<END
That is for rows we have grad for, we update var as follows:
prox_v = var - alpha * grad
var = sign(prox_v)/(1+alpha*l2) * max{|prox_v|-alpha*l1,0}
END
}
op {
  graph_op_name: "ResourceSparseApplyRMSProp"
  endpoint {
    name: "ResourceSparseApplyRMSProp"
  }
  summary: "Update \'*var\' according to the RMSProp algorithm."
  description: <<END
Note that in dense implementation of this algorithm, ms and mom will
update even if the grad is zero, but in this sparse implementation, ms
and mom will not update in iterations during which the grad is zero.

mean_square = decay * mean_square + (1-decay) * gradient ** 2
Delta = learning_rate * gradient / sqrt(mean_square + epsilon)

ms <- rho * ms_{t-1} + (1-rho) * grad * grad
mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
var <- var - mom
END
}
op {
  graph_op_name: "ResourceStridedSliceAssign"
  endpoint {
    name: "ResourceStridedSliceAssign"
  }
  summary: "Assign `value` to the sliced l-value reference of `ref`."
  description: <<END
The values of `value` are assigned to the positions in the variable
`ref` that are selected by the slice parameters. The slice parameters
`begin, `end`, `strides`, etc. work exactly as in `StridedSlice`.

NOTE this op currently does not support broadcasting and so `value`'s
shape must be exactly the shape produced by the slice of `ref`.
END
}
op {
  graph_op_name: "Restore"
  endpoint {
    name: "Restore"
  }
  summary: "Restores a tensor from checkpoint files."
  description: <<END
Reads a tensor stored in one or several files. If there are several files (for
instance because a tensor was saved as slices), `file_pattern` may contain
wildcard symbols (`*` and `?`) in the filename portion only, not in the
directory portion.

If a `file_pattern` matches several files, `preferred_shard` can be used to hint
in which file the requested tensor is likely to be found. This op will first
open the file at index `preferred_shard` in the list of matching files and try
to restore tensors from that file.  Only if some tensors or tensor slices are
not found in that first file, then the Op opens all the files. Setting
`preferred_shard` to match the value passed as the `shard` input
of a matching `Save` Op may speed up Restore.  This attribute only affects
performance, not correctness.  The default value -1 means files are processed in
order.

See also `RestoreSlice`.
END
}
op {
  graph_op_name: "RestoreIterator"
  endpoint {
    name: "RestoreIterator"
  }
  summary: "Restores the state of the `iterator` from the checkpoint saved at `path` using \"SaveIterator\"."
}
op {
  graph_op_name: "RestoreSlice"
  endpoint {
    name: "RestoreSlice"
  }
  summary: "Restores a tensor from checkpoint files."
  description: <<END
This is like `Restore` except that restored tensor can be listed as filling
only a slice of a larger tensor.  `shape_and_slice` specifies the shape of the
larger tensor and the slice that the restored tensor covers.

The `shape_and_slice` input has the same format as the
elements of the `shapes_and_slices` input of the `SaveSlices` op.
END
}
op {
  graph_op_name: "RestoreV2"
  endpoint {
    name: "RestoreV2"
  }
  summary: "Restores tensors from a V2 checkpoint."
  description: <<END
For backward compatibility with the V1 format, this Op currently allows
restoring from a V1 checkpoint as well:
  - This Op first attempts to find the V2 index file pointed to by "prefix", and
    if found proceed to read it as a V2 checkpoint;
  - Otherwise the V1 read path is invoked.
Relying on this behavior is not recommended, as the ability to fall back to read
V1 might be deprecated and eventually removed.

By default, restores the named tensors in full.  If the caller wishes to restore
specific slices of stored tensors, "shape_and_slices" should be non-empty
strings and correspondingly well-formed.

Callers must ensure all the named tensors are indeed stored in the checkpoint.
END
}
op {
  graph_op_name: "Reverse"
  endpoint {
    name: "Reverse"
  }
  summary: "Reverses specific dimensions of a tensor."
  description: <<END
Given a `tensor`, and a `bool` tensor `dims` representing the dimensions
of `tensor`, this operation reverses each dimension i of `tensor` where
`dims[i]` is `True`.

`tensor` can have up to 8 dimensions. The number of dimensions
of `tensor` must equal the number of elements in `dims`. In other words:

`rank(tensor) = size(dims)`

For example:

```
# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [False, False, False, True]
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is [False, True, False, False]
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is [False, False, True, False]
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
```
END
}
op {
  graph_op_name: "ReverseSequence"
  endpoint {
    name: "ReverseSequence"
  }
  summary: "Reverses variable length slices."
  description: <<END
This op first slices `input` along the dimension `batch_dim`, and for each
slice `i`, reverses the first `seq_lengths[i]` elements along
the dimension `seq_dim`.

The elements of `seq_lengths` must obey `seq_lengths[i] <= input.dims[seq_dim]`,
and `seq_lengths` must be a vector of length `input.dims[batch_dim]`.

The output slice `i` along dimension `batch_dim` is then given by input
slice `i`, with the first `seq_lengths[i]` slices along dimension
`seq_dim` reversed.

For example:

```
# Given this:
batch_dim = 0
seq_dim = 1
input.dims = (4, 8, ...)
seq_lengths = [7, 2, 3, 5]

# then slices of input are reversed on seq_dim, but only up to seq_lengths:
output[0, 0:7, :, ...] = input[0, 7:0:-1, :, ...]
output[1, 0:2, :, ...] = input[1, 2:0:-1, :, ...]
output[2, 0:3, :, ...] = input[2, 3:0:-1, :, ...]
output[3, 0:5, :, ...] = input[3, 5:0:-1, :, ...]

# while entries past seq_lens are copied through:
output[0, 7:, :, ...] = input[0, 7:, :, ...]
output[1, 2:, :, ...] = input[1, 2:, :, ...]
output[2, 3:, :, ...] = input[2, 3:, :, ...]
output[3, 2:, :, ...] = input[3, 2:, :, ...]
```

In contrast, if:

```
# Given this:
batch_dim = 2
seq_dim = 0
input.dims = (8, ?, 4, ...)
seq_lengths = [7, 2, 3, 5]

# then slices of input are reversed on seq_dim, but only up to seq_lengths:
output[0:7, :, 0, :, ...] = input[7:0:-1, :, 0, :, ...]
output[0:2, :, 1, :, ...] = input[2:0:-1, :, 1, :, ...]
output[0:3, :, 2, :, ...] = input[3:0:-1, :, 2, :, ...]
output[0:5, :, 3, :, ...] = input[5:0:-1, :, 3, :, ...]

# while entries past seq_lens are copied through:
output[7:, :, 0, :, ...] = input[7:, :, 0, :, ...]
output[2:, :, 1, :, ...] = input[2:, :, 1, :, ...]
output[3:, :, 2, :, ...] = input[3:, :, 2, :, ...]
output[2:, :, 3, :, ...] = input[2:, :, 3, :, ...]
```
END
}
op {
  graph_op_name: "ReverseV2"
  endpoint {
    name: "ReverseV2"
  }
  summary: "Reverses specific dimensions of a tensor."
  description: <<END
NOTE `tf.reverse` has now changed behavior in preparation for 1.0.
`tf.reverse_v2` is currently an alias that will be deprecated before TF 1.0.

Given a `tensor`, and a `int32` tensor `axis` representing the set of
dimensions of `tensor` to reverse. This operation reverses each dimension
`i` for which there exists `j` s.t. `axis[j] == i`.

`tensor` can have up to 8 dimensions. The number of dimensions specified
in `axis` may be 0 or more entries. If an index is specified more than
once, a InvalidArgument error is raised.

For example:

```
# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [3] or 'dims' is -1
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
```
END
}
op {
  graph_op_name: "Rint"
  endpoint {
    name: "Rint"
  }
  summary: "Returns element-wise integer closest to x."
  description: <<END
If the result is midway between two representable values,
the even representable is chosen.
For example:

```
rint(-1.5) ==> -2.0
rint(0.5000001) ==> 1.0
rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]
```
END
}
op {
  graph_op_name: "Round"
  endpoint {
    name: "Round"
  }
  summary: "Rounds the values of a tensor to the nearest integer, element-wise."
  description: <<END
Rounds half to even.  Also known as bankers rounding. If you want to round
according to the current system rounding mode use std::cint.
END
}
op {
  graph_op_name: "Rsqrt"
  endpoint {
    name: "Rsqrt"
  }
  summary: "Computes reciprocal of square root of x element-wise."
  description: <<END
I.e., \\(y = 1 / \sqrt{x}\\).
END
}
op {
  graph_op_name: "RsqrtGrad"
  endpoint {
    name: "RsqrtGrad"
  }
  summary: "Computes the gradient for the rsqrt of `x` wrt its input."
  description: <<END
Specifically, `grad = dy * -0.5 * y^3`, where `y = rsqrt(x)`, and `dy`
is the corresponding input gradient.
END
}
