op {
  graph_op_name: "Abort"
  endpoint {
    name: "Abort"
  }
  summary: "Raise a exception to abort the process when called."
  description: <<END
If exit_without_error is true, the process will exit normally,
otherwise it will exit with a SIGABORT signal.

Returns nothing but an exception.
END
}
op {
  graph_op_name: "Abs"
  endpoint {
    name: "Abs"
  }
  summary: "Computes the absolute value of a tensor."
  description: <<END
Given a tensor `x`, this operation returns a tensor containing the absolute
value of each element in `x`. For example, if x is an input element and y is
an output element, this operation computes \\(y = |x|\\).
END
}
op {
  graph_op_name: "AccumulatorApplyGradient"
  endpoint {
    name: "AccumulatorApplyGradient"
  }
  summary: "Applies a gradient to a given accumulator."
  description: <<END
Does not add if local_step is lesser than the accumulator's global_step.
END
}
op {
  graph_op_name: "AccumulatorNumAccumulated"
  endpoint {
    name: "AccumulatorNumAccumulated"
  }
  summary: "Returns the number of gradients aggregated in the given accumulators."
}
op {
  graph_op_name: "AccumulatorSetGlobalStep"
  endpoint {
    name: "AccumulatorSetGlobalStep"
  }
  summary: "Updates the accumulator with a new value for global_step."
  description: <<END
Logs warning if the accumulator's value is already higher than
new_global_step.
END
}
op {
  graph_op_name: "AccumulatorTakeGradient"
  endpoint {
    name: "AccumulatorTakeGradient"
  }
  summary: "Extracts the average gradient in the given ConditionalAccumulator."
  description: <<END
The op blocks until sufficient (i.e., more than num_required)
gradients have been accumulated.  If the accumulator has already
aggregated more than num_required gradients, it returns the average of
the accumulated gradients.  Also automatically increments the recorded
global_step in the accumulator by 1, and resets the aggregate to 0.
END
}
op {
  graph_op_name: "Acos"
  endpoint {
    name: "Acos"
  }
  summary: "Computes acos of x element-wise."
}
op {
  graph_op_name: "Acosh"
  endpoint {
    name: "Acosh"
  }
  summary: "Computes inverse hyperbolic cosine of x element-wise."
}
op {
  graph_op_name: "Add"
  endpoint {
    name: "Add"
  }
  summary: "Returns x + y element-wise."
  description: <<END
*NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
END
}
op {
  graph_op_name: "AddManySparseToTensorsMap"
  endpoint {
    name: "AddManySparseToTensorsMap"
  }
  summary: "Add an `N`-minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles."
  description: <<END
A `SparseTensor` of rank `R` is represented by three tensors: `sparse_indices`,
`sparse_values`, and `sparse_shape`, where

```sparse_indices.shape[1] == sparse_shape.shape[0] == R```

An `N`-minibatch of `SparseTensor` objects is represented as a `SparseTensor`
having a first `sparse_indices` column taking values between `[0, N)`, where
the minibatch size `N == sparse_shape[0]`.

The input `SparseTensor` must have rank `R` greater than 1, and the first
dimension is treated as the minibatch dimension.  Elements of the `SparseTensor`
must be sorted in increasing order of this first dimension.  The stored
`SparseTensor` objects pointed to by each row of the output `sparse_handles`
will have rank `R-1`.

The `SparseTensor` values can then be read out as part of a minibatch by passing
the given keys as vector elements to `TakeManySparseFromTensorsMap`.  To ensure
the correct `SparseTensorsMap` is accessed, ensure that the same
`container` and `shared_name` are passed to that Op.  If no `shared_name`
is provided here, instead use the *name* of the Operation created by calling
`AddManySparseToTensorsMap` as the `shared_name` passed to
`TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.
END
}
op {
  graph_op_name: "AddN"
  endpoint {
    name: "AddN"
  }
  summary: "Add all input tensors element wise."
}
op {
  graph_op_name: "AddSparseToTensorsMap"
  endpoint {
    name: "AddSparseToTensorsMap"
  }
  summary: "Add a `SparseTensor` to a `SparseTensorsMap` return its handle."
  description: <<END
A `SparseTensor` is represented by three tensors: `sparse_indices`,
`sparse_values`, and `sparse_shape`.

This operator takes the given `SparseTensor` and adds it to a container
object (a `SparseTensorsMap`).  A unique key within this container is generated
in the form of an `int64`, and this is the value that is returned.

The `SparseTensor` can then be read out as part of a minibatch by passing
the key as a vector element to `TakeManySparseFromTensorsMap`.  To ensure
the correct `SparseTensorsMap` is accessed, ensure that the same
`container` and `shared_name` are passed to that Op.  If no `shared_name`
is provided here, instead use the *name* of the Operation created by calling
`AddSparseToTensorsMap` as the `shared_name` passed to
`TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.
END
}
op {
  graph_op_name: "AdjustContrast"
  endpoint {
    name: "AdjustContrast"
  }
  summary: "Deprecated. Disallowed in GraphDef version >= 2."
}
op {
  graph_op_name: "AdjustContrastv2"
  endpoint {
    name: "AdjustContrastv2"
  }
  summary: "Adjust the contrast of one or more images."
  description: <<END
`images` is a tensor of at least 3 dimensions.  The last 3 dimensions are
interpreted as `[height, width, channels]`.  The other dimensions only
represent a collection of images, such as `[batch, height, width, channels].`

Contrast is adjusted independently for each channel of each image.

For each channel, the Op first computes the mean of the image pixels in the
channel and then adjusts each component of each pixel to
`(x - mean) * contrast_factor + mean`.
END
}
op {
  graph_op_name: "AdjustHue"
  endpoint {
    name: "AdjustHue"
  }
  summary: "Adjust the hue of one or more images."
  description: <<END
`images` is a tensor of at least 3 dimensions.  The last dimension is
interpretted as channels, and must be three.

The input image is considered in the RGB colorspace. Conceptually, the RGB
colors are first mapped into HSV. A delta is then applied all the hue values,
and then remapped back to RGB colorspace.
END
}
op {
  graph_op_name: "AdjustSaturation"
  endpoint {
    name: "AdjustSaturation"
  }
  summary: "Adjust the saturation of one or more images."
  description: <<END
`images` is a tensor of at least 3 dimensions.  The last dimension is
interpretted as channels, and must be three.

The input image is considered in the RGB colorspace. Conceptually, the RGB
colors are first mapped into HSV. A scale is then applied all the saturation
values, and then remapped back to RGB colorspace.
END
}
op {
  graph_op_name: "All"
  endpoint {
    name: "All"
  }
  summary: "Computes the \"logical and\" of elements across dimensions of a tensor."
  description: <<END
Reduces `input` along the dimensions given in `reduction_indices`. Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`reduction_indices`. If `keep_dims` is true, the reduced dimensions are
retained with length 1.
END
}
op {
  graph_op_name: "AllCandidateSampler"
  endpoint {
    name: "AllCandidateSampler"
  }
  summary: "Generates labels for candidate sampling with a learned unigram distribution."
  description: <<END
See explanations of candidate sampling and the data formats at
go/candidate-sampling.

For each batch, this op picks a single set of sampled candidate labels.

The advantages of sampling candidates per-batch are simplicity and the
possibility of efficient dense matrix multiplication. The disadvantage is that
the sampled candidates must be chosen independently of the context and of the
true labels.
END
}
op {
  graph_op_name: "Angle"
  endpoint {
    name: "Angle"
  }
  summary: "Returns the argument of a complex number."
  description: <<END
Given a tensor `input` of complex numbers, this operation returns a tensor of
type `float` that is the argument of each element in `input`. All elements in
`input` must be complex numbers of the form \\(a + bj\\), where *a*
is the real part and *b* is the imaginary part.

The argument returned by this operation is of the form \\(atan2(b, a)\\).

For example:

```
# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.angle(input) ==> [2.0132, 1.056]
```

@compatibility(numpy)
Equivalent to np.angle.
@end_compatibility
END
}
op {
  graph_op_name: "Any"
  endpoint {
    name: "Any"
  }
  summary: "Computes the \"logical or\" of elements across dimensions of a tensor."
  description: <<END
Reduces `input` along the dimensions given in `reduction_indices`. Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`reduction_indices`. If `keep_dims` is true, the reduced dimensions are
retained with length 1.
END
}
op {
  graph_op_name: "ApplyAdadelta"
  endpoint {
    name: "ApplyAdadelta"
  }
  summary: "Update \'*var\' according to the adadelta scheme."
  description: <<END
accum = rho() * accum + (1 - rho()) * grad.square();
update = (update_accum + epsilon).sqrt() * (accum + epsilon()).rsqrt() * grad;
update_accum = rho() * update_accum + (1 - rho()) * update.square();
var -= update;
END
}
op {
  graph_op_name: "ApplyAdagrad"
  endpoint {
    name: "ApplyAdagrad"
  }
  summary: "Update \'*var\' according to the adagrad scheme."
  description: <<END
accum += grad * grad
var -= lr * grad * (1 / sqrt(accum))
END
}
op {
  graph_op_name: "ApplyAdagradDA"
  endpoint {
    name: "ApplyAdagradDA"
  }
  summary: "Update \'*var\' according to the proximal adagrad scheme."
}
op {
  graph_op_name: "ApplyAdam"
  endpoint {
    name: "ApplyAdam"
  }
  summary: "Update \'*var\' according to the Adam algorithm."
  description: <<END
lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)
m_t <- beta1 * m_{t-1} + (1 - beta1) * g_t
v_t <- beta2 * v_{t-1} + (1 - beta2) * g_t * g_t
variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)
END
}
op {
  graph_op_name: "ApplyCenteredRMSProp"
  endpoint {
    name: "ApplyCenteredRMSProp"
  }
  summary: "Update \'*var\' according to the centered RMSProp algorithm."
  description: <<END
The centered RMSProp algorithm uses an estimate of the centered second moment
(i.e., the variance) for normalization, as opposed to regular RMSProp, which
uses the (uncentered) second moment. This often helps with training, but is
slightly more expensive in terms of computation and memory.

Note that in dense implementation of this algorithm, mg, ms, and mom will
update even if the grad is zero, but in this sparse implementation, mg, ms,
and mom will not update in iterations during which the grad is zero.

mean_square = decay * mean_square + (1-decay) * gradient ** 2
mean_grad = decay * mean_grad + (1-decay) * gradient

Delta = learning_rate * gradient / sqrt(mean_square + epsilon - mean_grad ** 2)

mg <- rho * mg_{t-1} + (1-rho) * grad
ms <- rho * ms_{t-1} + (1-rho) * grad * grad
mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms - mg * mg + epsilon)
var <- var - mom
END
}
op {
  graph_op_name: "ApplyFtrl"
  endpoint {
    name: "ApplyFtrl"
  }
  summary: "Update \'*var\' according to the Ftrl-proximal scheme."
  description: <<END
accum_new = accum + grad * grad
linear += grad + (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new
END
}
op {
  graph_op_name: "ApplyFtrlV2"
  endpoint {
    name: "ApplyFtrlV2"
  }
  summary: "Update \'*var\' according to the Ftrl-proximal scheme."
  description: <<END
grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad_with_shrinkage * grad_with_shrinkage
linear += grad_with_shrinkage +
    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
accum = accum_new
END
}
op {
  graph_op_name: "ApplyGradientDescent"
  endpoint {
    name: "ApplyGradientDescent"
  }
  summary: "Update \'*var\' by subtracting \'alpha\' * \'delta\' from it."
}
op {
  graph_op_name: "ApplyMomentum"
  endpoint {
    name: "ApplyMomentum"
  }
  summary: "Update \'*var\' according to the momentum scheme. Set use_nesterov = True if you"
  description: <<END
want to use Nesterov momentum.

accum = accum * momentum + grad
var -= lr * accum
END
}
op {
  graph_op_name: "ApplyProximalAdagrad"
  endpoint {
    name: "ApplyProximalAdagrad"
  }
  summary: "Update \'*var\' and \'*accum\' according to FOBOS with Adagrad learning rate."
  description: <<END
accum += grad * grad
prox_v = var - lr * grad * (1 / sqrt(accum))
var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}
END
}
op {
  graph_op_name: "ApplyProximalGradientDescent"
  endpoint {
    name: "ApplyProximalGradientDescent"
  }
  summary: "Update \'*var\' as FOBOS algorithm with fixed learning rate."
  description: <<END
prox_v = var - alpha * delta
var = sign(prox_v)/(1+alpha*l2) * max{|prox_v|-alpha*l1,0}
END
}
op {
  graph_op_name: "ApplyRMSProp"
  endpoint {
    name: "ApplyRMSProp"
  }
  summary: "Update \'*var\' according to the RMSProp algorithm."
  description: <<END
Note that in dense implementation of this algorithm, ms and mom will
update even if the grad is zero, but in this sparse implementation, ms
and mom will not update in iterations during which the grad is zero.

mean_square = decay * mean_square + (1-decay) * gradient ** 2
Delta = learning_rate * gradient / sqrt(mean_square + epsilon)

ms <- rho * ms_{t-1} + (1-rho) * grad * grad
mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
var <- var - mom
END
}
op {
  graph_op_name: "ApproximateEqual"
  endpoint {
    name: "ApproximateEqual"
  }
  summary: "Returns the truth value of abs(x-y) < tolerance element-wise."
}
op {
  graph_op_name: "ArgMax"
  endpoint {
    name: "ArgMax"
  }
  summary: "Returns the index with the largest value across dimensions of a tensor."
  description: <<END
Note that in case of ties the identity of the return value is not guaranteed.
END
}
op {
  graph_op_name: "ArgMin"
  endpoint {
    name: "ArgMin"
  }
  summary: "Returns the index with the smallest value across dimensions of a tensor."
  description: <<END
Note that in case of ties the identity of the return value is not guaranteed.
END
}
op {
  graph_op_name: "AsString"
  endpoint {
    name: "AsString"
  }
  summary: "Converts each entry in the given tensor to strings.  Supports many numeric"
  description: <<END
types and boolean.
END
}
op {
  graph_op_name: "Asin"
  endpoint {
    name: "Asin"
  }
  summary: "Computes asin of x element-wise."
}
op {
  graph_op_name: "Asinh"
  endpoint {
    name: "Asinh"
  }
  summary: "Computes inverse hyperbolic sine of x element-wise."
}
op {
  graph_op_name: "Assert"
  endpoint {
    name: "Assert"
  }
  summary: "Asserts that the given condition is true."
  description: <<END
If `condition` evaluates to false, print the list of tensors in `data`.
`summarize` determines how many entries of the tensors to print.
END
}
op {
  graph_op_name: "Assign"
  endpoint {
    name: "Assign"
  }
  summary: "Update \'ref\' by assigning \'value\' to it."
  description: <<END
This operation outputs "ref" after the assignment is done.
This makes it easier to chain operations that need to use the reset value.
END
}
op {
  graph_op_name: "AssignAdd"
  endpoint {
    name: "AssignAdd"
  }
  summary: "Update \'ref\' by adding \'value\' to it."
  description: <<END
This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.
END
}
op {
  graph_op_name: "AssignSub"
  endpoint {
    name: "AssignSub"
  }
  summary: "Update \'ref\' by subtracting \'value\' from it."
  description: <<END
This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.
END
}
op {
  graph_op_name: "Atan"
  endpoint {
    name: "Atan"
  }
  summary: "Computes atan of x element-wise."
}
op {
  graph_op_name: "Atan2"
  endpoint {
    name: "Atan2"
  }
  summary: "Computes arctangent of `y/x` element-wise, respecting signs of the arguments."
  description: <<END
This is the angle \( \theta \in [-\pi, \pi] \) such that
\[ x = r \cos(\theta) \]
and
\[ y = r \sin(\theta) \]
where \(r = \sqrt(x^2 + y^2) \).
END
}
op {
  graph_op_name: "Atanh"
  endpoint {
    name: "Atanh"
  }
  summary: "Computes inverse hyperbolic tangent of x element-wise."
}
op {
  graph_op_name: "AudioSpectrogram"
  endpoint {
    name: "AudioSpectrogram"
  }
  summary: "Produces a visualization of audio data over time."
  description: <<END
Spectrograms are a standard way of representing audio information as a series of
slices of frequency information, one slice for each window of time. By joining
these together into a sequence, they form a distinctive fingerprint of the sound
over time.

This op expects to receive audio data as an input, stored as floats in the range
-1 to 1, together with a window width in samples, and a stride specifying how
far to move the window between slices. From this it generates a three
dimensional output. The lowest dimension has an amplitude value for each
frequency during that time slice. The next dimension is time, with successive
frequency slices. The final dimension is for the channels in the input, so a
stereo audio input would have two here for example.

This means the layout when converted and saved as an image is rotated 90 degrees
clockwise from a typical spectrogram. Time is descending down the Y axis, and
the frequency decreases from left to right.

Each value in the result represents the square root of the sum of the real and
imaginary parts of an FFT on the current window of samples. In this way, the
lowest dimension represents the power of each frequency in the current window,
and adjacent windows are concatenated in the next dimension.

To get a more intuitive and visual look at what this operation does, you can run
tensorflow/examples/wav_to_spectrogram to read in an audio file and save out the
resulting spectrogram as a PNG image.
END
}
op {
  graph_op_name: "AudioSummary"
  endpoint {
    name: "AudioSummary"
  }
  summary: "Outputs a `Summary` protocol buffer with audio."
  description: <<END
The summary has up to `max_outputs` summary values containing audio. The
audio is built from `tensor` which must be 3-D with shape `[batch_size,
frames, channels]` or 2-D with shape `[batch_size, frames]`. The values are
assumed to be in the range of `[-1.0, 1.0]` with a sample rate of `sample_rate`.

The `tag` argument is a scalar `Tensor` of type `string`.  It is used to
build the `tag` of the summary values:

*  If `max_outputs` is 1, the summary value tag is '*tag*/audio'.
*  If `max_outputs` is greater than 1, the summary value tags are
   generated sequentially as '*tag*/audio/0', '*tag*/audio/1', etc.
END
}
op {
  graph_op_name: "AudioSummaryV2"
  endpoint {
    name: "AudioSummaryV2"
  }
  summary: "Outputs a `Summary` protocol buffer with audio."
  description: <<END
The summary has up to `max_outputs` summary values containing audio. The
audio is built from `tensor` which must be 3-D with shape `[batch_size,
frames, channels]` or 2-D with shape `[batch_size, frames]`. The values are
assumed to be in the range of `[-1.0, 1.0]` with a sample rate of `sample_rate`.

The `tag` argument is a scalar `Tensor` of type `string`.  It is used to
build the `tag` of the summary values:

*  If `max_outputs` is 1, the summary value tag is '*tag*/audio'.
*  If `max_outputs` is greater than 1, the summary value tags are
   generated sequentially as '*tag*/audio/0', '*tag*/audio/1', etc.
END
}
op {
  graph_op_name: "AvgPool"
  endpoint {
    name: "AvgPool"
  }
  summary: "Performs average pooling on the input."
  description: <<END
Each entry in `output` is the mean of the corresponding size `ksize`
window in `value`.
END
}
op {
  graph_op_name: "AvgPool3D"
  endpoint {
    name: "AvgPool3D"
  }
  summary: "Performs 3D average pooling on the input."
}
op {
  graph_op_name: "AvgPool3DGrad"
  endpoint {
    name: "AvgPool3DGrad"
  }
  summary: "Computes gradients of average pooling function."
}
op {
  graph_op_name: "AvgPoolGrad"
  endpoint {
    name: "AvgPoolGrad"
  }
  summary: "Computes gradients of the average pooling function."
}
