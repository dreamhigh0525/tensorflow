op {
  graph_op_name: "Pack"
  endpoint {
    name: "Pack"
  }
  summary: "Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor."
  description: <<END
Packs the `N` tensors in `values` into a tensor with rank one higher than each
tensor in `values`, by packing them along the `axis` dimension.
Given a list of tensors of shape `(A, B, C)`;

if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.
if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.
Etc.

For example:

```
# 'x' is [1, 4]
# 'y' is [2, 5]
# 'z' is [3, 6]
pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.
pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]
```

This is the opposite of `unpack`.
END
}
op {
  graph_op_name: "Pad"
  endpoint {
    name: "Pad"
  }
  summary: "Pads a tensor with zeros."
  description: <<END
This operation pads a `input` with zeros according to the `paddings` you
specify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is the
rank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates
how many zeros to add before the contents of `input` in that dimension, and
`paddings[D, 1]` indicates how many zeros to add after the contents of `input`
in that dimension.

The padded size of each dimension D of the output is:

`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`

For example:

```
# 't' is [[1, 1], [2, 2]]
# 'paddings' is [[1, 1], [2, 2]]
# rank of 't' is 2
pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]
                      [0, 0, 1, 1, 0, 0]
                      [0, 0, 2, 2, 0, 0]
                      [0, 0, 0, 0, 0, 0]]
```
END
}
op {
  graph_op_name: "PadV2"
  endpoint {
    name: "PadV2"
  }
  summary: "Pads a tensor."
  description: <<END
This operation pads `input` according to the `paddings` and `constant_values`
you specify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is
the rank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates
how many padding values to add before the contents of `input` in that dimension,
and `paddings[D, 1]` indicates how many padding values to add after the contents
of `input` in that dimension. `constant_values` is a scalar tensor of the same
type as `input` that indicates the value to use for padding `input`.

The padded size of each dimension D of the output is:

`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`

For example:

```
# 't' is [[1, 1], [2, 2]]
# 'paddings' is [[1, 1], [2, 2]]
# 'constant_values' is 0
# rank of 't' is 2
pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]
                      [0, 0, 1, 1, 0, 0]
                      [0, 0, 2, 2, 0, 0]
                      [0, 0, 0, 0, 0, 0]]
```
END
}
op {
  graph_op_name: "PaddedBatchDataset"
  endpoint {
    name: "PaddedBatchDataset"
  }
  summary: "Creates a dataset that batches and pads `batch_size` elements from the input."
}
op {
  graph_op_name: "PaddingFIFOQueue"
  endpoint {
    name: "PaddingFIFOQueue"
  }
  summary: "A queue that produces elements in first-in first-out order."
  description: <<END
Variable-size shapes are allowed by setting the corresponding shape dimensions
to 0 in the shape attr.  In this case DequeueMany will pad up to the maximum
size of any given element in the minibatch.  See below for details.
END
}
op {
  graph_op_name: "PaddingFIFOQueueV2"
  endpoint {
    name: "PaddingFIFOQueueV2"
  }
  summary: "A queue that produces elements in first-in first-out order."
  description: <<END
Variable-size shapes are allowed by setting the corresponding shape dimensions
to 0 in the shape attr.  In this case DequeueMany will pad up to the maximum
size of any given element in the minibatch.  See below for details.
END
}
op {
  graph_op_name: "ParallelConcat"
  endpoint {
    name: "ParallelConcat"
  }
  summary: "Concatenates a list of `N` tensors along the first dimension."
  description: <<END
The input tensors are all required to have size 1 in the first dimension.

For example:

```
# 'x' is [[1, 4]]
# 'y' is [[2, 5]]
# 'z' is [[3, 6]]
parallel_concat([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.
```

The difference between concat and parallel_concat is that concat requires all
of the inputs be computed before the operation will begin but doesn't require
that the input shapes be known during graph construction.  Parallel concat
will copy pieces of the input into the output as they become available, in
some situations this can provide a performance benefit.
END
}
op {
  graph_op_name: "ParallelDynamicStitch"
  endpoint {
    name: "ParallelDynamicStitch"
  }
  summary: "Interleave the values from the `data` tensors into a single tensor."
  description: <<END
Builds a merged tensor such that

```python
    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]
```

For example, if each `indices[m]` is scalar or vector, we have

```python
    # Scalar indices:
    merged[indices[m], ...] = data[m][...]

    # Vector indices:
    merged[indices[m][i], ...] = data[m][i, ...]
```

Each `data[i].shape` must start with the corresponding `indices[i].shape`,
and the rest of `data[i].shape` must be constant w.r.t. `i`.  That is, we
must have `data[i].shape = indices[i].shape + constant`.  In terms of this
`constant`, the output shape is

    merged.shape = [max(indices)] + constant

Values may be merged in parallel, so if an index appears in both `indices[m][i]`
and `indices[n][j]`, the result may be invalid. This differs from the normal
DynamicStitch operator that defines the behavior in that case.

For example:

```python
    indices[0] = 6
    indices[1] = [4, 1]
    indices[2] = [[5, 2], [0, 3]]
    data[0] = [61, 62]
    data[1] = [[41, 42], [11, 12]]
    data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]
    merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],
              [51, 52], [61, 62]]
```

This method can be used to merge partitions created by `dynamic_partition`
as illustrated on the following example:

```python
    # Apply function (increments x_i) on elements for which a certain condition
    # apply (x_i != -1 in this example).
    x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])
    condition_mask=tf.not_equal(x,tf.constant(-1.))
    partitioned_data = tf.dynamic_partition(
        x, tf.cast(condition_mask, tf.int32) , 2)
    partitioned_data[1] = partitioned_data[1] + 1.0
    condition_indices = tf.dynamic_partition(
        tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)
    x = tf.dynamic_stitch(condition_indices, partitioned_data)
    # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain
    # unchanged.
```

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/DynamicStitch.png" alt>
</div>
END
}
op {
  graph_op_name: "ParallelMapDataset"
  endpoint {
    name: "ParallelMapDataset"
  }
  summary: "Creates a dataset that applies `f` to the outputs of `input_dataset`."
  description: <<END
Unlike a "MapDataset", which applies `f` sequentially, this dataset invokes up
to `num_parallel_calls` copies of `f` in parallel.
END
}
op {
  graph_op_name: "ParameterizedTruncatedNormal"
  endpoint {
    name: "ParameterizedTruncatedNormal"
  }
  summary: "Outputs random values from a normal distribution. The parameters may each be a"
  description: <<END
scalar which applies to the entire output, or a vector of length shape[0] which
stores the parameters for each batch.
END
}
op {
  graph_op_name: "ParseExample"
  endpoint {
    name: "ParseExample"
  }
  summary: "Transforms a vector of brain.Example protos (as strings) into typed tensors."
}
op {
  graph_op_name: "ParseSingleSequenceExample"
  endpoint {
    name: "ParseSingleSequenceExample"
  }
  summary: "Transforms a scalar brain.SequenceExample proto (as strings) into typed tensors."
}
op {
  graph_op_name: "ParseTensor"
  endpoint {
    name: "ParseTensor"
  }
  summary: "Transforms a serialized tensorflow.TensorProto proto into a Tensor."
}
op {
  graph_op_name: "Placeholder"
  endpoint {
    name: "Placeholder"
  }
  summary: "A placeholder op for a value that will be fed into the computation."
  description: <<END
N.B. This operation will fail with an error if it is executed. It is
intended as a way to represent a value that will always be fed, and to
provide attrs that enable the fed value to be checked at runtime.
END
}
op {
  graph_op_name: "PlaceholderV2"
  endpoint {
    name: "PlaceholderV2"
  }
  summary: "A placeholder op for a value that will be fed into the computation."
  description: <<END
N.B. This operation will fail with an error if it is executed. It is
intended as a way to represent a value that will always be fed, and to
provide attrs that enable the fed value to be checked at runtime.
END
}
op {
  graph_op_name: "PlaceholderWithDefault"
  endpoint {
    name: "PlaceholderWithDefault"
  }
  summary: "A placeholder op that passes through `input` when its output is not fed."
}
op {
  graph_op_name: "Polygamma"
  endpoint {
    name: "Polygamma"
  }
  summary: "Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\)."
  description: <<END
The polygamma function is defined as:


\\(\psi^{(n)}(x) = \frac{d^n}{dx^n} \psi(x)\\)

where \\(\psi(x)\\) is the digamma function.
END
}
op {
  graph_op_name: "PopulationCount"
  endpoint {
    name: "PopulationCount"
  }
  summary: "Computes element-wise population count (a.k.a. popcount, bitsum, bitcount)."
  description: <<END
For each entry in `x`, calculates the number of `1` (on) bits in the binary
representation of that entry.

**NOTE**: It is more efficient to first `tf.bitcast` your tensors into
`int32` or `int64` and perform the bitcount on the result, than to feed in
8- or 16-bit inputs and then aggregate the resulting counts.
END
}
op {
  graph_op_name: "Pow"
  endpoint {
    name: "Pow"
  }
  summary: "Computes the power of one value to another."
  description: <<END
Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for
corresponding elements in `x` and `y`. For example:

```
# tensor 'x' is [[2, 2]], [3, 3]]
# tensor 'y' is [[8, 16], [2, 3]]
tf.pow(x, y) ==> [[256, 65536], [9, 27]]
```
END
}
op {
  graph_op_name: "PrefetchDataset"
  endpoint {
    name: "PrefetchDataset"
  }
  summary: "Creates a dataset that asynchronously prefetches elements from `input_dataset`."
}
op {
  graph_op_name: "PreventGradient"
  endpoint {
    name: "PreventGradient"
  }
  summary: "An identity op that triggers an error if a gradient is requested."
  description: <<END
When executed in a graph, this op outputs its input tensor as-is.

When building ops to compute gradients, the TensorFlow gradient system
will return an error when trying to lookup the gradient of this op,
because no gradient must ever be registered for this function.  This
op exists to prevent subtle bugs from silently returning unimplemented
gradients in some corner cases.
END
}
op {
  graph_op_name: "Print"
  endpoint {
    name: "Print"
  }
  summary: "Prints a list of tensors."
  description: <<END
Passes `input` through to `output` and prints `data` when evaluating.
END
}
op {
  graph_op_name: "PriorityQueue"
  endpoint {
    name: "PriorityQueue"
  }
  summary: "A queue that produces elements sorted by the first component value."
  description: <<END
Note that the PriorityQueue requires the first component of any element
to be a scalar int64, in addition to the other elements declared by
component_types.  Therefore calls to Enqueue and EnqueueMany (resp. Dequeue
and DequeueMany) on a PriorityQueue will all require (resp. output) one extra
entry in their input (resp. output) lists.
END
}
op {
  graph_op_name: "PriorityQueueV2"
  endpoint {
    name: "PriorityQueueV2"
  }
  summary: "A queue that produces elements sorted by the first component value."
  description: <<END
Note that the PriorityQueue requires the first component of any element
to be a scalar int64, in addition to the other elements declared by
component_types.  Therefore calls to Enqueue and EnqueueMany (resp. Dequeue
and DequeueMany) on a PriorityQueue will all require (resp. output) one extra
entry in their input (resp. output) lists.
END
}
op {
  graph_op_name: "Prod"
  endpoint {
    name: "Prod"
  }
  summary: "Computes the product of elements across dimensions of a tensor."
  description: <<END
Reduces `input` along the dimensions given in `reduction_indices`. Unless
`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
`reduction_indices`. If `keep_dims` is true, the reduced dimensions are
retained with length 1.
END
}
op {
  graph_op_name: "PyFunc"
  endpoint {
    name: "PyFunc"
  }
  summary: "Invokes a python function to compute func(input)->output."
  description: <<END
This operation is considered stateful. For a stateless version, see
PyFuncStateless.
END
}
op {
  graph_op_name: "PyFuncStateless"
  endpoint {
    name: "PyFuncStateless"
  }
  summary: "A stateless version of PyFunc."
}
