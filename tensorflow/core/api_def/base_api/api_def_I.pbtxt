op {
  graph_op_name: "IFFT"
  endpoint {
    name: "IFFT"
  }
  summary: "Inverse fast Fourier transform."
  description: <<END
Computes the inverse 1-dimensional discrete Fourier transform over the
inner-most dimension of `input`.
END
}
op {
  graph_op_name: "IFFT2D"
  endpoint {
    name: "IFFT2D"
  }
  summary: "Inverse 2D fast Fourier transform."
  description: <<END
Computes the inverse 2-dimensional discrete Fourier transform over the
inner-most 2 dimensions of `input`.
END
}
op {
  graph_op_name: "IFFT3D"
  endpoint {
    name: "IFFT3D"
  }
  summary: "Inverse 3D fast Fourier transform."
  description: <<END
Computes the inverse 3-dimensional discrete Fourier transform over the
inner-most 3 dimensions of `input`.
END
}
op {
  graph_op_name: "IRFFT"
  endpoint {
    name: "IRFFT"
  }
  summary: "Inverse real-valued fast Fourier transform."
  description: <<END
Computes the inverse 1-dimensional discrete Fourier transform of a real-valued
signal over the inner-most dimension of `input`.

The inner-most dimension of `input` is assumed to be the result of `RFFT`: the
`fft_length / 2 + 1` unique components of the DFT of a real-valued signal. If
`fft_length` is not provided, it is computed from the size of the inner-most
dimension of `input` (`fft_length = 2 * (inner - 1)`). If the FFT length used to
compute `input` is odd, it should be provided since it cannot be inferred
properly.

Along the axis `IRFFT` is computed on, if `fft_length / 2 + 1` is smaller
than the corresponding dimension of `input`, the dimension is cropped. If it is
larger, the dimension is padded with zeros.
END
}
op {
  graph_op_name: "IRFFT2D"
  endpoint {
    name: "IRFFT2D"
  }
  summary: "Inverse 2D real-valued fast Fourier transform."
  description: <<END
Computes the inverse 2-dimensional discrete Fourier transform of a real-valued
signal over the inner-most 2 dimensions of `input`.

The inner-most 2 dimensions of `input` are assumed to be the result of `RFFT2D`:
The inner-most dimension contains the `fft_length / 2 + 1` unique components of
the DFT of a real-valued signal. If `fft_length` is not provided, it is computed
from the size of the inner-most 2 dimensions of `input`. If the FFT length used
to compute `input` is odd, it should be provided since it cannot be inferred
properly.

Along each axis `IRFFT2D` is computed on, if `fft_length` (or
`fft_length / 2 + 1` for the inner-most dimension) is smaller than the
corresponding dimension of `input`, the dimension is cropped. If it is larger,
the dimension is padded with zeros.
END
}
op {
  graph_op_name: "IRFFT3D"
  endpoint {
    name: "IRFFT3D"
  }
  summary: "Inverse 3D real-valued fast Fourier transform."
  description: <<END
Computes the inverse 3-dimensional discrete Fourier transform of a real-valued
signal over the inner-most 3 dimensions of `input`.

The inner-most 3 dimensions of `input` are assumed to be the result of `RFFT3D`:
The inner-most dimension contains the `fft_length / 2 + 1` unique components of
the DFT of a real-valued signal. If `fft_length` is not provided, it is computed
from the size of the inner-most 3 dimensions of `input`. If the FFT length used
to compute `input` is odd, it should be provided since it cannot be inferred
properly.

Along each axis `IRFFT3D` is computed on, if `fft_length` (or
`fft_length / 2 + 1` for the inner-most dimension) is smaller than the
corresponding dimension of `input`, the dimension is cropped. If it is larger,
the dimension is padded with zeros.
END
}
op {
  graph_op_name: "Identity"
  endpoint {
    name: "Identity"
  }
  summary: "Return a tensor with the same shape and contents as the input tensor or value."
}
op {
  graph_op_name: "IdentityN"
  endpoint {
    name: "IdentityN"
  }
  summary: "Returns a list of tensors with the same shapes and contents as the input"
  description: <<END
tensors.

This op can be used to override the gradient for complicated functions. For
example, suppose y = f(x) and we wish to apply a custom function g for backprop
such that dx = g(dy). In Python,

```python
with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
  return [None, g(dy)]  # Do not backprop to f(x).
```
END
}
op {
  graph_op_name: "IdentityReader"
  endpoint {
    name: "IdentityReader"
  }
  summary: "A Reader that outputs the queued work as both the key and value."
  description: <<END
To use, enqueue strings in a Queue.  ReaderRead will take the front
work string and output (work, work).
END
}
op {
  graph_op_name: "IdentityReaderV2"
  endpoint {
    name: "IdentityReaderV2"
  }
  summary: "A Reader that outputs the queued work as both the key and value."
  description: <<END
To use, enqueue strings in a Queue.  ReaderRead will take the front
work string and output (work, work).
END
}
op {
  graph_op_name: "Igamma"
  endpoint {
    name: "Igamma"
  }
  summary: "Compute the lower regularized incomplete Gamma function `Q(a, x)`."
  description: <<END
The lower regularized incomplete Gamma function is defined as:


\\(P(a, x) = gamma(a, x) / Gamma(a) = 1 - Q(a, x)\\)

where

\\(gamma(a, x) = int_{0}^{x} t^{a-1} exp(-t) dt\\)

is the lower incomplete Gamma function.

Note, above `Q(a, x)` (`Igammac`) is the upper regularized complete
Gamma function.
END
}
op {
  graph_op_name: "Igammac"
  endpoint {
    name: "Igammac"
  }
  summary: "Compute the upper regularized incomplete Gamma function `Q(a, x)`."
  description: <<END
The upper regularized incomplete Gamma function is defined as:

\\(Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)\\)

where

\\(Gamma(a, x) = int_{x}^{\infty} t^{a-1} exp(-t) dt\\)

is the upper incomplete Gama function.

Note, above `P(a, x)` (`Igamma`) is the lower regularized complete
Gamma function.
END
}
op {
  graph_op_name: "IgnoreErrorsDataset"
  endpoint {
    name: "IgnoreErrorsDataset"
  }
  summary: "Creates a dataset that contains the elements of `input_dataset` ignoring errors."
}
op {
  graph_op_name: "Imag"
  endpoint {
    name: "Imag"
  }
  summary: "Returns the imaginary part of a complex number."
  description: <<END
Given a tensor `input` of complex numbers, this operation returns a tensor of
type `float` that is the imaginary part of each element in `input`. All
elements in `input` must be complex numbers of the form \\(a + bj\\), where *a*
is the real part and *b* is the imaginary part returned by this operation.

For example:

```
# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.imag(input) ==> [4.75, 5.75]
```
END
}
op {
  graph_op_name: "ImageSummary"
  endpoint {
    name: "ImageSummary"
  }
  summary: "Outputs a `Summary` protocol buffer with images."
  description: <<END
The summary has up to `max_images` summary values containing images. The
images are built from `tensor` which must be 4-D with shape `[batch_size,
height, width, channels]` and where `channels` can be:

*  1: `tensor` is interpreted as Grayscale.
*  3: `tensor` is interpreted as RGB.
*  4: `tensor` is interpreted as RGBA.

The images have the same number of channels as the input tensor. For float
input, the values are normalized one image at a time to fit in the range
`[0, 255]`.  `uint8` values are unchanged.  The op uses two different
normalization algorithms:

*  If the input values are all positive, they are rescaled so the largest one
   is 255.

*  If any input value is negative, the values are shifted so input value 0.0
   is at 127.  They are then rescaled so that either the smallest value is 0,
   or the largest one is 255.

The `tag` argument is a scalar `Tensor` of type `string`.  It is used to
build the `tag` of the summary values:

*  If `max_images` is 1, the summary value tag is '*tag*/image'.
*  If `max_images` is greater than 1, the summary value tags are
   generated sequentially as '*tag*/image/0', '*tag*/image/1', etc.

The `bad_color` argument is the color to use in the generated images for
non-finite input values.  It is a `unit8` 1-D tensor of length `channels`.
Each element must be in the range `[0, 255]` (It represents the value of a
pixel in the output image).  Non-finite values in the input tensor are
replaced by this tensor in the output image.  The default value is the color
red.
END
}
op {
  graph_op_name: "ImmutableConst"
  endpoint {
    name: "ImmutableConst"
  }
  summary: "Returns immutable tensor from memory region."
  description: <<END
The current implementation memmaps the tensor from a file.
END
}
op {
  graph_op_name: "InTopK"
  endpoint {
    name: "InTopK"
  }
  summary: "Says whether the targets are in the top `K` predictions."
  description: <<END
This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the
prediction for the target class is among the top `k` predictions among
all predictions for example `i`. Note that the behavior of `InTopK` differs
from the `TopK` op in its handling of ties; if multiple classes have the
same prediction value and straddle the top-`k` boundary, all of those
classes are considered to be in the top `k`.

More formally, let

  \\(predictions_i\\) be the predictions for all classes for example `i`,
  \\(targets_i\\) be the target class for example `i`,
  \\(out_i\\) be the output for example `i`,

$$out_i = predictions_{i, targets_i} \in TopKIncludingTies(predictions_i)$$
END
}
op {
  graph_op_name: "InTopKV2"
  endpoint {
    name: "InTopKV2"
  }
  summary: "Says whether the targets are in the top `K` predictions."
  description: <<END
This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the
prediction for the target class is among the top `k` predictions among
all predictions for example `i`. Note that the behavior of `InTopK` differs
from the `TopK` op in its handling of ties; if multiple classes have the
same prediction value and straddle the top-`k` boundary, all of those
classes are considered to be in the top `k`.

More formally, let

  \\(predictions_i\\) be the predictions for all classes for example `i`,
  \\(targets_i\\) be the target class for example `i`,
  \\(out_i\\) be the output for example `i`,

$$out_i = predictions_{i, targets_i} \in TopKIncludingTies(predictions_i)$$
END
}
op {
  graph_op_name: "InitializeTable"
  endpoint {
    name: "InitializeTable"
  }
  summary: "Table initializer that takes two tensors for keys and values respectively."
}
op {
  graph_op_name: "InitializeTableFromTextFile"
  endpoint {
    name: "InitializeTableFromTextFile"
  }
  summary: "Initializes a table from a text file."
  description: <<END
It inserts one key-value pair into the table for each line of the file.
The key and value is extracted from the whole line content, elements from the
split line based on `delimiter` or the line number (starting from zero).
Where to extract the key and value from a line is specified by `key_index` and
`value_index`.

- A value of -1 means use the line number(starting from zero), expects `int64`.
- A value of -2 means use the whole line content, expects `string`.
- A value >= 0 means use the index (starting at zero) of the split line based
  on `delimiter`.
END
}
op {
  graph_op_name: "InitializeTableFromTextFileV2"
  endpoint {
    name: "InitializeTableFromTextFileV2"
  }
  summary: "Initializes a table from a text file."
  description: <<END
It inserts one key-value pair into the table for each line of the file.
The key and value is extracted from the whole line content, elements from the
split line based on `delimiter` or the line number (starting from zero).
Where to extract the key and value from a line is specified by `key_index` and
`value_index`.

- A value of -1 means use the line number(starting from zero), expects `int64`.
- A value of -2 means use the whole line content, expects `string`.
- A value >= 0 means use the index (starting at zero) of the split line based
  on `delimiter`.
END
}
op {
  graph_op_name: "InitializeTableV2"
  endpoint {
    name: "InitializeTableV2"
  }
  summary: "Table initializer that takes two tensors for keys and values respectively."
}
op {
  graph_op_name: "InterleaveDataset"
  endpoint {
    name: "InterleaveDataset"
  }
  summary: "Creates a dataset that applies `f` to the outputs of `input_dataset`."
  description: <<END
Unlike MapDataset, the `f` in InterleaveDataset is expected to return
a Dataset variant, and InterleaveDataset will flatten successive
results into a single Dataset. Unlike FlatMapDataset,
InterleaveDataset will interleave sequences of up to `block_length`
consecutive elements from `cycle_length` input elements.
END
}
op {
  graph_op_name: "Inv"
  endpoint {
    name: "Inv"
  }
  summary: "Computes the reciprocal of x element-wise."
  description: <<END
I.e., \\(y = 1 / x\\).
END
}
op {
  graph_op_name: "InvGrad"
  endpoint {
    name: "InvGrad"
  }
  summary: "Computes the gradient for the inverse of `x` wrt its input."
  description: <<END
Specifically, `grad = -dy * y*y`, where `y = 1/x`, and `dy`
is the corresponding input gradient.
END
}
op {
  graph_op_name: "Invert"
  endpoint {
    name: "Invert"
  }
  summary: "Flips all bits elementwise."
  description: <<END
The result will have exactly those bits set, that are not set in `x`. The
computation is performed on the underlying representation of x.
END
}
op {
  graph_op_name: "InvertPermutation"
  endpoint {
    name: "InvertPermutation"
  }
  summary: "Computes the inverse permutation of a tensor."
  description: <<END
This operation computes the inverse of an index permutation. It takes a 1-D
integer tensor `x`, which represents the indices of a zero-based array, and
swaps each value with its index position. In other words, for an output tensor
`y` and an input tensor `x`, this operation computes the following:

`y[x[i]] = i for i in [0, 1, ..., len(x) - 1]`

The values must include 0. There can be no duplicate values or negative values.

For example:

```
# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
```
END
}
op {
  graph_op_name: "IsFinite"
  endpoint {
    name: "IsFinite"
  }
  summary: "Returns which elements of x are finite."
  description: <<END
@compatibility(numpy)
Equivalent to np.isfinite
@end_compatibility
END
}
op {
  graph_op_name: "IsInf"
  endpoint {
    name: "IsInf"
  }
  summary: "Returns which elements of x are Inf."
  description: <<END
@compatibility(numpy)
Equivalent to np.isinf
@end_compatibility
END
}
op {
  graph_op_name: "IsNan"
  endpoint {
    name: "IsNan"
  }
  summary: "Returns which elements of x are NaN."
  description: <<END
@compatibility(numpy)
Equivalent to np.isnan
@end_compatibility
END
}
op {
  graph_op_name: "IsVariableInitialized"
  endpoint {
    name: "IsVariableInitialized"
  }
  summary: "Checks whether a tensor has been initialized."
  description: <<END
Outputs boolean scalar indicating whether the tensor has been initialized.
END
}
op {
  graph_op_name: "Iterator"
  endpoint {
    name: "Iterator"
  }
  summary: "A container for an iterator resource."
}
op {
  graph_op_name: "IteratorFromStringHandle"
  endpoint {
    name: "IteratorFromStringHandle"
  }
  summary: "Converts the given string representing a handle to an iterator to a resource."
}
op {
  graph_op_name: "IteratorGetNext"
  endpoint {
    name: "IteratorGetNext"
  }
  summary: "Gets the next output from the given iterator."
}
op {
  graph_op_name: "IteratorToStringHandle"
  endpoint {
    name: "IteratorToStringHandle"
  }
  summary: "Converts the given `resource_handle` representing an iterator to a string."
}
