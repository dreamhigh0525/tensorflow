op {
  graph_op_name: "Barrier"
  endpoint {
    name: "Barrier"
  }
  summary: "Defines a barrier that persists across different graph executions."
  description: <<END
A barrier represents a key-value map, where each key is a string, and
each value is a tuple of tensors.

At runtime, the barrier contains 'complete' and 'incomplete'
elements. A complete element has defined tensors for all components of
its value tuple, and may be accessed using BarrierTakeMany. An
incomplete element has some undefined components in its value tuple,
and may be updated using BarrierInsertMany.
END
}
op {
  graph_op_name: "BarrierClose"
  endpoint {
    name: "BarrierClose"
  }
  summary: "Closes the given barrier."
  description: <<END
This operation signals that no more new elements will be inserted in the
given barrier. Subsequent InsertMany that try to introduce a new key will fail.
Subsequent InsertMany operations that just add missing components to already
existing elements will continue to succeed. Subsequent TakeMany operations will
continue to succeed if sufficient completed elements remain in the barrier.
Subsequent TakeMany operations that would block will fail immediately.
END
}
op {
  graph_op_name: "BarrierIncompleteSize"
  endpoint {
    name: "BarrierIncompleteSize"
  }
  summary: "Computes the number of incomplete elements in the given barrier."
}
op {
  graph_op_name: "BarrierInsertMany"
  endpoint {
    name: "BarrierInsertMany"
  }
  summary: "For each key, assigns the respective value to the specified component."
  description: <<END
If a key is not found in the barrier, this operation will create a new
incomplete element. If a key is found in the barrier, and the element
already has a value at component_index, this operation will fail with
INVALID_ARGUMENT, and leave the barrier in an undefined state.
END
}
op {
  graph_op_name: "BarrierReadySize"
  endpoint {
    name: "BarrierReadySize"
  }
  summary: "Computes the number of complete elements in the given barrier."
}
op {
  graph_op_name: "BarrierTakeMany"
  endpoint {
    name: "BarrierTakeMany"
  }
  summary: "Takes the given number of completed elements from a barrier."
  description: <<END
This operation concatenates completed-element component tensors along
the 0th dimension to make a single component tensor.

Elements come out of the barrier when they are complete, and in the order
in which they were placed into the barrier.  The indices output provides
information about the batch in which each element was originally inserted
into the barrier.
END
}
op {
  graph_op_name: "BatchCholesky"
  endpoint {
    name: "BatchCholesky"
  }
}
op {
  graph_op_name: "BatchCholeskyGrad"
  endpoint {
    name: "BatchCholeskyGrad"
  }
}
op {
  graph_op_name: "BatchDataset"
  endpoint {
    name: "BatchDataset"
  }
  summary: "Creates a dataset that batches `batch_size` elements from `input_dataset`."
}
op {
  graph_op_name: "BatchFFT"
  endpoint {
    name: "BatchFFT"
  }
}
op {
  graph_op_name: "BatchFFT2D"
  endpoint {
    name: "BatchFFT2D"
  }
}
op {
  graph_op_name: "BatchFFT3D"
  endpoint {
    name: "BatchFFT3D"
  }
}
op {
  graph_op_name: "BatchIFFT"
  endpoint {
    name: "BatchIFFT"
  }
}
op {
  graph_op_name: "BatchIFFT2D"
  endpoint {
    name: "BatchIFFT2D"
  }
}
op {
  graph_op_name: "BatchIFFT3D"
  endpoint {
    name: "BatchIFFT3D"
  }
}
op {
  graph_op_name: "BatchMatMul"
  endpoint {
    name: "BatchMatMul"
  }
  summary: "Multiplies slices of two tensors in batches."
  description: <<END
Multiplies all slices of `Tensor` `x` and `y` (each slice can be
viewed as an element of a batch), and arranges the individual results
in a single output tensor of the same batch size. Each of the
individual slices can optionally be adjointed (to adjoint a matrix
means to transpose and conjugate it) before multiplication by setting
the `adj_x` or `adj_y` flag to `True`, which are by default `False`.

The input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`
and `[..., r_y, c_y]`.

The output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:

    r_o = c_x if adj_x else r_x
    c_o = r_y if adj_y else c_y

It is computed as:

    output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])
END
}
op {
  graph_op_name: "BatchMatrixBandPart"
  endpoint {
    name: "BatchMatrixBandPart"
  }
}
op {
  graph_op_name: "BatchMatrixDeterminant"
  endpoint {
    name: "BatchMatrixDeterminant"
  }
}
op {
  graph_op_name: "BatchMatrixDiag"
  endpoint {
    name: "BatchMatrixDiag"
  }
}
op {
  graph_op_name: "BatchMatrixDiagPart"
  endpoint {
    name: "BatchMatrixDiagPart"
  }
}
op {
  graph_op_name: "BatchMatrixInverse"
  endpoint {
    name: "BatchMatrixInverse"
  }
}
op {
  graph_op_name: "BatchMatrixSetDiag"
  endpoint {
    name: "BatchMatrixSetDiag"
  }
}
op {
  graph_op_name: "BatchMatrixSolve"
  endpoint {
    name: "BatchMatrixSolve"
  }
}
op {
  graph_op_name: "BatchMatrixSolveLs"
  endpoint {
    name: "BatchMatrixSolveLs"
  }
}
op {
  graph_op_name: "BatchMatrixTriangularSolve"
  endpoint {
    name: "BatchMatrixTriangularSolve"
  }
}
op {
  graph_op_name: "BatchNormWithGlobalNormalization"
  endpoint {
    name: "BatchNormWithGlobalNormalization"
  }
  summary: "Batch normalization."
  description: <<END
This op is deprecated. Prefer `tf.nn.batch_normalization`.
END
}
op {
  graph_op_name: "BatchNormWithGlobalNormalizationGrad"
  endpoint {
    name: "BatchNormWithGlobalNormalizationGrad"
  }
  summary: "Gradients for batch normalization."
  description: <<END
This op is deprecated. See `tf.nn.batch_normalization`.
END
}
op {
  graph_op_name: "BatchSelfAdjointEig"
  endpoint {
    name: "BatchSelfAdjointEig"
  }
}
op {
  graph_op_name: "BatchSelfAdjointEigV2"
  endpoint {
    name: "BatchSelfAdjointEigV2"
  }
}
op {
  graph_op_name: "BatchSvd"
  endpoint {
    name: "BatchSvd"
  }
}
op {
  graph_op_name: "BatchToSpace"
  endpoint {
    name: "BatchToSpace"
  }
  summary: "BatchToSpace for 4-D tensors of type T."
  description: <<END
This is a legacy version of the more general BatchToSpaceND.

Rearranges (permutes) data from batch into blocks of spatial data, followed by
cropping. This is the reverse transformation of SpaceToBatch. More specifically,
this op outputs a copy of the input tensor where values from the `batch`
dimension are moved in spatial blocks to the `height` and `width` dimensions,
followed by cropping along the `height` and `width` dimensions.
END
}
op {
  graph_op_name: "BatchToSpaceND"
  endpoint {
    name: "BatchToSpaceND"
  }
  summary: "BatchToSpace for N-D tensors of type T."
  description: <<END
This operation reshapes the "batch" dimension 0 into `M + 1` dimensions of shape
`block_shape + [batch]`, interleaves these blocks back into the grid defined by
the spatial dimensions `[1, ..., M]`, to obtain a result with the same rank as
the input.  The spatial dimensions of this intermediate result are then
optionally cropped according to `crops` to produce the output.  This is the
reverse of SpaceToBatch.  See below for a precise description.
END
}
op {
  graph_op_name: "Betainc"
  endpoint {
    name: "Betainc"
  }
  summary: "Compute the regularized incomplete beta integral \\\\(I_x(a, b)\\\\)."
  description: <<END
The regularized incomplete beta integral is defined as:


\\(I_x(a, b) = \frac{B(x; a, b)}{B(a, b)}\\)

where


\\(B(x; a, b) = \int_0^x t^{a-1} (1 - t)^{b-1} dt\\)


is the incomplete beta function and \\(B(a, b)\\) is the *complete*
beta function.
END
}
op {
  graph_op_name: "BiasAdd"
  endpoint {
    name: "BiasAdd"
  }
  summary: "Adds `bias` to `value`."
  description: <<END
This is a special case of `tf.add` where `bias` is restricted to be 1-D.
Broadcasting is supported, so `value` may have any number of dimensions.
END
}
op {
  graph_op_name: "BiasAddGrad"
  endpoint {
    name: "BiasAddGrad"
  }
  summary: "The backward operation for \"BiasAdd\" on the \"bias\" tensor."
  description: <<END
It accumulates all the values from out_backprop into the feature dimension.
For NHWC data format, the feature dimension is the last. For NCHW data format,
the feature dimension is the third-to-last.
END
}
op {
  graph_op_name: "BiasAddV1"
  endpoint {
    name: "BiasAddV1"
  }
  summary: "Adds `bias` to `value`."
  description: <<END
This is a deprecated version of BiasAdd and will be soon removed.

This is a special case of `tf.add` where `bias` is restricted to be 1-D.
Broadcasting is supported, so `value` may have any number of dimensions.
END
}
op {
  graph_op_name: "Bincount"
  endpoint {
    name: "Bincount"
  }
  summary: "Counts the number of occurrences of each value in an integer array."
  description: <<END
Outputs a vector with length `size` and the same dtype as `weights`. If
`weights` are empty, then index `i` stores the number of times the value `i` is
counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of
the value in `weights` at each index where the corresponding value in `arr` is
`i`.

Values in `arr` outside of the range [0, size) are ignored.
END
}
op {
  graph_op_name: "Bitcast"
  endpoint {
    name: "Bitcast"
  }
  summary: "Bitcasts a tensor from one type to another without copying data."
  description: <<END
Given a tensor `input`, this operation returns a tensor that has the same buffer
data as `input` with datatype `type`.

If the input datatype `T` is larger than the output datatype `type` then the
shape changes from [...] to [..., sizeof(`T`)/sizeof(`type`)].

If `T` is smaller than `type`, the operator requires that the rightmost
dimension be equal to sizeof(`type`)/sizeof(`T`). The shape then goes from
[..., sizeof(`type`)/sizeof(`T`)] to [...].

*NOTE*: Bitcast is implemented as a low-level cast, so machines with different
endian orderings will give different results.
END
}
op {
  graph_op_name: "BitwiseAnd"
  endpoint {
    name: "BitwiseAnd"
  }
  summary: "Elementwise computes the bitwise AND of `x` and `y`."
  description: <<END
The result will have those bits set, that are set in both `x` and `y`. The
computation is performed on the underlying representations of `x` and `y`.
END
}
op {
  graph_op_name: "BitwiseOr"
  endpoint {
    name: "BitwiseOr"
  }
  summary: "Elementwise computes the bitwise OR of `x` and `y`."
  description: <<END
The result will have those bits set, that are set in `x`, `y` or both. The
computation is performed on the underlying representations of `x` and `y`.
END
}
op {
  graph_op_name: "BitwiseXor"
  endpoint {
    name: "BitwiseXor"
  }
  summary: "Elementwise computes the bitwise XOR of `x` and `y`."
  description: <<END
The result will have those bits set, that are different in `x` and `y`. The
computation is performed on the underlying representations of `x` and `y`.
END
}
op {
  graph_op_name: "BroadcastArgs"
  endpoint {
    name: "BroadcastArgs"
  }
  summary: "Return the shape of s0 op s1 with broadcast."
  description: <<END
Given `s0` and `s1`, tensors that represent shapes, compute `r0`, the
broadcasted shape. `s0`, `s1` and `r0` are all integer vectors.
END
}
op {
  graph_op_name: "BroadcastGradientArgs"
  endpoint {
    name: "BroadcastGradientArgs"
  }
  summary: "Return the reduction indices for computing gradients of s0 op s1 with broadcast."
  description: <<END
This is typically used by gradient computations for a broadcasting operation.
END
}
op {
  graph_op_name: "Bucketize"
  endpoint {
    name: "Bucketize"
  }
  summary: "Bucketizes \'input\' based on \'boundaries\'."
  description: <<END
For example, if the inputs are
    boundaries = [0, 10, 100]
    input = [[-5, 10000]
             [150,   10]
             [5,    100]]

then the output will be
    output = [[0, 3]
              [3, 2]
              [1, 3]]
END
}
