// RUN: hlo_to_llvm_ir %s | FileCheck %s

// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// CHECK-LABEL: entry:
// CHECK:         %[[VAL_0:.*]] = getelementptr inbounds i8, i8* %[[VAL_1:.*]], i64 0
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_4:.*]], i64 0
// CHECK:         %[[VAL_5:.*]] = bitcast i8* %[[VAL_3]] to float*
// CHECK:         %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_7:.*]], i64 0
// CHECK:         %[[VAL_8:.*]] = bitcast i8* %[[VAL_6]] to float*
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds i8, i8* %[[VAL_10:.*]], i64 0
// CHECK:         %[[VAL_11:.*]] = bitcast i8* %[[VAL_9]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_12:.*]] = getelementptr inbounds i8, i8* %[[VAL_13:.*]], i64 0
// CHECK:         %[[VAL_14:.*]] = bitcast i8* %[[VAL_12]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_15:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK:         %[[VAL_16:.*]] = zext i32 %[[VAL_15]] to i64
// CHECK:         %[[VAL_17:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK:         %[[VAL_18:.*]] = zext i32 %[[VAL_17]] to i64
// CHECK:         %[[VAL_19:.*]] = mul nuw nsw i64 %[[VAL_16]], 64
// CHECK:         %[[VAL_20:.*]] = add nuw nsw i64 %[[VAL_19]], %[[VAL_18]]
// CHECK:         %[[VAL_21:.*]] = icmp ult i64 %[[VAL_20]], 64
// CHECK:         call void @llvm.assume(i1 %[[VAL_21]])
// CHECK:         %[[VAL_22:.*]] = udiv i64 %[[VAL_20]], 1
// CHECK:         %[[VAL_23:.*]] = urem i64 %[[VAL_22]], 32
// CHECK:         %[[VAL_24:.*]] = udiv i64 %[[VAL_20]], 32
// CHECK:         %[[VAL_25:.*]] = icmp ult i64 %[[VAL_20]], 64
// CHECK:         br i1 %[[VAL_25]], label %[[VAL_26:.*]], label %[[VAL_27:.*]]
// CHECK:       fusion.in_bounds-after:                           ; preds = %[[VAL_26]], %[[VAL_28:.*]]
// CHECK:         ret void
// CHECK:       fusion.in_bounds-true:                            ; preds = %[[VAL_28]]
// CHECK:         %[[VAL_29:.*]] = load float, float* %[[VAL_5]], align 4, !invariant.load !8
// CHECK:         %[[VAL_30:.*]] = bitcast [2 x [32 x float]]* %[[VAL_11]] to float*
// CHECK:         %[[VAL_31:.*]] = getelementptr inbounds float, float* %[[VAL_30]], i64 %[[VAL_20]]
// CHECK:         store float %[[VAL_29]], float* %[[VAL_31]], align 4
// CHECK:         br label %[[VAL_27]]
// CHECK:       entry:
// CHECK:         %[[VAL_32:.*]] = getelementptr inbounds i8, i8* %[[VAL_33:.*]], i64 0
// CHECK:         %[[VAL_34:.*]] = bitcast i8* %[[VAL_32]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_35:.*]] = getelementptr inbounds i8, i8* %[[VAL_36:.*]], i64 0
// CHECK:         %[[VAL_37:.*]] = bitcast i8* %[[VAL_35]] to float*
// CHECK:         %[[VAL_38:.*]] = getelementptr inbounds i8, i8* %[[VAL_39:.*]], i64 0
// CHECK:         %[[VAL_40:.*]] = bitcast i8* %[[VAL_38]] to float*
// CHECK:         %[[VAL_41:.*]] = getelementptr inbounds i8, i8* %[[VAL_42:.*]], i64 0
// CHECK:         %[[VAL_43:.*]] = bitcast i8* %[[VAL_41]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_44:.*]] = getelementptr inbounds i8, i8* %[[VAL_45:.*]], i64 0
// CHECK:         %[[VAL_46:.*]] = bitcast i8* %[[VAL_44]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_47:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK:         %[[VAL_48:.*]] = zext i32 %[[VAL_47]] to i64
// CHECK:         %[[VAL_49:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK:         %[[VAL_50:.*]] = zext i32 %[[VAL_49]] to i64
// CHECK:         %[[VAL_51:.*]] = mul nuw nsw i64 %[[VAL_48]], 64
// CHECK:         %[[VAL_52:.*]] = add nuw nsw i64 %[[VAL_51]], %[[VAL_50]]
// CHECK:         %[[VAL_53:.*]] = icmp ult i64 %[[VAL_52]], 64
// CHECK:         call void @llvm.assume(i1 %[[VAL_53]])
// CHECK:         %[[VAL_54:.*]] = udiv i64 %[[VAL_52]], 1
// CHECK:         %[[VAL_55:.*]] = urem i64 %[[VAL_54]], 32
// CHECK:         %[[VAL_56:.*]] = udiv i64 %[[VAL_52]], 32
// CHECK:         %[[VAL_57:.*]] = icmp ult i64 %[[VAL_52]], 64
// CHECK:         br i1 %[[VAL_57]], label %[[VAL_58:.*]], label %[[VAL_59:.*]]
// CHECK:       fusion.in_bounds-after:                           ; preds = %[[VAL_58]], %[[VAL_60:.*]]
// CHECK:         ret void
// CHECK:       fusion.in_bounds-true:                            ; preds = %[[VAL_60]]
// CHECK:         %[[VAL_61:.*]] = load float, float* %[[VAL_40]], align 4, !invariant.load !8
// CHECK:         %[[VAL_62:.*]] = bitcast [2 x [32 x float]]* %[[VAL_46]] to float*
// CHECK:         %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i64 %[[VAL_52]]
// CHECK:         store float %[[VAL_61]], float* %[[VAL_63]], align 4
// CHECK:         br label %[[VAL_59]]
// CHECK:       entry:
// CHECK:         %[[VAL_64:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_65:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_66:.*]] = alloca float, align 4
// CHECK:         %[[VAL_67:.*]] = alloca float, align 4
// CHECK:         %[[VAL_68:.*]] = alloca float, align 4
// CHECK:         %[[VAL_69:.*]] = alloca float, align 4
// CHECK:         %[[VAL_70:.*]] = alloca float, align 4
// CHECK:         %[[VAL_71:.*]] = alloca float, align 4
// CHECK:         %[[VAL_72:.*]] = alloca float, align 4
// CHECK:         %[[VAL_73:.*]] = alloca float, align 4
// CHECK:         %[[VAL_74:.*]] = alloca float, align 4
// CHECK:         %[[VAL_75:.*]] = alloca float, align 4
// CHECK:         %[[VAL_76:.*]] = alloca float, align 4
// CHECK:         %[[VAL_77:.*]] = alloca float, align 4
// CHECK:         %[[VAL_78:.*]] = alloca float, align 4
// CHECK:         %[[VAL_79:.*]] = alloca float, align 4
// CHECK:         %[[VAL_80:.*]] = alloca float, align 4
// CHECK:         %[[VAL_81:.*]] = alloca float, align 4
// CHECK:         %[[VAL_82:.*]] = alloca float, align 4
// CHECK:         %[[VAL_83:.*]] = alloca float, align 4
// CHECK:         %[[VAL_84:.*]] = alloca float, align 4
// CHECK:         %[[VAL_85:.*]] = alloca float, align 4
// CHECK:         %[[VAL_86:.*]] = alloca float, align 4
// CHECK:         %[[VAL_87:.*]] = alloca float, align 4
// CHECK:         %[[VAL_88:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_89:.*]] = alloca float, align 4
// CHECK:         %[[VAL_90:.*]] = alloca float, align 4
// CHECK:         %[[VAL_91:.*]] = alloca float, align 4
// CHECK:         %[[VAL_92:.*]] = alloca float, align 4
// CHECK:         %[[VAL_93:.*]] = getelementptr inbounds i8, i8* %[[VAL_94:.*]], i64 0
// CHECK:         %[[VAL_95:.*]] = bitcast i8* %[[VAL_93]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_96:.*]] = getelementptr inbounds i8, i8* %[[VAL_97:.*]], i64 0
// CHECK:         %[[VAL_98:.*]] = bitcast i8* %[[VAL_96]] to float*
// CHECK:         %[[VAL_99:.*]] = getelementptr inbounds i8, i8* %[[VAL_100:.*]], i64 0
// CHECK:         %[[VAL_101:.*]] = bitcast i8* %[[VAL_99]] to float*
// CHECK:         %[[VAL_102:.*]] = getelementptr inbounds i8, i8* %[[VAL_103:.*]], i64 0
// CHECK:         %[[VAL_104:.*]] = bitcast i8* %[[VAL_102]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_105:.*]] = getelementptr inbounds i8, i8* %[[VAL_106:.*]], i64 0
// CHECK:         %[[VAL_107:.*]] = bitcast i8* %[[VAL_105]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_108:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(), !range !6
// CHECK:         %[[VAL_109:.*]] = icmp eq i32 %[[VAL_108]], 0
// CHECK:         br i1 %[[VAL_109]], label %[[VAL_110:.*]], label %[[VAL_111:.*]]
// CHECK:       reduce-group-0-after:                             ; preds = %[[VAL_112:.*]], %[[VAL_113:.*]]
// CHECK:         ret void
// CHECK:       reduce-group-0-true:                              ; preds = %[[VAL_113]]
// CHECK:         %[[VAL_114:.*]] = load float, float* %[[VAL_98]], align 4, !invariant.load !8
// CHECK:         %[[VAL_115:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         store float %[[VAL_114]], float* %[[VAL_115]], align 4
// CHECK:         %[[VAL_116:.*]] = load float, float* %[[VAL_101]], align 4, !invariant.load !8
// CHECK:         %[[VAL_117:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         store float %[[VAL_116]], float* %[[VAL_117]], align 4
// CHECK:         %[[VAL_118:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !9
// CHECK:         %[[VAL_119:.*]] = urem i32 %[[VAL_118]], 32
// CHECK:         %[[VAL_120:.*]] = udiv i32 %[[VAL_118]], 32
// CHECK:         %[[VAL_121:.*]] = urem i32 %[[VAL_118]], 32
// CHECK:         %[[VAL_122:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !7
// CHECK:         %[[VAL_123:.*]] = udiv i32 %[[VAL_122]], 1
// CHECK:         %[[VAL_124:.*]] = urem i32 %[[VAL_123]], 1
// CHECK:         %[[VAL_125:.*]] = udiv i32 %[[VAL_122]], 1
// CHECK:         %[[VAL_126:.*]] = urem i32 %[[VAL_125]], 64
// CHECK:         %[[VAL_127:.*]] = udiv i32 %[[VAL_122]], 64
// CHECK:         %[[VAL_128:.*]] = mul i32 %[[VAL_127]], 1
// CHECK:         %[[VAL_129:.*]] = icmp eq i32 %[[VAL_126]], 63
// CHECK:         %[[VAL_130:.*]] = select i1 %[[VAL_129]], i32 1, i32 1
// CHECK:         %[[VAL_131:.*]] = icmp eq i32 %[[VAL_124]], 0
// CHECK:         %[[VAL_132:.*]] = select i1 %[[VAL_131]], i32 32, i32 256
// CHECK:         %[[VAL_133:.*]] = mul i32 %[[VAL_126]], 1
// CHECK:         %[[VAL_134:.*]] = mul i32 %[[VAL_124]], 256
// CHECK:         %[[VAL_135:.*]] = mul i32 %[[VAL_119]], 2
// CHECK:         %[[VAL_136:.*]] = add i32 %[[VAL_134]], %[[VAL_135]]
// CHECK:         store i32 %[[VAL_120]], i32* %[[VAL_88]], align 4
// CHECK:         br label %[[VAL_137:.*]]
// CHECK:       output_y_in_tile.loop_header:                     ; preds = %[[VAL_138:.*]], %[[VAL_110]]
// CHECK:         %[[VAL_139:.*]] = load i32, i32* %[[VAL_88]], align 4
// CHECK:         %[[VAL_140:.*]] = icmp uge i32 %[[VAL_139]], %[[VAL_130]]
// CHECK:         br i1 %[[VAL_140]], label %[[VAL_141:.*]], label %[[VAL_142:.*]]
// CHECK:       output_y_in_tile.loop_body:                       ; preds = %[[VAL_137]]
// CHECK:         %[[VAL_143:.*]] = add nuw nsw i32 %[[VAL_139]], 1
// CHECK:         store i32 %[[VAL_143]], i32* %[[VAL_88]], align 4
// CHECK:         %[[VAL_144:.*]] = icmp eq i32 %[[VAL_139]], %[[VAL_120]]
// CHECK:         %[[VAL_145:.*]] = icmp eq i32 256, %[[VAL_132]]
// CHECK:         br i1 %[[VAL_145]], label %[[VAL_146:.*]], label %[[VAL_147:.*]]
// CHECK:       output_is_full_tile-after:                        ; preds = %[[VAL_148:.*]], %[[VAL_146]]
// CHECK:         br label %[[VAL_137]], !llvm.loop !10
// CHECK:       output_y_in_tile.loop_exit:                       ; preds = %[[VAL_137]]
// CHECK:         %[[VAL_149:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !9
// CHECK:         %[[VAL_150:.*]] = urem i32 %[[VAL_149]], 32
// CHECK:         %[[VAL_151:.*]] = udiv i32 %[[VAL_149]], 32
// CHECK:         %[[VAL_152:.*]] = urem i32 %[[VAL_149]], 32
// CHECK:         %[[VAL_153:.*]] = mul i32 %[[VAL_150]], 2
// CHECK:         %[[VAL_154:.*]] = add i32 %[[VAL_133]], %[[VAL_151]]
// CHECK:         %[[VAL_155:.*]] = add i32 %[[VAL_134]], %[[VAL_153]]
// CHECK:         %[[VAL_156:.*]] = add i32 %[[VAL_155]], 0
// CHECK:         %[[VAL_157:.*]] = udiv i32 %[[VAL_154]], 1
// CHECK:         %[[VAL_158:.*]] = urem i32 %[[VAL_157]], 32
// CHECK:         %[[VAL_159:.*]] = udiv i32 %[[VAL_154]], 32
// CHECK:         %[[VAL_160:.*]] = getelementptr inbounds [2 x [32 x float]], [2 x [32 x float]]* %[[VAL_104]], i32 0, i32 %[[VAL_159]], i32 %[[VAL_158]]
// CHECK:         %[[VAL_161:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         %[[VAL_162:.*]] = load float, float* %[[VAL_161]], align 4
// CHECK:         %[[VAL_163:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_162]], i32 16, i32 31)
// CHECK:         store float %[[VAL_163]], float* %[[VAL_87]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_161]], float* %[[VAL_87]], float* %[[VAL_161]])
// CHECK:         %[[VAL_164:.*]] = load float, float* %[[VAL_161]], align 4
// CHECK:         %[[VAL_165:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_164]], i32 8, i32 31)
// CHECK:         store float %[[VAL_165]], float* %[[VAL_86]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_161]], float* %[[VAL_86]], float* %[[VAL_161]])
// CHECK:         %[[VAL_166:.*]] = load float, float* %[[VAL_161]], align 4
// CHECK:         %[[VAL_167:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_166]], i32 4, i32 31)
// CHECK:         store float %[[VAL_167]], float* %[[VAL_85]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_161]], float* %[[VAL_85]], float* %[[VAL_161]])
// CHECK:         %[[VAL_168:.*]] = load float, float* %[[VAL_161]], align 4
// CHECK:         %[[VAL_169:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_168]], i32 2, i32 31)
// CHECK:         store float %[[VAL_169]], float* %[[VAL_84]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_161]], float* %[[VAL_84]], float* %[[VAL_161]])
// CHECK:         %[[VAL_170:.*]] = load float, float* %[[VAL_161]], align 4
// CHECK:         %[[VAL_171:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_170]], i32 1, i32 31)
// CHECK:         store float %[[VAL_171]], float* %[[VAL_83]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_161]], float* %[[VAL_83]], float* %[[VAL_161]])
// CHECK:         %[[VAL_172:.*]] = udiv i32 %[[VAL_150]], 32
// CHECK:         %[[VAL_173:.*]] = icmp eq i32 %[[VAL_152]], 0
// CHECK:         br i1 %[[VAL_173]], label %[[VAL_174:.*]], label %[[VAL_175:.*]]
// CHECK:       intra_warp_reduce_write-after:                    ; preds = %[[VAL_174]], %[[VAL_141]]
// CHECK:         call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_176:.*]] = icmp eq i32 %[[VAL_172]], 0
// CHECK:         br i1 %[[VAL_176]], label %[[VAL_177:.*]], label %[[VAL_178:.*]]
// CHECK:       inter_warp_reduce-after:                          ; preds = %[[VAL_179:.*]], %[[VAL_175]]
// CHECK:         %[[VAL_180:.*]] = add i32 %[[VAL_155]], 0
// CHECK:         %[[VAL_181:.*]] = udiv i32 %[[VAL_154]], 1
// CHECK:         %[[VAL_182:.*]] = urem i32 %[[VAL_181]], 32
// CHECK:         %[[VAL_183:.*]] = udiv i32 %[[VAL_154]], 32
// CHECK:         %[[VAL_184:.*]] = getelementptr inbounds [2 x [32 x float]], [2 x [32 x float]]* %[[VAL_107]], i32 0, i32 %[[VAL_183]], i32 %[[VAL_182]]
// CHECK:         %[[VAL_185:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         %[[VAL_186:.*]] = load float, float* %[[VAL_185]], align 4
// CHECK:         %[[VAL_187:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_186]], i32 16, i32 31)
// CHECK:         store float %[[VAL_187]], float* %[[VAL_76]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_185]], float* %[[VAL_76]], float* %[[VAL_185]])
// CHECK:         %[[VAL_188:.*]] = load float, float* %[[VAL_185]], align 4
// CHECK:         %[[VAL_189:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_188]], i32 8, i32 31)
// CHECK:         store float %[[VAL_189]], float* %[[VAL_75]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_185]], float* %[[VAL_75]], float* %[[VAL_185]])
// CHECK:         %[[VAL_190:.*]] = load float, float* %[[VAL_185]], align 4
// CHECK:         %[[VAL_191:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_190]], i32 4, i32 31)
// CHECK:         store float %[[VAL_191]], float* %[[VAL_74]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_185]], float* %[[VAL_74]], float* %[[VAL_185]])
// CHECK:         %[[VAL_192:.*]] = load float, float* %[[VAL_185]], align 4
// CHECK:         %[[VAL_193:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_192]], i32 2, i32 31)
// CHECK:         store float %[[VAL_193]], float* %[[VAL_73]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_185]], float* %[[VAL_73]], float* %[[VAL_185]])
// CHECK:         %[[VAL_194:.*]] = load float, float* %[[VAL_185]], align 4
// CHECK:         %[[VAL_195:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_194]], i32 1, i32 31)
// CHECK:         store float %[[VAL_195]], float* %[[VAL_72]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_185]], float* %[[VAL_72]], float* %[[VAL_185]])
// CHECK:         %[[VAL_196:.*]] = udiv i32 %[[VAL_150]], 32
// CHECK:         %[[VAL_197:.*]] = icmp eq i32 %[[VAL_152]], 0
// CHECK:         br i1 %[[VAL_197]], label %[[VAL_198:.*]], label %[[VAL_199:.*]]
// CHECK:       intra_warp_reduce_write-after129:                 ; preds = %[[VAL_198]], %[[VAL_178]]
// CHECK:         call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_200:.*]] = icmp eq i32 %[[VAL_196]], 0
// CHECK:         br i1 %[[VAL_200]], label %[[VAL_201:.*]], label %[[VAL_112]]
// CHECK:       inter_warp_reduce-after131:                       ; preds = %[[VAL_202:.*]], %[[VAL_199]]
// CHECK:         br label %[[VAL_111]]
// CHECK:       output_is_full_tile-true:                         ; preds = %[[VAL_142]]
// CHECK:         %[[VAL_203:.*]] = add i32 %[[VAL_133]], %[[VAL_139]]
// CHECK:         %[[VAL_204:.*]] = add i32 0, %[[VAL_135]]
// CHECK:         %[[VAL_205:.*]] = add i32 %[[VAL_136]], 0
// CHECK:         %[[VAL_206:.*]] = mul nuw nsw i32 %[[VAL_205]], 1
// CHECK:         %[[VAL_207:.*]] = add nuw nsw i32 0, %[[VAL_206]]
// CHECK:         %[[VAL_208:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_209:.*]] = add nuw nsw i32 %[[VAL_207]], %[[VAL_208]]
// CHECK:         %[[VAL_210:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_211:.*]] = add nuw nsw i32 %[[VAL_209]], %[[VAL_210]]
// CHECK:         %[[VAL_212:.*]] = udiv i32 %[[VAL_211]], 1
// CHECK:         %[[VAL_213:.*]] = urem i32 %[[VAL_212]], 32
// CHECK:         %[[VAL_214:.*]] = udiv i32 %[[VAL_211]], 32
// CHECK:         %[[VAL_215:.*]] = urem i32 %[[VAL_214]], 32
// CHECK:         %[[VAL_216:.*]] = udiv i32 %[[VAL_211]], 1024
// CHECK:         %[[VAL_217:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_218:.*]] = getelementptr inbounds float, float* %[[VAL_217]], i32 %[[VAL_211]]
// CHECK:         %[[VAL_219:.*]] = load float, float* %[[VAL_218]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_219]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_220:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_220]], float* %[[VAL_92]], float* %[[VAL_220]])
// CHECK:         %[[VAL_221:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_222:.*]] = getelementptr inbounds float, float* %[[VAL_221]], i32 %[[VAL_211]]
// CHECK:         %[[VAL_223:.*]] = load float, float* %[[VAL_222]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_223]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_224:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_224]], float* %[[VAL_90]], float* %[[VAL_224]])
// CHECK:         %[[VAL_225:.*]] = add i32 1, %[[VAL_135]]
// CHECK:         %[[VAL_226:.*]] = add i32 %[[VAL_136]], 1
// CHECK:         %[[VAL_227:.*]] = mul nuw nsw i32 %[[VAL_226]], 1
// CHECK:         %[[VAL_228:.*]] = add nuw nsw i32 0, %[[VAL_227]]
// CHECK:         %[[VAL_229:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_230:.*]] = add nuw nsw i32 %[[VAL_228]], %[[VAL_229]]
// CHECK:         %[[VAL_231:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_232:.*]] = add nuw nsw i32 %[[VAL_230]], %[[VAL_231]]
// CHECK:         %[[VAL_233:.*]] = udiv i32 %[[VAL_232]], 1
// CHECK:         %[[VAL_234:.*]] = urem i32 %[[VAL_233]], 32
// CHECK:         %[[VAL_235:.*]] = udiv i32 %[[VAL_232]], 32
// CHECK:         %[[VAL_236:.*]] = urem i32 %[[VAL_235]], 32
// CHECK:         %[[VAL_237:.*]] = udiv i32 %[[VAL_232]], 1024
// CHECK:         %[[VAL_238:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_239:.*]] = getelementptr inbounds float, float* %[[VAL_238]], i32 %[[VAL_232]]
// CHECK:         %[[VAL_240:.*]] = load float, float* %[[VAL_239]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_240]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_241:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_241]], float* %[[VAL_92]], float* %[[VAL_241]])
// CHECK:         %[[VAL_242:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_243:.*]] = getelementptr inbounds float, float* %[[VAL_242]], i32 %[[VAL_232]]
// CHECK:         %[[VAL_244:.*]] = load float, float* %[[VAL_243]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_244]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_245:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_245]], float* %[[VAL_90]], float* %[[VAL_245]])
// CHECK:         %[[VAL_246:.*]] = add i32 64, %[[VAL_135]]
// CHECK:         %[[VAL_247:.*]] = add i32 %[[VAL_136]], 64
// CHECK:         %[[VAL_248:.*]] = mul nuw nsw i32 %[[VAL_247]], 1
// CHECK:         %[[VAL_249:.*]] = add nuw nsw i32 0, %[[VAL_248]]
// CHECK:         %[[VAL_250:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_251:.*]] = add nuw nsw i32 %[[VAL_249]], %[[VAL_250]]
// CHECK:         %[[VAL_252:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_253:.*]] = add nuw nsw i32 %[[VAL_251]], %[[VAL_252]]
// CHECK:         %[[VAL_254:.*]] = udiv i32 %[[VAL_253]], 1
// CHECK:         %[[VAL_255:.*]] = urem i32 %[[VAL_254]], 32
// CHECK:         %[[VAL_256:.*]] = udiv i32 %[[VAL_253]], 32
// CHECK:         %[[VAL_257:.*]] = urem i32 %[[VAL_256]], 32
// CHECK:         %[[VAL_258:.*]] = udiv i32 %[[VAL_253]], 1024
// CHECK:         %[[VAL_259:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_260:.*]] = getelementptr inbounds float, float* %[[VAL_259]], i32 %[[VAL_253]]
// CHECK:         %[[VAL_261:.*]] = load float, float* %[[VAL_260]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_261]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_262:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_262]], float* %[[VAL_92]], float* %[[VAL_262]])
// CHECK:         %[[VAL_263:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_264:.*]] = getelementptr inbounds float, float* %[[VAL_263]], i32 %[[VAL_253]]
// CHECK:         %[[VAL_265:.*]] = load float, float* %[[VAL_264]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_265]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_266:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_266]], float* %[[VAL_90]], float* %[[VAL_266]])
// CHECK:         %[[VAL_267:.*]] = add i32 65, %[[VAL_135]]
// CHECK:         %[[VAL_268:.*]] = add i32 %[[VAL_136]], 65
// CHECK:         %[[VAL_269:.*]] = mul nuw nsw i32 %[[VAL_268]], 1
// CHECK:         %[[VAL_270:.*]] = add nuw nsw i32 0, %[[VAL_269]]
// CHECK:         %[[VAL_271:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_272:.*]] = add nuw nsw i32 %[[VAL_270]], %[[VAL_271]]
// CHECK:         %[[VAL_273:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_274:.*]] = add nuw nsw i32 %[[VAL_272]], %[[VAL_273]]
// CHECK:         %[[VAL_275:.*]] = udiv i32 %[[VAL_274]], 1
// CHECK:         %[[VAL_276:.*]] = urem i32 %[[VAL_275]], 32
// CHECK:         %[[VAL_277:.*]] = udiv i32 %[[VAL_274]], 32
// CHECK:         %[[VAL_278:.*]] = urem i32 %[[VAL_277]], 32
// CHECK:         %[[VAL_279:.*]] = udiv i32 %[[VAL_274]], 1024
// CHECK:         %[[VAL_280:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_281:.*]] = getelementptr inbounds float, float* %[[VAL_280]], i32 %[[VAL_274]]
// CHECK:         %[[VAL_282:.*]] = load float, float* %[[VAL_281]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_282]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_283:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_283]], float* %[[VAL_92]], float* %[[VAL_283]])
// CHECK:         %[[VAL_284:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_285:.*]] = getelementptr inbounds float, float* %[[VAL_284]], i32 %[[VAL_274]]
// CHECK:         %[[VAL_286:.*]] = load float, float* %[[VAL_285]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_286]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_287:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_287]], float* %[[VAL_90]], float* %[[VAL_287]])
// CHECK:         %[[VAL_288:.*]] = add i32 128, %[[VAL_135]]
// CHECK:         %[[VAL_289:.*]] = add i32 %[[VAL_136]], 128
// CHECK:         %[[VAL_290:.*]] = mul nuw nsw i32 %[[VAL_289]], 1
// CHECK:         %[[VAL_291:.*]] = add nuw nsw i32 0, %[[VAL_290]]
// CHECK:         %[[VAL_292:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_293:.*]] = add nuw nsw i32 %[[VAL_291]], %[[VAL_292]]
// CHECK:         %[[VAL_294:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_295:.*]] = add nuw nsw i32 %[[VAL_293]], %[[VAL_294]]
// CHECK:         %[[VAL_296:.*]] = udiv i32 %[[VAL_295]], 1
// CHECK:         %[[VAL_297:.*]] = urem i32 %[[VAL_296]], 32
// CHECK:         %[[VAL_298:.*]] = udiv i32 %[[VAL_295]], 32
// CHECK:         %[[VAL_299:.*]] = urem i32 %[[VAL_298]], 32
// CHECK:         %[[VAL_300:.*]] = udiv i32 %[[VAL_295]], 1024
// CHECK:         %[[VAL_301:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_302:.*]] = getelementptr inbounds float, float* %[[VAL_301]], i32 %[[VAL_295]]
// CHECK:         %[[VAL_303:.*]] = load float, float* %[[VAL_302]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_303]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_304:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_304]], float* %[[VAL_92]], float* %[[VAL_304]])
// CHECK:         %[[VAL_305:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_306:.*]] = getelementptr inbounds float, float* %[[VAL_305]], i32 %[[VAL_295]]
// CHECK:         %[[VAL_307:.*]] = load float, float* %[[VAL_306]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_307]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_308:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_308]], float* %[[VAL_90]], float* %[[VAL_308]])
// CHECK:         %[[VAL_309:.*]] = add i32 129, %[[VAL_135]]
// CHECK:         %[[VAL_310:.*]] = add i32 %[[VAL_136]], 129
// CHECK:         %[[VAL_311:.*]] = mul nuw nsw i32 %[[VAL_310]], 1
// CHECK:         %[[VAL_312:.*]] = add nuw nsw i32 0, %[[VAL_311]]
// CHECK:         %[[VAL_313:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_314:.*]] = add nuw nsw i32 %[[VAL_312]], %[[VAL_313]]
// CHECK:         %[[VAL_315:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_316:.*]] = add nuw nsw i32 %[[VAL_314]], %[[VAL_315]]
// CHECK:         %[[VAL_317:.*]] = udiv i32 %[[VAL_316]], 1
// CHECK:         %[[VAL_318:.*]] = urem i32 %[[VAL_317]], 32
// CHECK:         %[[VAL_319:.*]] = udiv i32 %[[VAL_316]], 32
// CHECK:         %[[VAL_320:.*]] = urem i32 %[[VAL_319]], 32
// CHECK:         %[[VAL_321:.*]] = udiv i32 %[[VAL_316]], 1024
// CHECK:         %[[VAL_322:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_323:.*]] = getelementptr inbounds float, float* %[[VAL_322]], i32 %[[VAL_316]]
// CHECK:         %[[VAL_324:.*]] = load float, float* %[[VAL_323]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_324]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_325:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_325]], float* %[[VAL_92]], float* %[[VAL_325]])
// CHECK:         %[[VAL_326:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_327:.*]] = getelementptr inbounds float, float* %[[VAL_326]], i32 %[[VAL_316]]
// CHECK:         %[[VAL_328:.*]] = load float, float* %[[VAL_327]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_328]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_329:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_329]], float* %[[VAL_90]], float* %[[VAL_329]])
// CHECK:         %[[VAL_330:.*]] = add i32 192, %[[VAL_135]]
// CHECK:         %[[VAL_331:.*]] = add i32 %[[VAL_136]], 192
// CHECK:         %[[VAL_332:.*]] = mul nuw nsw i32 %[[VAL_331]], 1
// CHECK:         %[[VAL_333:.*]] = add nuw nsw i32 0, %[[VAL_332]]
// CHECK:         %[[VAL_334:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_335:.*]] = add nuw nsw i32 %[[VAL_333]], %[[VAL_334]]
// CHECK:         %[[VAL_336:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_337:.*]] = add nuw nsw i32 %[[VAL_335]], %[[VAL_336]]
// CHECK:         %[[VAL_338:.*]] = udiv i32 %[[VAL_337]], 1
// CHECK:         %[[VAL_339:.*]] = urem i32 %[[VAL_338]], 32
// CHECK:         %[[VAL_340:.*]] = udiv i32 %[[VAL_337]], 32
// CHECK:         %[[VAL_341:.*]] = urem i32 %[[VAL_340]], 32
// CHECK:         %[[VAL_342:.*]] = udiv i32 %[[VAL_337]], 1024
// CHECK:         %[[VAL_343:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_344:.*]] = getelementptr inbounds float, float* %[[VAL_343]], i32 %[[VAL_337]]
// CHECK:         %[[VAL_345:.*]] = load float, float* %[[VAL_344]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_345]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_346:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_346]], float* %[[VAL_92]], float* %[[VAL_346]])
// CHECK:         %[[VAL_347:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_348:.*]] = getelementptr inbounds float, float* %[[VAL_347]], i32 %[[VAL_337]]
// CHECK:         %[[VAL_349:.*]] = load float, float* %[[VAL_348]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_349]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_350:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_350]], float* %[[VAL_90]], float* %[[VAL_350]])
// CHECK:         %[[VAL_351:.*]] = add i32 193, %[[VAL_135]]
// CHECK:         %[[VAL_352:.*]] = add i32 %[[VAL_136]], 193
// CHECK:         %[[VAL_353:.*]] = mul nuw nsw i32 %[[VAL_352]], 1
// CHECK:         %[[VAL_354:.*]] = add nuw nsw i32 0, %[[VAL_353]]
// CHECK:         %[[VAL_355:.*]] = mul nuw nsw i32 %[[VAL_203]], 32
// CHECK:         %[[VAL_356:.*]] = add nuw nsw i32 %[[VAL_354]], %[[VAL_355]]
// CHECK:         %[[VAL_357:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_358:.*]] = add nuw nsw i32 %[[VAL_356]], %[[VAL_357]]
// CHECK:         %[[VAL_359:.*]] = udiv i32 %[[VAL_358]], 1
// CHECK:         %[[VAL_360:.*]] = urem i32 %[[VAL_359]], 32
// CHECK:         %[[VAL_361:.*]] = udiv i32 %[[VAL_358]], 32
// CHECK:         %[[VAL_362:.*]] = urem i32 %[[VAL_361]], 32
// CHECK:         %[[VAL_363:.*]] = udiv i32 %[[VAL_358]], 1024
// CHECK:         %[[VAL_364:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_365:.*]] = getelementptr inbounds float, float* %[[VAL_364]], i32 %[[VAL_358]]
// CHECK:         %[[VAL_366:.*]] = load float, float* %[[VAL_365]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_366]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_367:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_367]], float* %[[VAL_92]], float* %[[VAL_367]])
// CHECK:         %[[VAL_368:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_369:.*]] = getelementptr inbounds float, float* %[[VAL_368]], i32 %[[VAL_358]]
// CHECK:         %[[VAL_370:.*]] = load float, float* %[[VAL_369]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_370]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_371:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_371]], float* %[[VAL_90]], float* %[[VAL_371]])
// CHECK:         br label %[[VAL_138]]
// CHECK:       output_is_full_tile-false:                        ; preds = %[[VAL_142]]
// CHECK:         %[[VAL_372:.*]] = add i32 %[[VAL_133]], %[[VAL_139]]
// CHECK:         %[[VAL_373:.*]] = add i32 0, %[[VAL_135]]
// CHECK:         %[[VAL_374:.*]] = add i32 %[[VAL_136]], 0
// CHECK:         %[[VAL_375:.*]] = icmp ult i32 %[[VAL_373]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_375]], label %[[VAL_376:.*]], label %[[VAL_377:.*]]
// CHECK:       output_x_in_tile-after:                           ; preds = %[[VAL_376]], %[[VAL_147]]
// CHECK:         %[[VAL_378:.*]] = add i32 1, %[[VAL_135]]
// CHECK:         %[[VAL_379:.*]] = add i32 %[[VAL_136]], 1
// CHECK:         %[[VAL_380:.*]] = icmp ult i32 %[[VAL_378]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_380]], label %[[VAL_381:.*]], label %[[VAL_382:.*]]
// CHECK:       output_x_in_tile-after48:                         ; preds = %[[VAL_381]], %[[VAL_377]]
// CHECK:         %[[VAL_383:.*]] = add i32 64, %[[VAL_135]]
// CHECK:         %[[VAL_384:.*]] = add i32 %[[VAL_136]], 64
// CHECK:         %[[VAL_385:.*]] = icmp ult i32 %[[VAL_383]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_385]], label %[[VAL_386:.*]], label %[[VAL_387:.*]]
// CHECK:       output_x_in_tile-after55:                         ; preds = %[[VAL_386]], %[[VAL_382]]
// CHECK:         %[[VAL_388:.*]] = add i32 65, %[[VAL_135]]
// CHECK:         %[[VAL_389:.*]] = add i32 %[[VAL_136]], 65
// CHECK:         %[[VAL_390:.*]] = icmp ult i32 %[[VAL_388]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_390]], label %[[VAL_391:.*]], label %[[VAL_392:.*]]
// CHECK:       output_x_in_tile-after62:                         ; preds = %[[VAL_391]], %[[VAL_387]]
// CHECK:         %[[VAL_393:.*]] = add i32 128, %[[VAL_135]]
// CHECK:         %[[VAL_394:.*]] = add i32 %[[VAL_136]], 128
// CHECK:         %[[VAL_395:.*]] = icmp ult i32 %[[VAL_393]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_395]], label %[[VAL_396:.*]], label %[[VAL_397:.*]]
// CHECK:       output_x_in_tile-after69:                         ; preds = %[[VAL_396]], %[[VAL_392]]
// CHECK:         %[[VAL_398:.*]] = add i32 129, %[[VAL_135]]
// CHECK:         %[[VAL_399:.*]] = add i32 %[[VAL_136]], 129
// CHECK:         %[[VAL_400:.*]] = icmp ult i32 %[[VAL_398]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_400]], label %[[VAL_401:.*]], label %[[VAL_402:.*]]
// CHECK:       output_x_in_tile-after76:                         ; preds = %[[VAL_401]], %[[VAL_397]]
// CHECK:         %[[VAL_403:.*]] = add i32 192, %[[VAL_135]]
// CHECK:         %[[VAL_404:.*]] = add i32 %[[VAL_136]], 192
// CHECK:         %[[VAL_405:.*]] = icmp ult i32 %[[VAL_403]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_405]], label %[[VAL_406:.*]], label %[[VAL_407:.*]]
// CHECK:       output_x_in_tile-after83:                         ; preds = %[[VAL_406]], %[[VAL_402]]
// CHECK:         %[[VAL_408:.*]] = add i32 193, %[[VAL_135]]
// CHECK:         %[[VAL_409:.*]] = add i32 %[[VAL_136]], 193
// CHECK:         %[[VAL_410:.*]] = icmp ult i32 %[[VAL_408]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_410]], label %[[VAL_411:.*]], label %[[VAL_148]]
// CHECK:       output_x_in_tile-after90:                         ; preds = %[[VAL_411]], %[[VAL_407]]
// CHECK:         br label %[[VAL_138]]
// CHECK:       output_x_in_tile-true:                            ; preds = %[[VAL_147]]
// CHECK:         %[[VAL_412:.*]] = mul nuw nsw i32 %[[VAL_374]], 1
// CHECK:         %[[VAL_413:.*]] = add nuw nsw i32 0, %[[VAL_412]]
// CHECK:         %[[VAL_414:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_415:.*]] = add nuw nsw i32 %[[VAL_413]], %[[VAL_414]]
// CHECK:         %[[VAL_416:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_417:.*]] = add nuw nsw i32 %[[VAL_415]], %[[VAL_416]]
// CHECK:         %[[VAL_418:.*]] = udiv i32 %[[VAL_417]], 1
// CHECK:         %[[VAL_419:.*]] = urem i32 %[[VAL_418]], 32
// CHECK:         %[[VAL_420:.*]] = udiv i32 %[[VAL_417]], 32
// CHECK:         %[[VAL_421:.*]] = urem i32 %[[VAL_420]], 32
// CHECK:         %[[VAL_422:.*]] = udiv i32 %[[VAL_417]], 1024
// CHECK:         %[[VAL_423:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_424:.*]] = getelementptr inbounds float, float* %[[VAL_423]], i32 %[[VAL_417]]
// CHECK:         %[[VAL_425:.*]] = load float, float* %[[VAL_424]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_425]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_426:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_426]], float* %[[VAL_92]], float* %[[VAL_426]])
// CHECK:         %[[VAL_427:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_428:.*]] = getelementptr inbounds float, float* %[[VAL_427]], i32 %[[VAL_417]]
// CHECK:         %[[VAL_429:.*]] = load float, float* %[[VAL_428]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_429]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_430:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_430]], float* %[[VAL_90]], float* %[[VAL_430]])
// CHECK:         br label %[[VAL_377]]
// CHECK:       output_x_in_tile-true47:                          ; preds = %[[VAL_377]]
// CHECK:         %[[VAL_431:.*]] = mul nuw nsw i32 %[[VAL_379]], 1
// CHECK:         %[[VAL_432:.*]] = add nuw nsw i32 0, %[[VAL_431]]
// CHECK:         %[[VAL_433:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_434:.*]] = add nuw nsw i32 %[[VAL_432]], %[[VAL_433]]
// CHECK:         %[[VAL_435:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_436:.*]] = add nuw nsw i32 %[[VAL_434]], %[[VAL_435]]
// CHECK:         %[[VAL_437:.*]] = udiv i32 %[[VAL_436]], 1
// CHECK:         %[[VAL_438:.*]] = urem i32 %[[VAL_437]], 32
// CHECK:         %[[VAL_439:.*]] = udiv i32 %[[VAL_436]], 32
// CHECK:         %[[VAL_440:.*]] = urem i32 %[[VAL_439]], 32
// CHECK:         %[[VAL_441:.*]] = udiv i32 %[[VAL_436]], 1024
// CHECK:         %[[VAL_442:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_443:.*]] = getelementptr inbounds float, float* %[[VAL_442]], i32 %[[VAL_436]]
// CHECK:         %[[VAL_444:.*]] = load float, float* %[[VAL_443]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_444]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_445:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_445]], float* %[[VAL_92]], float* %[[VAL_445]])
// CHECK:         %[[VAL_446:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_447:.*]] = getelementptr inbounds float, float* %[[VAL_446]], i32 %[[VAL_436]]
// CHECK:         %[[VAL_448:.*]] = load float, float* %[[VAL_447]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_448]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_449:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_449]], float* %[[VAL_90]], float* %[[VAL_449]])
// CHECK:         br label %[[VAL_382]]
// CHECK:       output_x_in_tile-true54:                          ; preds = %[[VAL_382]]
// CHECK:         %[[VAL_450:.*]] = mul nuw nsw i32 %[[VAL_384]], 1
// CHECK:         %[[VAL_451:.*]] = add nuw nsw i32 0, %[[VAL_450]]
// CHECK:         %[[VAL_452:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_453:.*]] = add nuw nsw i32 %[[VAL_451]], %[[VAL_452]]
// CHECK:         %[[VAL_454:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_455:.*]] = add nuw nsw i32 %[[VAL_453]], %[[VAL_454]]
// CHECK:         %[[VAL_456:.*]] = udiv i32 %[[VAL_455]], 1
// CHECK:         %[[VAL_457:.*]] = urem i32 %[[VAL_456]], 32
// CHECK:         %[[VAL_458:.*]] = udiv i32 %[[VAL_455]], 32
// CHECK:         %[[VAL_459:.*]] = urem i32 %[[VAL_458]], 32
// CHECK:         %[[VAL_460:.*]] = udiv i32 %[[VAL_455]], 1024
// CHECK:         %[[VAL_461:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_462:.*]] = getelementptr inbounds float, float* %[[VAL_461]], i32 %[[VAL_455]]
// CHECK:         %[[VAL_463:.*]] = load float, float* %[[VAL_462]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_463]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_464:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_464]], float* %[[VAL_92]], float* %[[VAL_464]])
// CHECK:         %[[VAL_465:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_466:.*]] = getelementptr inbounds float, float* %[[VAL_465]], i32 %[[VAL_455]]
// CHECK:         %[[VAL_467:.*]] = load float, float* %[[VAL_466]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_467]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_468:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_468]], float* %[[VAL_90]], float* %[[VAL_468]])
// CHECK:         br label %[[VAL_387]]
// CHECK:       output_x_in_tile-true61:                          ; preds = %[[VAL_387]]
// CHECK:         %[[VAL_469:.*]] = mul nuw nsw i32 %[[VAL_389]], 1
// CHECK:         %[[VAL_470:.*]] = add nuw nsw i32 0, %[[VAL_469]]
// CHECK:         %[[VAL_471:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_472:.*]] = add nuw nsw i32 %[[VAL_470]], %[[VAL_471]]
// CHECK:         %[[VAL_473:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_474:.*]] = add nuw nsw i32 %[[VAL_472]], %[[VAL_473]]
// CHECK:         %[[VAL_475:.*]] = udiv i32 %[[VAL_474]], 1
// CHECK:         %[[VAL_476:.*]] = urem i32 %[[VAL_475]], 32
// CHECK:         %[[VAL_477:.*]] = udiv i32 %[[VAL_474]], 32
// CHECK:         %[[VAL_478:.*]] = urem i32 %[[VAL_477]], 32
// CHECK:         %[[VAL_479:.*]] = udiv i32 %[[VAL_474]], 1024
// CHECK:         %[[VAL_480:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_481:.*]] = getelementptr inbounds float, float* %[[VAL_480]], i32 %[[VAL_474]]
// CHECK:         %[[VAL_482:.*]] = load float, float* %[[VAL_481]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_482]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_483:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_483]], float* %[[VAL_92]], float* %[[VAL_483]])
// CHECK:         %[[VAL_484:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_485:.*]] = getelementptr inbounds float, float* %[[VAL_484]], i32 %[[VAL_474]]
// CHECK:         %[[VAL_486:.*]] = load float, float* %[[VAL_485]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_486]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_487:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_487]], float* %[[VAL_90]], float* %[[VAL_487]])
// CHECK:         br label %[[VAL_392]]
// CHECK:       output_x_in_tile-true68:                          ; preds = %[[VAL_392]]
// CHECK:         %[[VAL_488:.*]] = mul nuw nsw i32 %[[VAL_394]], 1
// CHECK:         %[[VAL_489:.*]] = add nuw nsw i32 0, %[[VAL_488]]
// CHECK:         %[[VAL_490:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_491:.*]] = add nuw nsw i32 %[[VAL_489]], %[[VAL_490]]
// CHECK:         %[[VAL_492:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_493:.*]] = add nuw nsw i32 %[[VAL_491]], %[[VAL_492]]
// CHECK:         %[[VAL_494:.*]] = udiv i32 %[[VAL_493]], 1
// CHECK:         %[[VAL_495:.*]] = urem i32 %[[VAL_494]], 32
// CHECK:         %[[VAL_496:.*]] = udiv i32 %[[VAL_493]], 32
// CHECK:         %[[VAL_497:.*]] = urem i32 %[[VAL_496]], 32
// CHECK:         %[[VAL_498:.*]] = udiv i32 %[[VAL_493]], 1024
// CHECK:         %[[VAL_499:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_500:.*]] = getelementptr inbounds float, float* %[[VAL_499]], i32 %[[VAL_493]]
// CHECK:         %[[VAL_501:.*]] = load float, float* %[[VAL_500]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_501]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_502:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_502]], float* %[[VAL_92]], float* %[[VAL_502]])
// CHECK:         %[[VAL_503:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_504:.*]] = getelementptr inbounds float, float* %[[VAL_503]], i32 %[[VAL_493]]
// CHECK:         %[[VAL_505:.*]] = load float, float* %[[VAL_504]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_505]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_506:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_506]], float* %[[VAL_90]], float* %[[VAL_506]])
// CHECK:         br label %[[VAL_397]]
// CHECK:       output_x_in_tile-true75:                          ; preds = %[[VAL_397]]
// CHECK:         %[[VAL_507:.*]] = mul nuw nsw i32 %[[VAL_399]], 1
// CHECK:         %[[VAL_508:.*]] = add nuw nsw i32 0, %[[VAL_507]]
// CHECK:         %[[VAL_509:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_510:.*]] = add nuw nsw i32 %[[VAL_508]], %[[VAL_509]]
// CHECK:         %[[VAL_511:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_512:.*]] = add nuw nsw i32 %[[VAL_510]], %[[VAL_511]]
// CHECK:         %[[VAL_513:.*]] = udiv i32 %[[VAL_512]], 1
// CHECK:         %[[VAL_514:.*]] = urem i32 %[[VAL_513]], 32
// CHECK:         %[[VAL_515:.*]] = udiv i32 %[[VAL_512]], 32
// CHECK:         %[[VAL_516:.*]] = urem i32 %[[VAL_515]], 32
// CHECK:         %[[VAL_517:.*]] = udiv i32 %[[VAL_512]], 1024
// CHECK:         %[[VAL_518:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_519:.*]] = getelementptr inbounds float, float* %[[VAL_518]], i32 %[[VAL_512]]
// CHECK:         %[[VAL_520:.*]] = load float, float* %[[VAL_519]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_520]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_521:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_521]], float* %[[VAL_92]], float* %[[VAL_521]])
// CHECK:         %[[VAL_522:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_523:.*]] = getelementptr inbounds float, float* %[[VAL_522]], i32 %[[VAL_512]]
// CHECK:         %[[VAL_524:.*]] = load float, float* %[[VAL_523]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_524]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_525:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_525]], float* %[[VAL_90]], float* %[[VAL_525]])
// CHECK:         br label %[[VAL_402]]
// CHECK:       output_x_in_tile-true82:                          ; preds = %[[VAL_402]]
// CHECK:         %[[VAL_526:.*]] = mul nuw nsw i32 %[[VAL_404]], 1
// CHECK:         %[[VAL_527:.*]] = add nuw nsw i32 0, %[[VAL_526]]
// CHECK:         %[[VAL_528:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_529:.*]] = add nuw nsw i32 %[[VAL_527]], %[[VAL_528]]
// CHECK:         %[[VAL_530:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_531:.*]] = add nuw nsw i32 %[[VAL_529]], %[[VAL_530]]
// CHECK:         %[[VAL_532:.*]] = udiv i32 %[[VAL_531]], 1
// CHECK:         %[[VAL_533:.*]] = urem i32 %[[VAL_532]], 32
// CHECK:         %[[VAL_534:.*]] = udiv i32 %[[VAL_531]], 32
// CHECK:         %[[VAL_535:.*]] = urem i32 %[[VAL_534]], 32
// CHECK:         %[[VAL_536:.*]] = udiv i32 %[[VAL_531]], 1024
// CHECK:         %[[VAL_537:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_538:.*]] = getelementptr inbounds float, float* %[[VAL_537]], i32 %[[VAL_531]]
// CHECK:         %[[VAL_539:.*]] = load float, float* %[[VAL_538]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_539]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_540:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_540]], float* %[[VAL_92]], float* %[[VAL_540]])
// CHECK:         %[[VAL_541:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_542:.*]] = getelementptr inbounds float, float* %[[VAL_541]], i32 %[[VAL_531]]
// CHECK:         %[[VAL_543:.*]] = load float, float* %[[VAL_542]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_543]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_544:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_544]], float* %[[VAL_90]], float* %[[VAL_544]])
// CHECK:         br label %[[VAL_407]]
// CHECK:       output_x_in_tile-true89:                          ; preds = %[[VAL_407]]
// CHECK:         %[[VAL_545:.*]] = mul nuw nsw i32 %[[VAL_409]], 1
// CHECK:         %[[VAL_546:.*]] = add nuw nsw i32 0, %[[VAL_545]]
// CHECK:         %[[VAL_547:.*]] = mul nuw nsw i32 %[[VAL_372]], 32
// CHECK:         %[[VAL_548:.*]] = add nuw nsw i32 %[[VAL_546]], %[[VAL_547]]
// CHECK:         %[[VAL_549:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_550:.*]] = add nuw nsw i32 %[[VAL_548]], %[[VAL_549]]
// CHECK:         %[[VAL_551:.*]] = udiv i32 %[[VAL_550]], 1
// CHECK:         %[[VAL_552:.*]] = urem i32 %[[VAL_551]], 32
// CHECK:         %[[VAL_553:.*]] = udiv i32 %[[VAL_550]], 32
// CHECK:         %[[VAL_554:.*]] = urem i32 %[[VAL_553]], 32
// CHECK:         %[[VAL_555:.*]] = udiv i32 %[[VAL_550]], 1024
// CHECK:         %[[VAL_556:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_557:.*]] = getelementptr inbounds float, float* %[[VAL_556]], i32 %[[VAL_550]]
// CHECK:         %[[VAL_558:.*]] = load float, float* %[[VAL_557]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_558]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_559:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_559]], float* %[[VAL_92]], float* %[[VAL_559]])
// CHECK:         %[[VAL_560:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_561:.*]] = getelementptr inbounds float, float* %[[VAL_560]], i32 %[[VAL_550]]
// CHECK:         %[[VAL_562:.*]] = load float, float* %[[VAL_561]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_562]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_563:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_563]], float* %[[VAL_90]], float* %[[VAL_563]])
// CHECK:         br label %[[VAL_148]]
// CHECK:       intra_warp_reduce_write-true:                     ; preds = %[[VAL_141]]
// CHECK:         %[[VAL_564:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_0, i32 0, i32 0, i32 %[[VAL_172]]
// CHECK:         %[[VAL_565:.*]] = addrspacecast float addrspace(3)* %[[VAL_564]] to float*
// CHECK:         %[[VAL_566:.*]] = load float, float* %[[VAL_161]], align 4
// CHECK:         store float %[[VAL_566]], float* %[[VAL_565]], align 4
// CHECK:         br label %[[VAL_175]]
// CHECK:       inter_warp_reduce-true:                           ; preds = %[[VAL_175]]
// CHECK:         %[[VAL_567:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_0, i32 0, i32 0, i32 %[[VAL_152]]
// CHECK:         %[[VAL_568:.*]] = addrspacecast float addrspace(3)* %[[VAL_567]] to float*
// CHECK:         store float %[[VAL_114]], float* %[[VAL_82]], align 4
// CHECK:         %[[VAL_569:.*]] = icmp ult i32 %[[VAL_150]], 1
// CHECK:         %[[VAL_570:.*]] = select i1 %[[VAL_569]], float* %[[VAL_568]], float* %[[VAL_82]]
// CHECK:         %[[VAL_571:.*]] = load float, float* %[[VAL_570]], align 4
// CHECK:         %[[VAL_572:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_571]], i32 16, i32 31)
// CHECK:         store float %[[VAL_572]], float* %[[VAL_81]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_570]], float* %[[VAL_81]], float* %[[VAL_570]])
// CHECK:         %[[VAL_573:.*]] = load float, float* %[[VAL_570]], align 4
// CHECK:         %[[VAL_574:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_573]], i32 8, i32 31)
// CHECK:         store float %[[VAL_574]], float* %[[VAL_80]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_570]], float* %[[VAL_80]], float* %[[VAL_570]])
// CHECK:         %[[VAL_575:.*]] = load float, float* %[[VAL_570]], align 4
// CHECK:         %[[VAL_576:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_575]], i32 4, i32 31)
// CHECK:         store float %[[VAL_576]], float* %[[VAL_79]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_570]], float* %[[VAL_79]], float* %[[VAL_570]])
// CHECK:         %[[VAL_577:.*]] = load float, float* %[[VAL_570]], align 4
// CHECK:         %[[VAL_578:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_577]], i32 2, i32 31)
// CHECK:         store float %[[VAL_578]], float* %[[VAL_78]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_570]], float* %[[VAL_78]], float* %[[VAL_570]])
// CHECK:         %[[VAL_579:.*]] = load float, float* %[[VAL_570]], align 4
// CHECK:         %[[VAL_580:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_579]], i32 1, i32 31)
// CHECK:         store float %[[VAL_580]], float* %[[VAL_77]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_570]], float* %[[VAL_77]], float* %[[VAL_570]])
// CHECK:         %[[VAL_581:.*]] = icmp eq i32 %[[VAL_150]], 0
// CHECK:         br i1 %[[VAL_581]], label %[[VAL_582:.*]], label %[[VAL_179]]
// CHECK:       reduction_atomic_update-after:                    ; preds = %[[VAL_582]], %[[VAL_177]]
// CHECK:         br label %[[VAL_178]]
// CHECK:       reduction_atomic_update-true:                     ; preds = %[[VAL_177]]
// CHECK:         %[[VAL_583:.*]] = load float, float* %[[VAL_568]], align 4
// CHECK:         %[[VAL_584:.*]] = atomicrmw fadd float* %[[VAL_160]], float %[[VAL_583]] seq_cst, align 4
// CHECK:         br label %[[VAL_179]]
// CHECK:       intra_warp_reduce_write-true128:                  ; preds = %[[VAL_178]]
// CHECK:         %[[VAL_585:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_1, i32 0, i32 0, i32 %[[VAL_196]]
// CHECK:         %[[VAL_586:.*]] = addrspacecast float addrspace(3)* %[[VAL_585]] to float*
// CHECK:         %[[VAL_587:.*]] = load float, float* %[[VAL_185]], align 4
// CHECK:         store float %[[VAL_587]], float* %[[VAL_586]], align 4
// CHECK:         br label %[[VAL_199]]
// CHECK:       inter_warp_reduce-true130:                        ; preds = %[[VAL_199]]
// CHECK:         %[[VAL_588:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_1, i32 0, i32 0, i32 %[[VAL_152]]
// CHECK:         %[[VAL_589:.*]] = addrspacecast float addrspace(3)* %[[VAL_588]] to float*
// CHECK:         store float %[[VAL_116]], float* %[[VAL_71]], align 4
// CHECK:         %[[VAL_590:.*]] = icmp ult i32 %[[VAL_150]], 1
// CHECK:         %[[VAL_591:.*]] = select i1 %[[VAL_590]], float* %[[VAL_589]], float* %[[VAL_71]]
// CHECK:         %[[VAL_592:.*]] = load float, float* %[[VAL_591]], align 4
// CHECK:         %[[VAL_593:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_592]], i32 16, i32 31)
// CHECK:         store float %[[VAL_593]], float* %[[VAL_70]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_591]], float* %[[VAL_70]], float* %[[VAL_591]])
// CHECK:         %[[VAL_594:.*]] = load float, float* %[[VAL_591]], align 4
// CHECK:         %[[VAL_595:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_594]], i32 8, i32 31)
// CHECK:         store float %[[VAL_595]], float* %[[VAL_69]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_591]], float* %[[VAL_69]], float* %[[VAL_591]])
// CHECK:         %[[VAL_596:.*]] = load float, float* %[[VAL_591]], align 4
// CHECK:         %[[VAL_597:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_596]], i32 4, i32 31)
// CHECK:         store float %[[VAL_597]], float* %[[VAL_68]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_591]], float* %[[VAL_68]], float* %[[VAL_591]])
// CHECK:         %[[VAL_598:.*]] = load float, float* %[[VAL_591]], align 4
// CHECK:         %[[VAL_599:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_598]], i32 2, i32 31)
// CHECK:         store float %[[VAL_599]], float* %[[VAL_67]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_591]], float* %[[VAL_67]], float* %[[VAL_591]])
// CHECK:         %[[VAL_600:.*]] = load float, float* %[[VAL_591]], align 4
// CHECK:         %[[VAL_601:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_600]], i32 1, i32 31)
// CHECK:         store float %[[VAL_601]], float* %[[VAL_66]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_591]], float* %[[VAL_66]], float* %[[VAL_591]])
// CHECK:         %[[VAL_602:.*]] = icmp eq i32 %[[VAL_150]], 0
// CHECK:         br i1 %[[VAL_602]], label %[[VAL_603:.*]], label %[[VAL_202]]
// CHECK:       reduction_atomic_update-after144:                 ; preds = %[[VAL_604:.*]], %[[VAL_201]]
// CHECK:         br label %[[VAL_112]]
// CHECK:       reduction_atomic_update-true143:                  ; preds = %[[VAL_201]]
// CHECK:         %[[VAL_605:.*]] = load float, float* %[[VAL_589]], align 4
// CHECK:         %[[VAL_606:.*]] = bitcast float* %[[VAL_184]] to i32*
// CHECK:         %[[VAL_607:.*]] = bitcast i32* %[[VAL_64]] to float*
// CHECK:         %[[VAL_608:.*]] = load i32, i32* %[[VAL_606]], align 4
// CHECK:         store i32 %[[VAL_608]], i32* %[[VAL_65]], align 4
// CHECK:         br label %[[VAL_609:.*]]
// CHECK:       atomic_op_loop_exit:                              ; preds = %[[VAL_610:.*]], %[[VAL_609]]
// CHECK:         br label %[[VAL_202]]
// CHECK:       atomic_op_loop_body:                              ; preds = %[[VAL_610]], %[[VAL_603]]
// CHECK:         %[[VAL_611:.*]] = load i32, i32* %[[VAL_65]], align 4
// CHECK:         store i32 %[[VAL_611]], i32* %[[VAL_64]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_607]], float* %[[VAL_589]], float* %[[VAL_607]])
// CHECK:         %[[VAL_612:.*]] = load i32, i32* %[[VAL_64]], align 4
// CHECK:         %[[VAL_613:.*]] = icmp eq i32 %[[VAL_611]], %[[VAL_612]]
// CHECK:         br i1 %[[VAL_613]], label %[[VAL_604]], label %[[VAL_610]]
// CHECK:       atomic_op_loop_cas:                               ; preds = %[[VAL_609]]
// CHECK:         %[[VAL_614:.*]] = cmpxchg i32* %[[VAL_606]], i32 %[[VAL_611]], i32 %[[VAL_612]] seq_cst seq_cst, align 4
// CHECK:         %[[VAL_615:.*]] = extractvalue { i32, i1 } %[[VAL_614]], 0
// CHECK:         store i32 %[[VAL_615]], i32* %[[VAL_65]], align 4
// CHECK:         %[[VAL_616:.*]] = extractvalue { i32, i1 } %[[VAL_614]], 1
// CHECK:         br i1 %[[VAL_616]], label %[[VAL_604]], label %[[VAL_609]]
// CHECK:       entry:
// CHECK:         %[[VAL_617:.*]] = alloca float, align 4
// CHECK:         %[[VAL_618:.*]] = load float, float* %[[VAL_619:.*]], align 4
// CHECK:         %[[VAL_620:.*]] = load float, float* %[[VAL_621:.*]], align 4
// CHECK:         %[[VAL_622:.*]] = fadd float %[[VAL_618]], %[[VAL_620]]
// CHECK:         store float %[[VAL_622]], float* %[[VAL_617]], align 4
// CHECK:         %[[VAL_623:.*]] = load float, float* %[[VAL_617]], align 4
// CHECK:         store float %[[VAL_623]], float* %[[VAL_624:.*]], align 4
// CHECK:         ret void
// CHECK:       entry:
// CHECK:         %[[VAL_625:.*]] = alloca float, align 4
// CHECK:         %[[VAL_626:.*]] = load float, float* %[[VAL_627:.*]], align 4
// CHECK:         %[[VAL_628:.*]] = load float, float* %[[VAL_629:.*]], align 4
// CHECK:         %[[VAL_630:.*]] = call float @llvm.maxnum.f32(float %[[VAL_626]], float %[[VAL_628]])
// CHECK:         store float %[[VAL_630]], float* %[[VAL_625]], align 4
// CHECK:         %[[VAL_631:.*]] = load float, float* %[[VAL_625]], align 4
// CHECK:         store float %[[VAL_631]], float* %[[VAL_632:.*]], align 4
// CHECK:         ret void

HloModule Test

Add {
  lhsadd = f32[] parameter(0)
  rhsadd = f32[] parameter(1)
  ROOT add = f32[] add(lhsadd, rhsadd)
}

Max {
  lhsmax = f32[] parameter(0)
  rhsmax = f32[] parameter(1)
  ROOT max = f32[] maximum(lhsmax, rhsmax)
}


fused_reduce {
  p0 = f32[2,32,32]{2,1,0} parameter(0)
  init1 = f32[] parameter(1)
  init2 = f32[] parameter(2)
  r1 = f32[2,32]{1,0} reduce(p0, init1), dimensions={2}, to_apply=Add
  r2 = f32[2,32]{1,0} reduce(p0, init2), dimensions={2}, to_apply=Max
  ROOT tuple = (f32[2,32]{1,0}, f32[2,32]{1,0}) tuple(r1, r2)
}

ENTRY reduce {
  p = f32[2,32,32]{2,1,0} parameter(0)
  i = f32[] parameter(1)
  j = f32[] parameter(2)
  ROOT fusion = (f32[2,32]{1,0}, f32[2,32]{1,0}) fusion(p, i, j),
   kind=kInput, calls=fused_reduce
}
