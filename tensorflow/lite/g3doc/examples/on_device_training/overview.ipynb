{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nWetWWd_ns"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2pHVBk_seED1"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7vSdG6sAIQn"
      },
      "source": [
        "# On-Device Training with TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwc5GKHBASdc"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/lite/examples/on_device_training/overview\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ee074e4"
      },
      "source": [
        "When deploying TensorFlow Lite machine learning model to device or mobile app, you may want to enable the model to be improved or personalized based on input from the device or end user. Using on-device training techniques allows you to update a model *without* data leaving your users' devices, improving user privacy, and without requiring users to update the device software.\n",
        "\n",
        "For example, you may have a model in your mobile app that recognizes fashion items, but you want users to get improved recognition performance over time based on their interests. Enabling on-device training allows users who are interested in shoes to get better at recognizing a particular style of shoe or shoe brand the more often they use your app.\n",
        "\n",
        "This tutorial shows you how to construct a TensorFlow Lite model that can be incrementally trained and improved within an installed Android app. \n",
        "\n",
        "Note: The on-device training technique can be added to existing TensorFlow Lite implementations, provided the devices you are targeting support local file storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaWdLA3fQDK2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This tutorial uses Python to train and convert a TensorFlow model before incorporating it into an Android app. Get started by installing and importing the following packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_3qi01Is62j"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow keras\n",
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j4MGqyKQEo4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOCyq3dTvfL6"
      },
      "source": [
        "Note: The On-Device Training APIs are available in TensorFlow version 2.7 and higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_omifE5JKpt4"
      },
      "source": [
        "## Classify images of clothing\n",
        "\n",
        "This example code uses the [Fashion MNIST dataset](https://keras.io/api/datasets/fashion_mnist/) to train a neural network model for classifying images of clothing. This dataset contains 60,000 small (28 x 28 pixel) grayscale images containing 10 different categories of fashion accessories, including dresses, shirts, and sandals.\n",
        "\n",
        "\u003cfigure\u003e\n",
        "  \u003cimg src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "       alt=\"Fashion MNIST images\"\u003e\n",
        "  \u003cfigcaption\u003e\u003cb\u003eFigure 1\u003c/b\u003e: \u003ca href=\"https://github.com/zalandoresearch/fashion-mnist\"\u003eFashion-MNIST samples\u003c/a\u003e (by Zalando, MIT License).\u003c/figcaption\u003e\n",
        "\u003c/figure\u003e\n",
        "\n",
        "You can explore this dataset in more depth in the [Keras classification tutorial](https://www.tensorflow.org/tutorials/keras/classification#import_the_fashion_mnist_dataset). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN2N6hPEP-Ay"
      },
      "source": [
        "## Build a model for on-device training\n",
        "\n",
        "TensorFlow Lite models typically have only a single exposed function method (or [signature](https://www.tensorflow.org/lite/guide/signatures)) that allows you to call the model to run an inference. For a model to be trained and used on a device, you must be able to perform several separate operations, including train, infer, save, and restore functions for the model. You can enable this functionality by first extending your TensorFlow model to have multiple functions, and then exposing those functions as signatures when you convert your model to the TensorFlow Lite model format. \n",
        "\n",
        "The code example below shows you how to add the following functions to a TensorFlow model:\n",
        "\n",
        "*   `train` function trains the model with training data.\n",
        "*   `infer` function invokes the inference.\n",
        "*   `save` function saves the trainable weights into the file system.\n",
        "*   `restore` function loads the trainable weights from the file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8577c80"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 28\n",
        "\n",
        "class Model(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),\n",
        "        tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax', name='dense_2')\n",
        "    ])\n",
        "\n",
        "    self.model.compile(\n",
        "        optimizer='sgd',\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy'])\n",
        "    self._LOSS_FN = tf.keras.losses.CategoricalCrossentropy()\n",
        "    self._OPTIM = tf.optimizers.SGD()\n",
        "\n",
        "  # The `train` function takes a batch of input images and labels.\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
        "      tf.TensorSpec([None, 10], tf.float32),\n",
        "  ])\n",
        "  def train(self, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "      prediction = self.model(x)\n",
        "      loss = self._LOSS_FN(prediction, y)\n",
        "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self._OPTIM.apply_gradients(\n",
        "        zip(gradients, self.model.trainable_variables))\n",
        "    result = {\"loss\": loss}\n",
        "    for grad in gradients:\n",
        "      result[grad.name] = grad\n",
        "    return result\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
        "  ])\n",
        "  def infer(self, x):\n",
        "    return {\n",
        "        \"output\": self.model(x)\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def save(self, checkpoint_path):\n",
        "    tensor_names = [weight.name for weight in self.model.weights]\n",
        "    tensors_to_save = [weight.read_value() for weight in self.model.weights]\n",
        "    tf.raw_ops.Save(\n",
        "        filename=checkpoint_path, tensor_names=tensor_names,\n",
        "        data=tensors_to_save, name='save')\n",
        "    return {\n",
        "        \"checkpoint_path\": checkpoint_path\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def restore(self, checkpoint_path):\n",
        "    restored_tensors = {}\n",
        "    for var in self.model.weights:\n",
        "      restored = tf.raw_ops.Restore(\n",
        "          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\n",
        "          name='restore')\n",
        "      var.assign(restored)\n",
        "      restored_tensors[var.name] = restored\n",
        "    return restored_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86tBMjoBnp6d"
      },
      "source": [
        "The `train` function in the code above uses the [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) class to record operations for automatic differentiation. For more information on how to use this class, see the [Introduction to gradients and automatic differentiation](https://www.tensorflow.org/guide/autodiff).\n",
        "\n",
        "Note: The weights generated by this model are serialized as a TensorFlow version one checkpoint file format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a813419961ef"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "Get the the Fashion MNIST dataset for training your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "315b8b4dfc16"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eDn-bRD30sw"
      },
      "source": [
        "### Preprocess the dataset\n",
        "\n",
        "Pixel values in this dataset are between 0 and 255, and must be normalized to a value between 0 and 1 for processing by the model. Divide the values by 255 to make this adjustment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0FqHC0yCg6n"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bbee849ab73"
      },
      "source": [
        "Convert the data labels to categorical values by performing one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fmc7EgYO30sw"
      },
      "outputs": [],
      "source": [
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79f5f372fb0e"
      },
      "source": [
        "Note: Make sure you preprocess your *training* and *testing* datasets in the same way, so that your testing accurately evaluate your model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkuDUFNNyAVN"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Before converting and setting up your TensorFlow Lite model, complete the initial training of your model using the preprocessed dataset and the `train` signature method. The following code runs model training for 100 epochs, processing batches of 100 images at a time, and displaying the loss value after every 10 epochs. Since this training run is processing quite a bit of data, it may take a few minutes to finish.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Diwn1MmkNVeX"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 100\n",
        "epochs = np.arange(1, NUM_EPOCHS + 1, 1)\n",
        "losses = np.zeros([NUM_EPOCHS])\n",
        "m = Model()\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "  for batch_idx in range(len(train_images) // BATCH_SIZE):\n",
        "    batched_images = train_images[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]\n",
        "    batched_labels = train_labels[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]\n",
        "    result = m.train(\n",
        "        x=tf.constant(batched_images, shape=(BATCH_SIZE, IMG_SIZE, IMG_SIZE),\n",
        "                      dtype=tf.float32),\n",
        "        y=tf.constant(batched_labels, shape=(BATCH_SIZE, 10), dtype=tf.float32))\n",
        "  losses[i] = result['loss']\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print('Finished {0} epochs, current loss: {1}'.format(i + 1, losses[i]))\n",
        "\n",
        "plt.plot(epochs, losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaMMDLLewAaX"
      },
      "source": [
        "Note: You should complete initial training of your model before converting it to TensorFlow Lite format, so that the model has an initial set of weights, and is able to perform reasonable inferences *before* you start collecting data and conducting training runs on the device. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8YUTIvzMVw5"
      },
      "source": [
        "## Convert model to TensorFlow Lite format\n",
        "\n",
        "After you have extended your TensorFlow model to enable additional functions for on-device training and completed initial training of the model, you can convert it to TensorFlow Lite format. The following code converts and saves your model to that format, including the set of signatures that you use with the TensorFlow Lite model on a device: `train, infer, save, restore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwsDUEKFMYtq"
      },
      "outputs": [],
      "source": [
        "SAVED_MODEL_DIR = \"saved_model\"\n",
        "m = Model()\n",
        "tf.saved_model.save(\n",
        "    m,\n",
        "    SAVED_MODEL_DIR,\n",
        "    signatures={\n",
        "        'train':\n",
        "            m.train.get_concrete_function(),\n",
        "        'infer':\n",
        "            m.infer.get_concrete_function(),\n",
        "        'save':\n",
        "            m.save.get_concrete_function(),\n",
        "        'restore':\n",
        "            m.restore.get_concrete_function(),\n",
        "    })\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
        "]\n",
        "converter.experimental_enable_resource_variables = True\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJSOtPISKSn2"
      },
      "source": [
        "### Setup the TensorFlow Lite signatures\n",
        "\n",
        "The TensorFlow Lite model you saved in the previous step contains several function signatures. You can access them through the `tf.lite.Interpreter` class and invoke each `restore`, `train`, `save`, and `infer` signature separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNX2vqXd2-HM"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "\n",
        "train = interpreter.get_signature_runner(\"train\")\n",
        "infer = interpreter.get_signature_runner(\"infer\")\n",
        "save = interpreter.get_signature_runner(\"save\")\n",
        "restore = interpreter.get_signature_runner(\"restore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5f73c8c5d6"
      },
      "source": [
        "## Retrain the model on a device\n",
        "\n",
        "After converting your model to TensorFlow Lite and deploying it with your app, you can retrain the model on a device using new data and the `train` signature method of your model. Each training run generates a new set of weights that you can save for re-use and further improvement of the model, as shown in the next section.\n",
        "\n",
        "Note: Since training tasks are resource intensive, you should consider performing them when users are not actively interacting with the device, and as a background process. Consider using the [WorkManager](https://developer.android.com/topic/libraries/architecture/workmanager) API to schedule model retraining as an asynchronous task. \n",
        "\n",
        "On Android, you can perform on-device training with TensorFlow Lite using either Java or C++ APIs. In Java, use the `Interpreter` class to load a model and drive model training tasks. The following example shows how to run the training procedure using the `runSignature` method:\n",
        "\n",
        "```Java\n",
        "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
        "    int NUM_EPOCHS = 100;\n",
        "    int BATCH_SIZE = 100;\n",
        "    int NUM_TRAININGS = 60000;\n",
        "    int NUM_BATCHES = NUM_TRAININGS / BATCH_SIZE;\n",
        "\n",
        "    List\u003cfloat[][][]\u003e trainImageBatches = new ArrayList\u003c\u003e(NUM_BATCHES);\n",
        "    List\u003cfloat[][]\u003e trainLabelBatches = new ArrayList\u003c\u003e(NUM_BATCHES);\n",
        "\n",
        "    // Prepare training batches.\n",
        "    for (int i = 0; i \u003c NUM_BATCHES; ++i) {\n",
        "        float[][][] trainImages = new float[BATCH_SIZE][28][28];\n",
        "        float[][] trainLabels = new float[BATCH_SIZE][10];\n",
        "        \n",
        "        // Fill the data values...\n",
        "        trainImageBatches.add(trainImages);\n",
        "        trainImageLabels.add(trainLabels);\n",
        "    }\n",
        "\n",
        "    // Run training for a few steps.\n",
        "    float[] losses = new float[NUM_EPOCHS];\n",
        "    for (int epoch = 0; epoch \u003c NUM_EPOCHS; ++epoch) {\n",
        "        for (int batchIdx = 0; batchIdx \u003c NUM_BATCHES; ++batchIdx) {\n",
        "            Map\u003cString, Object\u003e inputs = new HashMap\u003c\u003e();\n",
        "            inputs.put(\"x\", trainImageBatches.get(batchIdx));\n",
        "            inputs.put(\"y\", trainLabelBatches.get(batchIdx));\n",
        "\n",
        "            Map\u003cString, Object\u003e outputs = new HashMap\u003c\u003e();\n",
        "            FloatBuffer loss = FloatBuffer.allocate(1);\n",
        "            outputs.put(\"loss\", loss);\n",
        "            \n",
        "            interpreter.runSignature(inputs, outputs, \"train\");\n",
        "\n",
        "            // Record the last loss.\n",
        "            if (batchIdx == NUM_BATCHES - 1) losses[epoch] = loss.get(0);\n",
        "        }\n",
        "\n",
        "        // Print the loss output for every 10 epochs.\n",
        "        if ((epoch + 1) % 10 == 0) {\n",
        "            System.out.println(\n",
        "              \"Finished \" + (epoch + 1) + \" epochs, current loss: \" + loss.get(0));\n",
        "        } \n",
        "    }\n",
        "\n",
        "    // ...\n",
        "}\n",
        "```\n",
        "\n",
        "You can see a complete code example of model retraining inside an Android app in the [model personalization demo app](https://github.com/tensorflow/examples/blob/master/lite/examples/model_personalization/android/transfer_api/src/main/java/org/tensorflow/lite/examples/transfer/api/LiteMultipleSignatureModel.java)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDIi0_RlPb2n"
      },
      "source": [
        "## Save the trained weights\n",
        "\n",
        "When you complete a training run on a device, the model updates the set of weights it is using in memory. Using the `save` signature method you created in your TensorFlow Lite model, you can save these weights to a checkpoint file for later reuse and improve your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c3d3cc5f171"
      },
      "outputs": [],
      "source": [
        "save(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvlZN-rhR_Ag"
      },
      "source": [
        "In your Android application, you can store the generated weights as a checkpoint file in the internal storage space allocated for your app.\n",
        "\n",
        "```Java\n",
        "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
        "    // Conduct the training jobs.\n",
        "\n",
        "    // Export the trained weights as a checkpoint file.\n",
        "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
        "    Map\u003cString, Object\u003e inputs = new HashMap\u003c\u003e();\n",
        "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
        "    Map\u003cString, Object\u003e outputs = new HashMap\u003c\u003e();\n",
        "    interpreter.runSignature(inputs, outputs, \"save\");\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSDydMyOQfL5"
      },
      "source": [
        "## Restore the trained weights\n",
        "\n",
        "After you save a checkpoint file, you can restore it using the `restore` signature method. Loading this additional weighting data into your model allows you to potentially improve performance for individual users or create personalized models based on individual usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yIZoLveRZgp"
      },
      "outputs": [],
      "source": [
        "another_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "\n",
        "train = another_interpreter.get_signature_runner(\"train\")\n",
        "infer = another_interpreter.get_signature_runner(\"infer\")\n",
        "save = another_interpreter.get_signature_runner(\"save\")\n",
        "restore = another_interpreter.get_signature_runner(\"restore\")\n",
        "\n",
        "# Restore the trained weights from /tmp/model.ckpt\n",
        "restore(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAo-3Eg7oGH7"
      },
      "source": [
        "Note: Loading the saved weights from the checkpoint can take time, based on the number of variables in the model and the size of the checkpoint file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9I_-gjdSnGn"
      },
      "source": [
        "In your Android app, you can restore the serialized, trained weights from the checkpoint file you stored earlier.\n",
        "\n",
        "```Java\n",
        "try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {\n",
        "    // Load the trained weights from the checkpoint file.\n",
        "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
        "    Map\u003cString, Object\u003e inputs = new HashMap\u003c\u003e();\n",
        "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
        "    Map\u003cString, Object\u003e outputs = new HashMap\u003c\u003e();\n",
        "    anotherInterpreter.runSignature(inputs, outputs, \"restore\");\n",
        "}\n",
        "```\n",
        "\n",
        "Note: When your application restarts, you should reload your trained weights prior to running new inferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcrv57DSkz2"
      },
      "source": [
        "## Run Inference using trained weights\n",
        "\n",
        "Once you have loaded previously saved weights from a checkpoint file, running the `infer` method uses those weights with your original model to improve predictions. After loading the saved weights, you can use the `infer` signature method as shown below.\n",
        "\n",
        "Note: Loading the saved weights is not required to run an inference, but running in that configuration produces predictions using the originally trained model, without improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ROmlpHWS0nX"
      },
      "outputs": [],
      "source": [
        "infer = interpreter.get_signature_runner(\"infer\")\n",
        "result = infer(\n",
        "    x=tf.constant(test_images, shape=(len(test_images), IMG_SIZE, IMG_SIZE), dtype=tf.float32))\n",
        "result_labels = np.argmax(result[\"output\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGPtqeVULZui"
      },
      "source": [
        "Plot the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHbRasdfasd4"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "def plot(images, labels):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for i in range(25):\n",
        "      plt.subplot(5,5,i+1)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.grid(False)\n",
        "      plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "      plt.xlabel(class_names[labels[i]])\n",
        "  plt.show()\n",
        "\n",
        "plot(test_images, result_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eijDL3jNS6WI"
      },
      "source": [
        "In your Android application, after restoring the trained weights, run the inferences based on the loaded data.\n",
        "\n",
        "```Java\n",
        "try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {\n",
        "    // Restore the weights from the checkpoint file.\n",
        "\n",
        "    int NUM_TESTS = 10;\n",
        "    float[][][] testImages = new float[NUM_TESTS][28][28];\n",
        "    float[][] output = new float[NUM_TESTS][10];\n",
        "\n",
        "    // Fill the test data.\n",
        "\n",
        "    // Run the inference.\n",
        "    Map\u003cString, Object\u003e inputs = new HashMap\u003c\u003e();\n",
        "    inputs.put(\"x\", testImages);\n",
        "    Map\u003cString, Object\u003e outputs = new HashMap\u003c\u003e();\n",
        "    outputs.put(\"output\", output);\n",
        "    anotherInterpreter.runSignature(inputs, outputs, \"infer\");\n",
        "\n",
        "    // Process the result to get the final category values.\n",
        "    int[] testLabels = new int[NUM_TESTS];\n",
        "    for (int i = 0; i \u003c NUM_TESTS; ++i) {\n",
        "        int index = 0;\n",
        "        for (int j = 1; j \u003c 10; ++j) {\n",
        "            if (output[i][index] \u003c output[i][j]) index = testLabels[j];\n",
        "        }\n",
        "        testLabels[i] = index;\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cznoIDphGPEg"
      },
      "source": [
        "Congratulations! You now have built a TensorFlow Lite model that supports on-device training. For more coding details, check out the example implementation in the [model personalization demo app](https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization). \n",
        "\n",
        "If you are interested in learning more about image classification, check [Keras classification tutorial](https://www.tensorflow.org/tutorials/keras/classification) in the TensorFlow official guide page. This tutorial is based on that exercise and provides more depth on the subject of classification.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "overview.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
